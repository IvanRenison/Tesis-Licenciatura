\documentclass[12pt]{report}

\usepackage{polyglossia}
\setdefaultlanguage{spanish}

\usepackage{amsthm}
\usepackage{fontspec}
\usepackage{enumitem}
\usepackage[bookmarks]{hyperref}
\defaultfontfeatures{Renderer=Basic,Ligatures={TeX}}
\usepackage[math-style=ISO,bold-style=ISO]{unicode-math}
\setmathfont{Asana Math}
\usepackage{witharrows}
\WithArrowsOptions{fleqn,displaystyle,i,tikz={font={\small\normalfont}},ygap=0.5em,wrap-lines}

\DeclareEmphSequence{\bfseries}

\hfuzz=50pt % Elimina warnings de "Overfull \hbox"

\usepackage[margin=2cm]{geometry} % Adjust the margin values as desired

\usepackage{float}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\SetKw{Break}{break}
\SetKwFor{Loop}{loop}{}{end loop}

\usepackage{listings}
\lstset{basicstyle=\ttfamily} % Cambia la fuente a monospace
\usepackage{minted}

\usepackage[capitalise]{cleveref}
\crefname{section}{Sección}{Secciones} % Para que use "Sección" en vez de "Apartado"

\usepackage[backend=biber, autolang=hyphen, bibencoding=inputenc, isbn=false, uniquename=false, style=alphabetic]{biblatex}
\addbibresource{biblio.bib}

\setlist[enumerate,1]{label={\em{(\arabic*)}}}

\newtheoremstyle{customstyle} % <name>
{} % <Space above>
{} % <Space below>
{\slshape} % <Body font> % Preguntar que opinan, y porque hay un warning
{} % <Indent amount>
{\bfseries} % <Theorem head font>
{.} % <Punctuation after theorem head>
{.5em} % <Space after theorem head>
{} % <Theorem head spec (can be left empty, meaning `normal`)

\theoremstyle{customstyle}
\newtheorem{definition}{Definición}[chapter]
\newtheorem{theorem}{Teorema}[chapter]
\newtheorem{lemma}{Lema}[chapter]
\addto\captionsspanish{\renewcommand{\proofname}{Demostración}}
\renewenvironment{proof}[1][\proofname]{{\noindent \bfseries #1: }}{\qed} % Cambiar estilo del título de la demostración

\newtheoremstyle{factstyle} % <name>
{} % <Space above>
{} % <Space below>
{\normalfont} % <Body font> % Preguntar que opinan, y porque hay un warning
{1.5em} % <Indent amount>
{\bfseries} % <Theorem head font>
{.} % <Punctuation after theorem head>
{.5em} % <Space after theorem head>
{} % <Theorem head spec (can be left empty, meaning `normal`)

\theoremstyle{factstyle}
\newtheorem{fact}{Afirmación}[theorem]



\DeclareMathOperator{\sop}{sop}
\DeclareMathOperator{\lm}{lm}
\DeclareMathOperator{\lc}{lc}
\DeclareMathOperator{\lt}{lt}
\DeclareMathOperator{\tail}{tail}
\DeclareMathOperator{\amb}{amb}
\renewcommand{\S}{\text{S}}
\DeclareMathOperator{\B}{B}
\DeclareMathOperator{\spn}{span} % span está ocupado y re rompe todo si lo re-declaro
\DeclareMathOperator{\mat}{mat}
\DeclareMathOperator{\poli}{poli}
\DeclareMathOperator{\filas}{filas}

% Convenciones de uso de letras:
% X: Alfabeto
% K: Cuerpo
% R: Anillo
% I: Ideal
% G: Conjunto generador
% F: Conjunto de polinomios (que no sea un conjunto generador)
% f, g: Polinomios
% m: Monomios
% c: Coeficiente de K
% a, b, c, d: Coeficientes de monomios
% α: ambigüedades
% n: límite de una sumatoria
% i, j: índices

\begin{document}

\chapter{Introducción}

En este capítulo se explica informalmente de qué se trata la tesis.

Usted posiblemente haya escuchado hablar de los polinomios de varias variables, esos como $5  + 3 y - 2 x y + x^3 y^5$, y que todo el conjunto de los polinomios con, por ejemplo, variables $x, y$ sobre un cuerpo $K$ se denota como $K[x, y]$. En esos polinomios el producto entre las variables conmuta, por lo que, por ejemplo, es lo mismo el polinomio $x y$ que el polinomio $y x$. Algo que se puede hacer es considerar que eso no pasa, que el producto de las variables no conmuta, por lo que por ejemplo $x y ≠ y x$ (pero que el producto de las variables con los coeficientes sí conmuta, así que por ejemplo $x 3 y = 3 x y$). Al hacer eso se obtiene algo como los polinomios, pero en el que los monomios son palabras. A esos polinomios se los llama polinomios no conmutativos y, por ejemplo, para las variables $x, y$ con coeficientes en un cuerpo $K$ se denotan $K⟨x, y⟩$. En general si $X$ es un alfabeto, $K⟨X⟩$ es el conjunto de los polinomios no conmutativos sobre $X$.

Un problema de decisión que existe sobre los polinomios no conmutativos es: dado un conjunto $G$ de polinomios no conmutativos y un polinomio no conmutativo $f$, decidir si $f$ se puede escribir como combinación lineal de elementos de $G$ con coeficientes en $K⟨X⟩$, o escrito formalmente, determinar si vale que:
\[ ∃n ∈ ℕ, g_1, …, g_n ∈ G, f_1, …, f_n, f'_1, …, f'_n ∈ K⟨X⟩ : f = ∑_{i = 1}^n f_i g_i f'_i \text{.}\]

Este problema así planteado no es decidible, % Estaría bueno agregar una cita acá, pero no se a que
pero sí es semi-decidible. Esto significa que no hay un algoritmo que dice si vale la expresión o no, pero sí hay un algoritmo que en los casos en los que sí vale termina diciendo que sí y los casos en los que no vale puede no terminar.

Para trabajar en algoritmos para ese problema se considera el conjunto $(G)$ (que así se denota) de todos los polinomios no conmutativos que satisfacen ese $∃$ y se calcula algo, que se llama base de Gröbner de $(G)$, la cual puede ser finita o infinita computacionalmente enumerable. Son estos últimos casos los que hacen que el problema no sea computable.

Para calcular esas bases de Gröbner hay dos algoritmos destacados, llamados el algoritmo de Buchberger y el algoritmo F4. Estos algoritmos, al igual que mucha de la teoría, están inspirados en la teoría para resolver el problema análogo para los polinomios conmutativos. De hecho, muchos de los nombres de las cosas se usaron primero para el caso conmutativo y después se usó el mismo nombre para el caso no conmutativo. Por ejemplo, ``algoritmo de Buchberger'' en otros contextos se puede estar refiriendo al algoritmo para el caso conmutativo. En esta tesis siempre se va a estar hablando del caso no conmutativo. Para una explicación del caso conmutativo ver \cite{book:ideals-varieties-algorithms}.

De antes de esta tesis existen varias implementaciones del algoritmo de Buchberger en el caso no conmutativo, por ejemplo \cite{lib:GBNP, lib:DGPS}, % Está también NCAlgebra, pero no encontré como citarlo
y sólo una % Creo (Cristian puso un ? cuando lo dijo)
del algoritmo F4. Esta última fue desarrollada por Clemens Hofstalder en su tesis de maestría \cite{thesis:Hof20} en Python usando SageMath. Como esa implementación está hecha en python y no corre en paralelo, el objetivo original de esta tesis fue implementar en C++ esos algoritmos y hacerlos correr en paralelo.

La parte de implementar en C++ esos algoritmos se logró, pero hay una optimización que ayuda bastante y no se hizo. La parte de hacerlo correr en paralelo se hizo, pero solo para una parte y para la parte de la optimización que no se hizo tampoco se hizo nada en paralelo. Hacer esa optimización podría ser un trabajo para una tesis futura.

Toda esta exposición es autocontenida para facilitar su lectura, así que se incluyen algunas cosas que no son estrictamente del tema de la tesis, si no que son necesarias para los temas de la tesis.

\chapter{Preliminares}\label{cap:Preliminares}

En este capítulo se desarrolla toda la parte matemática, basándonos principalmente en la exposición de \cite{thesis:Hof20}. Primero se explican sistemas de re-escritura, después se introduce el álgebra libre, se prueba que es un anillo, se dan algunas definiciones y teoremas de anillos y se definen las bases de Gröbner y por último se explican y dan seudocódigos para los algoritmos de Buchberger y F4, que son los dos algoritmos importantes para calcular bases de Gröbner no conmutativas.

El capítulo incluye demostraciones de muchos de los teoremas, pero son salteables. En una primera lectura se pueden saltear tranquilamente, y en general todo el capitulo se puede leer por encima.

\section{Sistemas de re-escritura}

En esta sección se dan algunas definiciones y teoremas básicos de sistemas de re-escritura.

Sistemas de re-escritura trata básicamente sobre el estudio de relaciones entre objetos con la idea de usar las relaciones para convertir un objeto en otro. En general vamos a usar relaciones denotadas con algún símbolo con forma de flecha porque está la idea de que se \textsl{usa} la relación para convertir el objeto de la izquierda en el objeto de la derecha. Como recordatorio: una relación entre $A$ y $B$ es un subconjunto de $A × B$ y una relación sobre $A$ un subconjunto de $A^2$.

Para toda esta sección fijemos un conjunto $A$. Primero algunas operaciones muy comunes sobre relaciones.

\begin{definition}\label{def:operaciones relaciones}
  Dadas relaciones $→$ y $⟿$ sobre $A$ se define:
  \begin{itemize}
    \item $→ ∘ ⟿\ = \{(x, z) ∈ A^2 : ∃y ∈ A : x → y ∧ y ⟿ z\}$.
    \item $→^0\ = \{(x, x) : x ∈ A\}$.
    \item $→^{i + 1}\ =\ →^i ∘ →$.
    \item $→^*\ = ⋃_{i = 0}^∞ →^i$.
    \item $←\ = \{(y, x) ∈ A^2 : (x, y) ∈\ →\}$.
    \item $↔\ =\ → ∪ ←$.
  \end{itemize}

  Con $←$ y $↔$ podría haber problema para usarlas con una relación denotada por un símbolo que no tenga forma de flecha, pero en esta tesis las relaciones que se usan siempre se denotan con un símbolo con forma de flecha.
\end{definition}

Con está definición se define automáticamente también $↔^*$ que se llama la clausura reflexiva y transitiva de $→$.

Para el resto de la sección fijemos una relación $→$ sobre $A$.

Como vamos a hablar de usar $→$ para transformar un objeto en otro, es útil la proxima definición que permite hablar de la forma normal de un objeto como el elemento al que se llega aplicando $→$ hasta que no se puede más.

\begin{definition}\label{def:forma normal}
  Dados $a, b ∈ A$ se define:
  \begin{itemize}
    \item $a$ está en forma normal $⇔ ∄x ∈ A : a → x$.
    \item $b$ es forma normal de $a ⇔ a →^* b ∧ b$ está en forma normal.
    \item $a$ tiene forma normal $⇔ ∃x ∈ A : x$ es forma normal de $a$.
    \item $a ↓ b ⇔ ∃x ∈ A : a →^* x ∧ b →^* x$.
  \end{itemize}

  Con $↓$ pasa lo mismo que con $←$ y $↔$ de que podría ser problemático usarla para una relación denotada con un símbolo que no tenga forma de flecha, pero en esta tesis eso no pasa.
\end{definition}

También necesitamos definir las siguientes propiedades de las relaciones.

\begin{definition}\
  \begin{itemize}
    \item $→$ es confluente $⇔ ∀x, y, z ∈ A : x →^* y ∧ x →^* z ⇒ y ↓ z$.
    \item $→$ es Church-Rosser $⇔ ∀x, y ∈ A : x ↔^* y ⇔ x ↓ y$.
    \item $→$ es normalizante $⇔ ∀x ∈ A : x$ tiene forma normal.
    \item $→$ es terminante $⇔ ∄X ∈ A^ℕ : ∀i ∈ ℕ : X_i → X_{i + 1}$.
  \end{itemize}
\end{definition}

Sobre estas definiciones y propiedades los siguientes teoremas y lemas:

\begin{lemma}\label{lemma:→* como ∃}
  Sean $a, b ∈ A$. Entonces
  \[ a →^* b ⇔ ∃n ∈ ℕ ∪ \{0\}, x_0, …, x_n ∈ A : x_0 = a ∧ x_n = b ∧ ∀i ∈ {1, …, n} : x_{i-1} → x_i \text{.}\]
  \qed
\end{lemma}

\begin{lemma}\label{lemma:↔* min equiv que contiene a →}
  $↔^*$ es la mínima relación de equivalencia que contiene a $→$.
  \qed
\end{lemma}

\begin{theorem}\label{thm:terminante ⇒ normalizante}
  $→$ es terminante $⇒ →$ es normalizante.
  \qed
\end{theorem}

\begin{theorem}\label{thm:confluente ⇔ Church-Rosser}
  $→$ es confluente $⇔ →$ es Church-Rosser.
  \qed
\end{theorem}

\section{Álgebra libre}

El álgebra libre es básicamente el conjunto de los polinomios no conmutativos con sus operaciones. En los polinomios no conmutativos los monomios son palabras y la única operación interna de los monomios (entre monomios y que devuelve monomios) es la multiplicación, que equivale a la concatenación de palabras.

\begin{definition}
  Sea $X$ un alfabeto finito. Se define la estructura $(⟨X⟩, ·)$ de la siguiente manera:
  \begin{itemize}
    \item $⟨X⟩$ es el conjunto palabras finitas sobre $X$.
    \item $· : ⟨X⟩^2 → ⟨X⟩$ es la concatenación.
  \end{itemize}
  A $(⟨X⟩, ·)$ se lo llama el monoide libre sobre $X$, a los elementos de $⟨X⟩$ se los llama monomios libres sobre $X$ y a $·$ el producto de $⟨X⟩$.

  Si $X = \{x_1, …, x_n\}$ escribimos $⟨x_1, …, x_n⟩$ un lugar de $⟨X⟩$.
\end{definition}

Por ejemplo, si $X = \{a, b, c\}$, algunos monomios son:

\begin{align*}
  m_0 &= abbcb \text{,} \\
  m_1 &= bbc \text{,} \\
  m_2 &= bcb \text{,} \\
  m_3 &= ε \text{,} \\
  m_1 · m_2 &= bccbcb \text{.}
\end{align*}

El $ε$ de $m_3$ es la palabra vacía. Notar que $m_1 ≠ m_2$ ya que el producto es no conmutativo.

\

Para todo el resto de la tesis fijemos $X$ un alfabeto finito. En estos monomios, al igual que en los conmutativos, se puede hablar de que un monomio divida a otro, pero acá que un monomio divida a otro es equivalente a que sea una sub-palabra.

\begin{definition}
  Sean $v, w ∈ ⟨X⟩$. Se define
  \begin{itemize}
    \item $v | w ⇔ ∃a , b ∈ ⟨X⟩ : w = avb$.
  \end{itemize}
  Y cuando $v | w$ se dice que $v$ divide a $w$.
\end{definition}

En el ejemplo de antes tenemos que $m_1 | m_0$ ya que $m_0 = a m_1 b$.

De lo que es un poco más complicado hablar acá es de el resultado de la división porque tendría que haber dos resultados, el $a$ y el $b$ de la definición, así que no vamos a hablar de dividir un monomio en otro ni escribir divisiones entre monomios.

Mas adelante va a ser necesario tener un orden entre los elementos de $⟨X⟩$, sino que con que satisfaga la siguiente definición alcanza.

\begin{definition}\label{def:buen orden monomial}
  Sea $≤$ un orden total sobre $⟨X⟩$. Se define que $≤$ es un buen orden monomial si y solo si:
  \begin{enumerate}
    \item $∀v, w, a, b ∈ ⟨X⟩ : v ≤ w ⇒ avb ≤ awb$.
    \item $∀S ⊆ ⟨X⟩ : S ≠ ∅ ⇒ S$ tiene mínimo elemento con respecto a $≤$.
  \end{enumerate}
\end{definition}

Definir esto así en general va a permitir que todo el trabajo sea sobre cualquier orden con las propiedades mínimas necesarias.

Un ejemplo de orden que cumple con esta definición es el siguiente.

\begin{definition}
  Fijemos $X = \{x_1, …, x_n\}$ y un orden total sobre $X$: $x_1 ≤ … ≤ x_n$, el cual se extiende (como es usual) de forma lexicográfica a $⟨X⟩$. El orden lexicográfico por grado $ ≤_{deglex}$ sobre $⟨X⟩$ se define así:
  \[ a ≤_{deglex} b ⇔ |a| < |b| ∨ (|a| = |b| ∧ a ≤ b) \text{.}\]
\end{definition}

O sea, el orden lexicográfico por grado ordena primero por cardinalidad, también llamado grado, y desempata con el orden lexicográfico. Por ejemplo, tenemos que $bc ≤_{deglex} abb$, $aabbc ≤_{deglex} abbcc$ y $ε ≤_{deglex} a$. Se puede probar fácilmente que este orden es un buen orden monomial.

\

A partir de ahora fijamos un buen orden monomial $≤$ y vamos a usar $<$, $≥$ y $>$ como se usan habitualmente.

La siguiente propiedad es consecuencia directa de la definición de buen orden monomial.

\begin{theorem}\label{thm:≤ no sucesiones dec inf}
  La relación $≤$ no tiene sucesiones estrictamente decrecientes infinitas.
  \qed
\end{theorem}

Ahora pasemos a hablar de sumar monomios entre sí para tener polinomios no conmutativos.

\begin{definition}
  Sea $R$ un anillo conmutativo. Se define la $R$-álgebra libre sobre $X$ como el conjunto
  \[ R⟨X⟩ = \{∑_{i = 1}^n c_i w_i : c_1, …, c_n ∈ R, w_1, …, w_n ∈ ⟨X⟩\}\]

  \noindent con las siguientes operaciones.

  \noindent La suma en $R⟨X⟩$ se define de la manera esperable. El producto por escalares de define como
  \[ c (∑_{i = 1}^n c_i w_i) = (∑_{i = 1}^n c c_i w_i) \text{.}\]

  \noindent El producto entre elementos de $R⟨X⟩$ se define como
  \[ (∑_{i = 1}^n c_i w_i) · (∑_{i = 1}^m c'_i w'_i) = ∑_{i = 1}^n ∑_{j = 1}^m c_i c'_j w_i w'_j \text{.}\]

  Y a los elementos de $R⟨X⟩$ se los llama polinomios no conmutativos.
  % Esta definición creo que hay que mejorarla, pero no se como
\end{definition}

Algunos ejemplos de polinomios no conmutativos en $ℚ⟨a, b, c⟩$ son los siguientes:
\begin{align*}
f_0 &= a \text{,} \\
f_1 &= ab + cb \text{,} \\
f_2 &= 3 abb + 4 bcca - 2 acab \text{.}
\end{align*}

\noindent Notar que $f_1 ≠ ab + bc$ ya que el producto es no conmutativo.

Sobre los polinomios no conmutativos se hacen las siguientes definiciones que después vamos a usar mucho.

\begin{definition}\label{def:cosas de polinomios}
  Sean $R$ un anillo conmutativo, $f ∈ R⟨X⟩$, $c_1, …, c_n ∈ R - \{0\}$, $w_1, …, w_n, w ∈ ⟨X⟩$, $f = ∑_{i = 1}^n c_i w_i$ y $≤$ un buen orden monomial. Se definen:
  \begin{itemize}
    \item el coeficiente de $w$ en $f$ como $f_w = \left\{\begin{array}{ll} w = w_i → c_i \\ \text{si no} → 0  \end{array} \right.$.
    \item el soporte de $f$ como el conjunto $\sop(f) = \{w_1, …, w_n\}$.
    \item el monomio principal de $f$ como $\lm_≤(f) = \max_≤(\sop(f))$.
    \item el coeficiente principal de $f$ como $\lc_≤(f) = f_{\lm(f)}$.
    \item el término principal de $f$ como $\lt_≤(f) = \lc_≤(f) · \lm_≤(f)$.
    \item $\tail_≤(f) = f - \lt_≤(f)$.
    \item $f$ es mónico $⇔ \lc_≤(f) = 1$.
  \end{itemize}

  Los nombres lm, lc, lt y tail vienen del inglés leading monomial, leading coefficient, leading term y tail respectivamente.
\end{definition}

Sobre estas definiciones valen muchas propiedades fáciles de probar, como por ejemplo $\lm(f) \lm(f') = \lm(f f')$, que durante el resto de la tesis vamos a usar mucho, pero no las vamos a probar porque es tedioso y aburrido.

Para varias cosas va a ser necesario comparar no solo monomios sino también polinomios, así que el orden $≤$ se extiende a $K⟨X⟩$ así:

\begin{definition}
  Sean $f, g ∈ K⟨X⟩$. Se define que $f < g$ si y solo si vale alguna de las siguientes:
  \begin{enumerate}
    \item $f = 0 ∧ g ≠ 0$.
    \item $\lm_≤(f) < \lm_≤(g)$.
    \item $\lm(f) = \lm(g) ∧ \tail(f) < \tail(g)$.
  \end{enumerate}
  Y como es usual definimos $f ≤ g$ si y solo si $f < g ∨ f = g$.
\end{definition}

Dicho en palabras, el orden en los polinomios es orden lexicográfico con el polinomio visto como una lista de monomios, sin coeficientes, ordenada de mayor a menor. Por ejemplo, tenemos estas desigualdades en $K⟨X⟩$:
\begin{align*}
  a &< ab \text{,} \\
  bcc + c &< bcc + abb \text{,} \\
  ac &< ac + aa \text{.}
\end{align*}

Si bien a este $<$ lo estamos llamando (y lo vamos a seguir llamando) orden, en realidad no es un orden sino un preorden porque cuando solo cambian los coeficientes entre un polinomio y otro, ninguno de los polinomios es menor que el otro.

\begin{theorem}
  La relación $<$ en $K⟨X⟩$ es un preorden parcial.
  \qed
\end{theorem}

Y además, el polinomio $0$ es el mínimo.

\begin{lemma}\label{lemma:0 es mínimo}
  $0$ es el mínimo de $<$.
  \qed
\end{lemma}

En este orden, al igual que para los monomios, vale que no hay sucesiones estrictamente decrecientes infinitas.

\begin{lemma}\label{lemma:≤ en KX no sucesiones dec inf}
  La relación $≤$ en $K⟨X⟩$ no tiene sucesiones estrictamente decrecientes infinitas.
\end{lemma}
\begin{proof}
  Supongamos que existen sucesiones estrictamente decrecientes infinitas. Tomemos una sucesión estrictamente decreciente infinita $P$, o sea que valga $P_1 > P_2 > P_3 > …$ que minimice $\lm(P_1)$. Tomar este mínimo es posible por el \cref{thm:≤ no sucesiones dec inf} que dice que no hay sucesiones estrictamente decrecientes infinitas en los monomios.

  Notemos que:

  \begin{fact}\label{fact:≤ en KX no sucesiones dec inf:1}
    $∀i ∈ ℕ : \lm(P_i) = \lm(P_1)$.
  \end{fact}
  En efecto, no puede ser $\lm(P_i) < \lm(P_1)$ porque entonces $P_i, P_{i + 1}, …$ sería una sucesión estrictamente decreciente infinita que rompería la minimalidad de $\lm(P_1)$ y no puede ser $\lm(P_i) > \lm(P_1)$ porque eso implicaría $P_i > P_1$ y eso contradice que $P$ sea decreciente.

  \begin{fact}\label{fact:≤ en KX no sucesiones dec inf:2}
    $\tail(P_1), \tail(P_2), …$ es una sucesión estrictamente decreciente infinita.
  \end{fact}
  Esto vale porque al aplicar la definición del orden polinomial sobre $P_1 > P_2 > P_3 > …$ y usar la \cref{fact:≤ en KX no sucesiones dec inf:1} queda $\tail(P_1) > \tail(P_2) > \tail(P_3) > …$

  Como claramente $0$ es un mínimo:

  \begin{fact}\label{fact:≤ en KX no sucesiones dec inf:3}
    $P_1 ≠ 0$.
  \end{fact}

  Sin embargo, la \cref{fact:≤ en KX no sucesiones dec inf:2} contradice la minimalidad de $\lm(P_1)$ ya que por la \cref{fact:≤ en KX no sucesiones dec inf:3} vale que $\lm(\tail(P_1)) < \lm(P_1)$.

\end{proof}

La estructura del álgebra libre es un anillo, lo cual nos va a ser muy útil por muchas definiciones y teoremas que ya existen sobre los anillos.

\begin{theorem}
  Sea $R$ un anillo conmutativo. Entonces
  \[ (R⟨X⟩, +, ·)\text{ es un anillo} \text{.}\]
  \qed
\end{theorem}
% Tendría que decir algo de que no se incluye la demostración porque es fácil?

Las definiciones y teoremas sobre anillos que vamos a usar las vamos a repasar a continuación para no tener que estar buscándolos en otro lado.

\begin{definition}\label{def:ideal}
  Sean $R$ un anillo e $I ⊆ R$. Se define que $I$ es un ideal de $R$ si y solo si
  \begin{enumerate}
    \item $I ≠ ∅$.
    \item $∀a, b ∈ I : a + b ∈ I$.
    \item $∀a ∈ I, r, r' ∈ R : r a r' ∈ I$.
  \end{enumerate}
\end{definition}

\begin{definition}\label{def:ideal gen}
  Sean $R$ un anillo y $G ⊆ R$. Se define la notación $(G)$ como
  \[ (G) = \{∑_{i = 1}^n c_i g_i c_i' : n ∈ ℕ ∪ \{0\}, g_1, …, g_n ∈ G, c_1, …, c_n, c_1', …, c_n' ∈ R\} \text{.}\]
\end{definition}

Los ideales son básicamente conjuntos cerrados por la suma y por el producto por cualquier elemento del anillo. A $(G)$ se lo llama el ideal generado por $G$, ya que como dice el siguiente teorema, es un ideal.

\begin{theorem}
  Sean $R$ un anillo y $G ⊆ R$. Entonces
  \[ (G)\text{ es un ideal de }R \text{.}\]
  \qed
\end{theorem}

Cada ideal define una clase de equivalencia en el anillo, la cual se llama congruencia módulo el ideal.

\begin{definition}\label{def:congruencia mod ideal}
  Sean $R$ un anillo e $I ⊆ R$. Se define $≡_I$ como relación en $R$ así:
  \[ a ≡_I b ⇔ a - b ∈ I \text{.}\]
\end{definition}

\begin{theorem}\label{thm:congruencia mod ideal es equivalencia}
  Sean $R$ un anillo y $I ⊆ R$ un ideal. Entonces
  \[ ≡_I \text{es una relación de equivalencia} \text{.}\]
  \qed
\end{theorem}

Esta congruencia es parecida a la de los enteros módulo un natural (de hecho, la congruencia de los enteros módulo un natural es un subcaso de esta tomando como ideal a todos los múltiplos del módulo). Vale que la clase de equivalencia del $0$ es el propio ideal:

\begin{lemma}\label{lemma:en ideal ⇔ congruente 0}
  Sean $R$ un anillo, $I ⊆ R$ un ideal y $a ∈ R$. Entonces
  \[ a ∈ I ⇔ a ≡_I 0 \text{.}\]
  \qed
\end{lemma}

Por último, con respecto al ideal generado por un conjunto, tenemos los siguientes dos lemas.

\begin{lemma}\label{lemma:gen G = gen G U a con a ∈ gen G}
  Sean $R$ un anillo, $G ⊆ R$ y $a ∈ (G)$. Entonces
  \[ (G) = (G ∪ \{a\}) \text{.}\]
  \qed
\end{lemma}

\begin{lemma}\label{lemma:sub gen y sub gen ⇒ eq}
  Sean $R$ un anillo y $G, G' ⊆ R$. Entonces
  \[ G ⊆ (G') ∧ G' ⊆ (G) ⇒ (G) = (G') \text{.}\]
  \qed
\end{lemma}

Ahora volvemos al álgebra libre y a partir de ahora fijamos un cuerpo $K$.

Con las definiciones que tenemos ahora la pregunta que planteábamos en la introducción se puede escribir como: dado un conjunto finito $G ⊆ K⟨X⟩$ y un elemento $f ∈ K⟨X⟩$, determinar si $f ∈ (G)$.

Si bien $(G)$ para un anillo general está definido usando combinaciones lineales con coeficientes en el anillo, para el álgebra libre va a ser útil la siguiente equivalencia.

\begin{lemma}\label{lemma:(G) equiv}
  Sea $G ⊆ K⟨X⟩$. Entonces
  \[ (G) = \{∑_{i = 0}^n c_i m_i g_i m'_i : n ∈ ℕ ∪ \{0\}, c_1, …, c_n ∈ K, m_1, …, m_n, m'_1, …, m'_n ∈ ⟨X⟩, g_1, …, g_n ∈ G\} \text{.}\]
\end{lemma}
\begin{proof} Lo probamos probando que $f$ esta en uno si y solo si está en el otro.
  \begin{description}
    \item[Ida ($⇒$):] Supongamos $f ∈ (G)$. Sean:

    \begin{itemize}
      \item $g_1, …, g_n ∈ G, c_1, …, c_n, c_1', …, c_n' ∈ K⟨X⟩$ tales que $f = ∑_{i = 1}^n c_i g_i c_i'$, los cuales existen por lo que estamos suponiendo y la definición de $(G)$.
      \item Para cada $i ∈\{1, …, n\}$:
      \begin{itemize}
        \item $k_{i, 1}, …, k_{i, n_i} ∈ K, m_{i, 1}, …, m_{i, n_i} ∈ ⟨X⟩$ tales que $c_i = ∑_{j = 1}^{n_i} k_{i, j} m_{i, j}$.
        \item $k'_{i, 1}, …, k'_{i, n'_i} ∈ K, m'_{i, 1}, …, m'_{i, n'_i} ∈ ⟨X⟩$ tales que $c'_i = ∑_{j = 1}^{n'_i} k'_{i, j} m'_{i, j}$.
      \end{itemize}
    \end{itemize}

    Con esto tenemos:
    \begin{DispWithArrows*}
      f &= ∑_{i = 1}^n c_i g_i c_i' \\
        &= ∑_{i = 1}^n (∑_{j = 1}^{n_i} k_{i, j} m_{i, j}) g_i (∑_{j = 1}^{n'_i} k'_{i, j} m'_{i, j}) \\
        &= ∑_{i = 1}^n ∑_{j = 1}^{n_i} ∑_{j' = 1}^{n'_i} k_{i, j} k'_{i, j'} m_{i, j} g_i m'_{i, j'} \text{.}
    \end{DispWithArrows*}

    Esto último es una expresión que se transforma a la forma que queremos.

    \item[Vuelta ($⇐$):] La vuelta es cierta porque, como $c_i m_i, m'_i ∈ K⟨X⟩$, es un caso particular de la definición.
  \end{description}
\end{proof}

\section{Reducción de polinomios}

Ahora vamos a definir una relación de reducción en los polinomios no conmutativos y acá es donde entra lo de sistemas de re-escritura. La relación que vamos a definir depende del orden monomial y de un conjunto de polinomios no conmutativos y lo que va a hacer es tratar de achicar un polinomio con los elementos del conjunto para que llegue a quedar lo menor posible con respecto al orden polinomial.

\begin{definition}\label{def:reducciones}
  Sean $G ⊆ K⟨X⟩$, $g ∈ K⟨X⟩ - \{0\}$, $a, b ∈ ⟨X⟩$ y $f, f' ∈ K⟨X⟩$. Se define la relacione $→_{≤, G}$ del siguiente modo:
  \[ f →_{≤, G} f' ⇔ ∃a, b ∈ ⟨X⟩, g ∈ G : \lm_≤(agb) ∈ \sop(f) ∧ f' = f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb \text{.} \]
  Y cuando vale $f →_{≤, G}f'$ se dice que $f$ se reduce a $f'$ (via $G$).
\end{definition}

Por ejemplo, si tenemos $f = 2 cba + 3 dbcda$, $g_0 = a - bcd$ y $G = \{g_0\}$ tenemos:

\[f →_{≤, G} f + 3d g_0 a = 3 daa + 2 cba \text{.}\]

\

Como teníamos la \cref{def:operaciones relaciones} y la \cref{def:forma normal} quedan definidas automáticamente las relaciones $→^*_{≤, G}$, $←_{≤, G}$, $↓_{≤, G}$, etc.

A continuación la prueba de que esta relación efectivamente achica.

\begin{theorem}\label{thm:→ achican}
  Sean $G ⊆ K⟨X⟩$, $g ∈ K⟨X⟩ - \{0\}$, $a, b ∈ ⟨X⟩$ y $f, f' ∈ K⟨X⟩$. Entonces
  \[ f →_{≤, G} f' ⇒ f' < f \text{.} \]
\end{theorem}
\begin{proof}
  Supongamos el antecedente. Por definición de reducciones vale que:
  \begin{enumerate}[label=(\roman*)]
    \item $\lm_≤(agb) ∈ \sop(f)$.
    \item $f' = f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb$.
  \end{enumerate}

  \noindent Escribamos $f = ∑_{i = 1}^n c_i m_i$ con $c_1, …, c_n ∈ K$, $m_1, …, m_n ∈ ⟨X⟩, m_1 > m_2 > ⋯ > m_n$.

  Sea $i$ tal que $m_i = \lm_≤(agb)$, el cual existe por (i).

  Notar también que:

  \begin{enumerate}[label=(\roman*)]
    \setcounter{enumi}{2}
    \item $m_i = \lm_≤(\frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb)$.
  \end{enumerate}

  (iii) implica que los términos $c_1 m_1, c_2 m_2, …, c_{i-1}, m_{i-1}$ son iguales en $f$ y en $f'$ y no hay nada más en el medio, porque $f'$ es $f$ con cosas menores o iguales a $\lm_≤(\frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb)$ restadas (por (ii)).

  Además, por (iii) y (ii) vale que $f'_{m_i} = 0$, por ende, el término que sigue después de $m_{i-1}$ (si es que hay) es menor que $m_i$.

  Combinando que los primeros $i - 1$ términos son iguales y el $i$ es menor en $f'$ que en $f$, por la definición de $<$ para polinomios vale que $f' < f$.

\end{proof}

La relación $→_{≤, G}$ tiene varias propiedades que van a ser útiles y que se prueban a continuación.

\begin{lemma}\label{lemma:suma →↓}
  Sean $G ⊆ K⟨X⟩$ y $f_0, f_1, f ∈ K⟨X⟩$. Entonces
  \[ f_0 →_{≤, G} f_1 ⇒ f_0 + f ↓_{≤, G} f_1 + f \text{.}\]
\end{lemma}
\begin{proof}
  Supongamos el antecedente $f_0 →_{≤, G} f_1$.

  Sean $g ∈ G, a, b ∈ K⟨X⟩$ tales que $f_1 = f_0 - \frac{{f_0}_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb$, los cuales existen por definición de $→_{≤, G}$.

  Dividamos en casos según la pertenencia de $\lm_≤(agb)$ a $\sop(f)$:

  \begin{description}
    \item[Caso $\lm_≤(agb) ∉ \sop(f)$:] Partiendo de la condición de $a, g, b$ hacemos los siguiente:

    \begin{DispWithArrows*}
      &f_1 = f_0 - \frac{{f_0}_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} \\
      & ⇒ f_1 + f = f_0 - \frac{{f_0}_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} + f \Arrow{Definición de $→_{≤, a, g, b}$}\\
      & ⇒ f_0 + f →_{≤, a, g, b} f_1 + f \\
      & ⇒ f_0 + f →_{≤, G} f_1 + f \Arrow{Definición de $↓$, ambos se reducen a $f_1 + f$}\\
      & ⇒ f_0 + f ↓_{≤, G} f_1 + f \text{.}
    \end{DispWithArrows*}

    \item[Caso $\lm_≤(agb) ∈ \sop(f)$:]\
    \begin{description}
      \item[Subcaso $f_{\lm_≤(agb)} = -{f_0}_{\lm_≤(agb)}$]

      Este es el caso en el que $\lm_≤(agb)$ se cancela en la suma $f_0 + f$. Y como además $\lm_≤(agb) ∉ \sop(f_1)$ por el antecedente, tenemos:

      \begin{DispWithArrows*}
        &f_1 + f →_{≤, G} f_1 + f - \frac{f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \Arrow{Subcaso} \\
        & ⇒ f_1 + f →_{≤, G} f_1 + f + \frac{{f_0}_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \Arrow{Condición de $g$, $a$ y $b$} \\
        & ⇒ f_1 + f →_{≤, G} f_0 + f \\
        & ⇒ f_1 + f ↓_{≤, G} f_0 + f \text{.}
      \end{DispWithArrows*}

      \item[Subcaso $f_{\lm_≤(agb)} ≠ -{f_0}_{\lm_≤(agb)}$:] En este caso $\lm_≤(agb)$ no se cancela en la suma $f_0 + f$, así que podemos aplicar $→_{≤, G}$:
      \begin{DispWithArrows*}
        &f_0 + f →_{≤, G} f_0 + f - \frac{(f_0 + f)_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \\
        & ⇒ f_0 + f →_{≤, G} f_0 + f - \frac{{f_0}_{\lm_≤(agb)} + f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \\
        & ⇒ f_0 + f →_{≤, G} f_0 + f - \frac{{f_0}_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb - \frac{f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \Arrow{Condición de $g$, $a$ y $b$}\\
        & ⇒ f_0 + f →_{≤, G} f_1 + f - \frac{f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb  \text{.}
      \end{DispWithArrows*}
        Llamemos (i) a este último resultado.

      Además por el caso y el hecho de que $lm_≤(agb) ∉ \sop(f_1)$ también tenemos $\lm_≤(agb) ∈ \sop(f_1 + f)$, entonces:
      \begin{DispWithArrows*}
        &f_1 + f →_{≤, G} f_1 + f - \frac{(f_1 + f)_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \\
        & ⇒ f_1 + f →_{≤, G} f_1 + f - \frac{{f_1}_{\lm_≤(agb)} + f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \Arrow{$\lm_≤(agb) ∉ \sop(f_1)$ porque $f_0 →_{≤, G} f_1$ y definición de $→_{≤, G}$}\\
        & ⇒ f_1 + f →_{≤, G} f_1 + f - \frac{0 + f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \\
        & ⇒ f_1 + f →_{≤, G} f_1 + f - \frac{f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \text{.}
      \end{DispWithArrows*}
        Llamemos (ii) a este último resultado.

      Por (i) e (ii) tenemos $f_0 + f ↓_{≤, G} f_0' + f$.
    \end{description}
  \end{description}
\end{proof}

\begin{lemma}\label{lemma:prod mon →}
  Sean $G ⊆ K⟨X⟩$, $f, f' ∈ K⟨X⟩$, $c ∈ K$ y $m, m' ∈ ⟨X⟩$. Entonces
  \[ f →_{≤, G} f' ⇒ c m f m' →_{≤, G} c m f' m' \text{.}\]
\end{lemma}
\begin{proof} Supongamos el antecedente.

  Sean $g ∈ G$ y $a, b ∈ ⟨X⟩$ tales que $f' = f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb$, los cuales existen por definición de $→_{≤, G}$.

  Tenemos ahora:
  \begin{DispWithArrows*}
    &f' = f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb \\
    & ⇒ c m f' m' = c m (f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb) m \\
    & ⇒ c m f' m' = c m f m' - \frac{{(c m f m')}_{\lm_≤(c ma g bm')}}{\lc_≤(g)}c ma g bm' \Arrow{Definición $→_{≤, ma, g, bm'}$} \\
    & ⇒ c m f m' →_{≤, ma, g, bm'} c m f' m' \Arrow{Definición $→_{≤, G}$} \\
    & ⇒ c m f' m' →_{≤, G} c m f m' \text{.}
  \end{DispWithArrows*}

\end{proof}

Recordar que $≡_{(G)}$ es la congruencia módulo un ideal definida en la \cref{def:congruencia mod ideal}. Un lema importante es que la clausura reflexo transitiva de $→_{≤, G}$ es una congruencia.

\begin{lemma}\label{lemma:→^* = ≡}
  Sea $G ⊆ K⟨X⟩$. Entonces
  \[ ↔^*_{≤, G}\ =\ ≡_{(G)} \text{.}\]
\end{lemma}
\begin{proof} Probemos las dos inclusiones.
  \begin{description}
    \item[Prueba de $↔^*_{≤, G}\ ⊆\ ≡_{(G)}$:] Como $↔^*_{≤, G}$ y $≡_{(G)}$ son relaciones de equivalencia y además por el \cref{lemma:↔* min equiv que contiene a →} $↔^*_{≤, G}$ es la mínima relación de equivalencia que contiene a $→_{≤, G}$, alcanza con probar $→_{≤, G}\ ⊆\ ≡_{(G)}$. Para eso supongamos $f →_{≤, G} f'$ y probemos $f ≡_{(G)} f'$.

    Sean $g ∈ G, a, b ∈ K⟨X⟩$ tales que $\lm(agb) ∈ f$ y $f' = f - \frac{f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb$, los cuales existen por definición de $→_{≤, G}$. Tenemos:
    \begin{DispWithArrows*}
      &f ≡_{(G)} f' \\
      & ⇔ f - f' ∈ (G) \\
      & ⇔ f - (f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb) ∈ (G) \\
      & ⇔ \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb ∈ (G) \text{.}
    \end{DispWithArrows*}
    Y esto último es claramente cierto por la definición de ideal (\cref{def:ideal}).

    \item[Prueba de $≡_{(G)}\ ⊆\ ↔^*_{≤, G}$:] Supongamos $f ≡_{(G)} f'$ y probemos $f ↔^*_{≤, G} f'$. Sean:
    \begin{itemize}
      \item $g = f - f'$.
      \item $m_1, …, m_n, m_1', …, m_n' ∈ ⟨X⟩$, $g_1, …, g_n ∈ G$ tales que $g = ∑_{i = 1}^n m_i g_i m_i'$, los cuales existen porque por definición de $≡_G$ tenemos $g ∈ (G)$ y por el \cref{lemma:(G) equiv}.
      \item $f_0 = f$.
      \item Para cada $i ∈ \{1, …, n\}$: $f_i = f_{i - 1} - m_i g_i m_i'$.
    \end{itemize}

    Tenemos entonces:
    \begin{DispWithArrows*}
      &∀i ∈ \{1, …, n\} : g_i →_{≤, G} 0 \Arrow{\cref{lemma:prod mon →}} \\
      & ⇒ ∀i ∈ \{1, …, n\} : m_i g_i m_i' →_{≤, G} 0 \Arrow{\cref{lemma:suma →↓}} \\
      & ⇒ ∀i ∈ \{1, …, n\} : m_i g_i m_i' + f_i ↓_{≤, G} 0 + f_i \Arrow{Definición de los $f_i$}\\
      & ⇒ ∀i ∈ \{1, …, n\} : f_{i - 1} ↓_{≤, G} f_i \\ % Debería agregar un lema para este paso?
      & ⇒ ∀i ∈ \{1, …, n\} : f_{i - 1} ↔^*_{≤, G} f_i \\
      & ⇒ f_0 ↔^*_{≤, G} f_n \Arrow{$f_n = f - g = f'$} \\
      & ⇒ f ↔^*_{≤, G} f_n \text{.}
    \end{DispWithArrows*}

  \end{description}
\end{proof}

\begin{lemma}\label{thm:→ mantiene pertenencia a ideal}
  Sean $G ⊆ K⟨X⟩, f, f' ∈ K⟨X⟩$. Entonces
  \[ f →^*_{≤, G} f' ⇒ (f ∈ (G) ⇔ f' ∈ (G)) \text{.}\]
\end{lemma}
\begin{proof}
  Si asumimos $f →^*_{≤, G} f'$, tenemos por el \cref{lemma:→^* = ≡} que $f ≡_{(G)} f'$ y entonces por el \cref{lemma:en ideal ⇔ congruente 0} vale que $f ∈ (G) ⇔ f' ∈ (G)$.
\end{proof}

\begin{theorem}
  Sea $G ⊆ K⟨X⟩$. Entonces
  \[ →_{≤, G} \text{ es terminante.} \]
\end{theorem}
\begin{proof}
  Lo vamos a demostrar por contradicción. Supongamos que $→_{≤, G}$ no es terminante. Por definición podemos tomar una sucesión $P ∈ K⟨X⟩^ℕ$ tal que:

  \[ ∀i ∈ ℕ : P_i →_{≤, G} P_{i+1}. \]

  \noindent Por el \cref{thm:→ achican} tenemos que:

  \[ ∀i ∈ ℕ : P_i > P_{i+1}. \]

  \noindent Pero esto contradice el \cref{lemma:≤ en KX no sucesiones dec inf} que dice que no hay sucesiones estrictamente decrecientes infinitas en $K⟨X⟩$.
\end{proof}

El siguiente teorema caracteriza las formas normales de $→$.

\begin{theorem}
  Sean $G ⊆ K⟨X⟩, f ∈ K⟨X⟩$. Entonces
  \[ f\text{ está en forma normal con respecto a} →_{≤, G} ⇔ ∄g ∈ G, m ∈ \sop(f) : \lm(g) | m \text{.}\]
\end{theorem}
\begin{proof}
  Por contradicción, supongamos que tenemos $g ∈ G, m ∈ \sop(f)$ tales que $\lm(g) ∈ \sop(f)$.

  Sean $a, b ∈ ⟨X⟩$ tal que $m = agb$, los cuales existen por la definición de divisibilidad.

  Entonces tenemos $f →_{≤, G} f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb$ y por ende $f$ no está en forma normal.

\end{proof}

Probamos que $→_{≤, G}$ es terminante, pero no que sea confluente, ya que no lo es. Por ejemplo, siguiendo con el mismo ejemplo de antes en el que ya teníamos $f = 2 cba + 3 dbcda$, $g_0 = a - bcd$, si ahora agregamos $g_1 = b - cda$ y $G = \{g_0, g_1\}$ tenemos, como antes:

\[f →_{≤, G} f + 3d g_0 a = 3 daa + 2 cba \text{.}\]

\noindent Y también:

\[f →_{≤, G} f - 3db g_1 = 3 dbb + 2 cba = f'' \text{.}\]

\noindent Pero ninguno monomio principal de $G$ divide a ninguno monomio de $f'$ o $f''$ así que $f'$ y $f''$ no se pueden reducir mas.

Que fuera confluente sería muy útil porque significaría que para cualquier clase de equivalencia de $≡_{(G)}$ podríamos siempre llegar a una misma forma normal y así poder determinar si dos elementos son equivalentes (y en particular poder determinar si un elemento está en el ideal viendo si se llega a $0$ como la forma normal).

Los casos en los que sí es confluente se llaman bases de Gröbner y lo que vamos a hacer después es calcular una base de Gröbner de un ideal generado por un conjunto que no es base de Gröbner.

\begin{definition}\label{def:base de Gröbner}
  Sean $I$ un ideal de $K⟨X⟩$ y $G ⊆ K⟨X⟩$. Se define que
  \[G\text{ es una base de Gröbner de }I ⇔ (G) = I ∧ →_{≤, G}\text{ es confluente} \text{.} \]
  Además se dice que ``$G$ es una base de Gröbner'' si lo es de algún ideal.
\end{definition}

Una consecuencia directa de la definición es el siguiente lema.

\begin{lemma}\label{lemma:→ gröbner es Church-Rosser}
  Sea $G$ una base de Gröbner. Entonces
  \[→_{≤, G}\text{ es Church-Rosser.}\]
\end{lemma}
\begin{proof}
  Es una aplicación directa del \cref{thm:confluente ⇔ Church-Rosser}.
\end{proof}

Como en algunos casos $→_{≤, G}$ no es confluente, la siguiente definición nos permite hablar más cómodamente de una forma normal de un elemento (tanto en las definiciones y teoremas como en los algoritmos).

\begin{definition}\label{def:reductor}
  Sea $e_≤ : 𝒫(K⟨X⟩) → K⟨X⟩ → K⟨X⟩$. Se define que
  \[ e_≤\text{ es un reductor }⇔ ∀G ⊆ K⟨X⟩, f ∈ K⟨X⟩ : e_≤(G)(f)\text{ es forma normal de }f\text{ con respecto a }→_{≤, G} \text{.} \]
\end{definition}

Un ejemplo de reductor podría calcularse con el siguiente seudocódigo.

\begin{algorithm}[H] % La H es para que se quede acá, porque se iba a otra página. Estaría bueno hacerlo global
  \caption{Ejemplo de reductor}\label{alg:reductor}
  \KwData{$G = \{f_1, …, f_n\} ⊆ K⟨X⟩, g ∈ K⟨X⟩$}
  \KwResult{$g' ∈ K⟨X⟩$}
  $g' ← g$

  $i ← 1$

  \While{$i ≤ n$} {
    \While{$i ≤ n$} {
      \If{$f_i ∈ \sop(g')$} {
        $g' ← g' - \frac{g'_{\lm(f_i)}}{\lc(f_i)}f_i$

        $i ← 1$

        \Break
      }
      \Else{
        $i ← i + 1$
      }
    }
  }
  \Return{$g'$}
\end{algorithm}

Este algoritmo consiste básicamente en siempre buscar entre los elementos de $G$ si hay alguno con el cual reducir, y parar cuando ya no hay ninguno.

Una propiedad sobre los reductores que vamos a necesitar es que mantienen la pertenencia a ideales (lo cual tiene mucho sentido por la definición).

\begin{lemma}\label{lemma:e mantiene pertenencia a ideal}
  Sean $e_≤$ un reductor, $G ⊆ K⟨X⟩$ y $f ∈ (G)$. Entonces
  \[ e_≤(G)(f) ∈ (G) \text{.}\]
\end{lemma}
\begin{proof}
  Es consecuencia directa de la definición y del \cref{thm:→ mantiene pertenencia a ideal}.
\end{proof}

Las bases de Gröbner se definieron por la propiedad más importante que queremos que tengan, pero van a ser mucho mas cómodas de trabajar usando las siguientes equivalencias.

\begin{theorem}\label{thm:equivalencias de base de Gröbner}
  Sean $I$ un ideal de $K⟨X⟩$ y $G ⊆ K⟨X⟩$. Las siguientes afirmaciones son equivalentes:
  \begin{enumerate}
    \item $G$ es una base de Gröbner de $I$.

    \item $∀f ∈ K⟨X⟩ : f ∈ I ⇔ f →^*_{≤, G} 0$.

    \item $(G) = I ∧ ∀f ∈ K⟨X⟩ : f ∈ I ⇒ f →^*_{≤, G} 0$.

    \item $(G) = I ∧ ∀f ∈ I - \{0\} : ∃g ∈ G : \lm(g) | \lm(f)$.

    \item $∀f ∈ I - \{0\} : ∃g_1, …, g_n ∈ G, a_1, …, a_n, b_1, …, b_n ∈ ⟨X⟩ : \lm(a_i g_i b_i) ≤ \lm(f) ∧ f = ∑_{i = 1}^n a_i g_i b_i$.
  \end{enumerate}

\end{theorem}
\begin{proof} Vamos a probar (1) $⇒$ (2), (2) $⇒$ (3), (3) $⇒$ (1), (3) $⇒$ (2), (4) $⇒$ (3), (2) $⇒$ (5) y (5) $⇒$ (4).
  \begin{description}

    \item[(1) $⇒$ (2):] Supongamos que $G$ es una base de Gröbner de $I$ y tomemos $f ∈ K⟨X⟩$. Tenemos que probar $f ∈ I ⇔ f →^*_{≤, G} 0$. Vamos de un lado para el otro:
    \begin{DispWithArrows*}
      &f ∈ I \Arrow{\cref{lemma:en ideal ⇔ congruente 0}} \\
      & ⇔ f ≡_I 0 \Arrow{\cref{lemma:→^* = ≡}} \\
      & ⇔ f ↔^*_{≤, G} 0 \Arrow{Por el \cref{lemma:→ gröbner es Church-Rosser}, $→_{≤, G}$ es Church-Rosser, definición de Church-Rosser} \\
      & ⇔ f ↓_{≤, G} 0 \Arrow{Definición de $↓$} \\
      & ⇔ ∃f' ∈ K⟨X⟩ : f →^*_{≤, G} f' ∧ 0 →^*_{≤, G} f' \Arrow{\cref{thm:→ achican}} \\
      & ⇔ ∃f' ∈ K⟨X⟩ : f →^*_{≤, G} f' ∧ f' ≤ 0 \Arrow{\cref{lemma:0 es mínimo}} \\
      & ⇔ ∃f' ∈ K⟨X⟩ : f →^*_{≤, G} f' ∧ f' = 0 \\
      & ⇔ f →^*_{≤, G} 0 \text{.}
    \end{DispWithArrows*}

    % \item[(2) $⇒$ (1):]
    % Supongamos (2), o sea $∀f ∈ K⟨X⟩ : f ∈ I ⇔ f →^*_{≤, G} 0$. Tenemos que probar que $G$ es una base de Gröbner de $I$, es decir $(G) = I ∧ →_{≤, G}$ es confluente.

    % Probemos cada término del $∧$ por separado:

    % \begin{description}
    %   \item[Prueba de $(G) = I$:] Tomemos $f ∈ K⟨X⟩$ y probemos $f ∈ (G) ⇔ f ∈ I$, probando ida y vuelta por separado:

    %   \begin{description}
    %     \item[Ida ($⇒$):] Supongamos antecedente $f ∈ (G)$.

    %     Sean $c_1, …, c_n, c_1', …, c_n' ∈ K⟨X⟩$, $g_1, …, g_n ∈ G$ tales que $f = ∑_{i = 1}^n c_i g_i c_i'$, los cuales existen por la definición de $(\ ·\ )$.

    %     Definamos $f_0 = f$ y para $i ∈ \{1, …, n\}$ $f_i = f_{i-1} - c_i g_i c_i'$.

    %     Notar que tenemos $∀i ∈ \{1, …, n\} : f_{i-1} →_{≤, G} f_i$ y que $f_n = 0$.

    %     Esto significa que $f →^*_{≤, G} 0$ y por ende por (2) vale $f ∈ I$.

    %     \item[Vuelta ($⇐$):] Supongamos el antecedente $f ∈ I$.

    %     Por (2) tenemos que $f →^*_{≤, G} 0$.

    %     Así que sean $f_0, f_1, …, f_n ∈ K⟨X⟩$ tales que $f_0 = f$, $f_n = 0$ y $∀i ∈ \{1, …, n\} : f_{i-1} →_{≤, G} f_i$, los cuales existen por la definición de $^*$.

    %     Además, para cada $i ∈ \{1, …, n\}$ sean $c_i, c_i' ∈ K⟨X⟩, g_i ∈ G$ tales que $f_i = f_{i-1} - c_i g_i c_i'$, los cuales existen por definición de $→_{≤, G}$.

    %     Notar que en particular $f_{i-1} = f_i + c_i g_i c_i'$ y por ende $f = ∑_{i = 1}^n c_i g_i c_i'$, lo cual prueba que $f ∈ (G)$.

    %   \end{description}

    %   \item[Prueba de $→_{≤, G}\text{ es confluente}$:]\

    %   Por definición de confluencia alcanza que probar $∀f, f_0, f_1 ∈ K⟨X⟩ : f →^*_{≤, G} f_0 ∧ f →^*_{≤, G} f_1 ⇒ f_0 ↓_{≤, G} f_1$. En tal caso, vale por lo siguiente:

    %   \begin{DispWithArrows*}
    %     &f →^*_{≤, G} f_0 ∧ f →^*_{≤, G} f_1 \\
    %     & ⇒ f_0 ↔^*_{≤, G} f_1 \Arrow{\cref{lemma:→^* = ≡}} \\
    %     & ⇒ f_0 ≡_{(G)} f_1 \Arrow{Definición $≡_{\ ·\ }$} \\
    %     & ⇒ f_0 - f_1 ∈ (G) \Arrow{(6), ya probamos que $(G) = I$} \\
    %     & ⇒ f_0 - f_1 →^*_{≤, G} 0 \Arrow{\cref{lemma:suma →↓}} \\
    %     & ⇒ (f_0 - f_1) + f_1 ↓_{≤, G} 0 + f_1 \\
    %     & ⇒ f_0 ↓_{≤, G} f_1 \text{.}
    %   \end{DispWithArrows*}
    % \end{description}

    \item[(2) $⇒$ (3):] Supongamos el antecedente $∀f ∈ K⟨X⟩ : f ∈ I ⇔ f →^*_{≤, G} 0$. Tenemos que probar que $(G) = I ∧ ∀f ∈ K⟨X⟩ : f ∈ I ⇒ f →^*_{≤, G} 0$. Probemos cada termino del $∧$ por separado:

    \begin{description}
      \item[Prueba de $(G) = I$:] Es cierto porque (2) $⇒$ (1) y $(G) = I$ es parte de la definición de base de Gröbner.
      \item[Prueba de $∀f ∈ K⟨X⟩ : f ∈ I ⇒ f →^*_{≤, G} 0$:] Es cierto por (2).
    \end{description}

    \item[(3) $⇒$ (1):] Supongamos el antecedente $(G) = I ∧ ∀f ∈ K⟨X⟩ : f ∈ I ⇒ f →^*_{≤, G} 0$. Tenemos que probar que $G$ es una base de Gröbner de $I$, es decir $(G) = I ∧ →_{≤, G}$ es confluente. La parte de $(G) = I$ es valida porque es parte de (3).

    Para lo otro, por definición de confluente, alcanza con probar $∀f, f_0, f_1 ∈ K⟨X⟩ : f →^*_{≤, G} f_0 ∧ f →^*_{≤, G} f_1 ⇒ f_0 ↓_{≤, G} f_1$. En tal caso, vale por lo siguiente:
    \begin{DispWithArrows*}
      &f →^*_{≤, G} f_0 ∧ f →^*_{≤, G} f_1 \\
      & ⇒ f_0 ↔^*_{≤, G} f_1 \Arrow{\cref{lemma:→^* = ≡}} \\
      & ⇒ f_0 ≡_{(G)} f_1 \Arrow{Definición $≡_{\ ·\ }$} \\
      & ⇒ f_0 - f_1 ∈ (G) \Arrow{(6), ya probamos que $(G) = I$} \\
      & ⇒ f_0 - f_1 →^*_{≤, G} 0 \Arrow{\cref{lemma:suma →↓}} \\
      & ⇒ (f_0 - f_1) + f_1 ↓_{≤, G} 0 + f_1 \\
      & ⇒ f_0 ↓_{≤, G} f_1 \text{.}
    \end{DispWithArrows*}

    % \item[(2) $⇒$ (4):] Supongamos (2).

    % La parte de $(G) = I$ es valida porque (2) $⇒$ (1) y $(G) = I$ es parte de la definición de base de Gröbner.

    % Para la parte de $∀f ∈ I : ∃g ∈ G : \lm(g) | \lm(f)$ tomemos $f ∈ I$ y mostremos un $g$ que cumple el $∃$:

    % Por (2) tenemos $f →^*_{≤, G} 0$, esto significa que en alguno de los pasos de el $→^*_{≤, G}$ se tiene que reducir el monomio principal de $f$, o sea, uno de los pasos es de la forma $→_{≤, a, g, b}$ tal que $a \lm(g) b = \lm(f)$ con $g ∈ G, a, b ∈ ⟨X⟩$, por lo tanto, tenemos que $\lm(g) | \lm(f)$.

    \item[(4) $⇒$ (3):] Supongamos el antecedente $(G) = I ∧ ∀f ∈ I - \{0\} : ∃g ∈ G : \lm(g) | \lm(f)$. Tenemos que probar $(G) = I ∧ ∀f ∈ K⟨X⟩ : f ∈ I ⇒ f →^*_{≤, G} 0$. La parte de $(G) = I$ es valida porque es parte de (4). Para la otra parte probemoslo por contradicción. En particular tomemos el mínimo $f$ tal que $f ∈ I$ pero no se cumple que $f →^*_{≤, G} 0$.

    Por (4) sea $g ∈ G$ tal que $\lm(g) | \lm(f)$ y sean también:
    \begin{itemize}
      \item $a, b ∈ ⟨X⟩$ tales que $a \lm(g) b = \lm(f)$
      \item $f' = f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb$
    \end{itemize}

    Notar que $f' ∈ I$ ya que $f ∈ I$ y $g ∈ (G) = I$ y notar que $f →_{≤, G} f'$.

    Además, por \cref{thm:→ achican}, $f' < f$.

    Y como no vale $f →^*_{≤, G} 0$, tampoco puede valer $f' →^*_{≤, G} 0$. Sin embargo, esto contradice que $f$ sea mínimo.

    \item[(2) $⇒$ (5):] Supongamos (2) y tomemos $f ∈ I - \{0\}$.

    Por (2) tenemos que $f →^*_{≤, G} 0$.

    Sean:
    \begin{itemize}
      \item $n ∈ ℕ, f_0, …, f_n ∈ K⟨X⟩$ tales que $f_0 = f$, $f_n = 0$ y $f_0 →_{≤, G} f_1 →_{≤, G} … →_{≤, G} f_n$, los cuales existen porque $f →^*_{≤, G} 0$ y por el \cref{lemma:→* como ∃}.
      \item Para cada $i ∈ {1, …, n}$, $g_i ∈ G, a_i, b_i ∈ X$ tales que $\lm_≤(a_i g_i b_i) ∈ \sop(f_{i-1})$ y $f_i = f_{i-1} - \frac{(f_{i-1})_{\lm_≤(a_i g_i b_i)}}{\lc_≤(g_i)}a_i g_i b_i$ los cuales existen por definición de $→_{≤, G}$.
    \end{itemize}

    Vamos a probar que $g_1, …, g_n, …, a_1, …, a_n, b_1, …, b_n$ satisfacen el $∃$ de (5), es decir, vamos a probar $\lm(a_i g_i b_i) ≤ \lm(f)$ fijando $i$ y $f = ∑_{i = 1}^n a_i g_i b_i$.

    \begin{description}
      \item[Prueba de $\lm(a_i g_i b_i) ≤ \lm(f)$ fijando $i$:] Por el \cref{thm:→ achican} y por transitividad de $<$ tenemos $f_{i-1} ≤ f$. Además tenemos $a_i g_i b_i ≤ f_{i-1}$ porque $\lm_≤(a_i g_i b_i) ∈ \sop(f_{i-1})$, así que por transitividad de $≤$ vale $\lm(a_i g_i b_i) ≤ \lm(f)$.
      \item[Prueba de $f = ∑_{i = 1}^n a_i g_i b_i$:] Es consecuencia directa de como se eligieron los $g_i, a_i, b_i$.
    \end{description}

    \item[(5) $⇒$ (4):] Supongamos (5) y probemos (4). Para eso tenemos que probar $(G) = I$ y $∀f ∈ I : ∃g ∈ G - \{0\} : \lm(g) | \lm(f)$.

    \begin{description}
      \item[Prueba de $(G) = I$:] Tomemos $f ∈ I$ y probemos $f ∈ (G)$.

      Por (5) tenemos que $f = ∑_{i = 1}^n a_i g_i b_i$ con $g_i ∈ G$.

      Esto encaja exactamente con la definición de $(G)$, así que queda probado $f ∈ (G)$.
      \item[Prueba de $∀f ∈ I - \{0\} : ∃g ∈ G : \lm(g) | \lm(f)$:] Fijemos $f ∈ I - \{0\}$.

      Por (5) tenemos que $f = ∑_{i = 1}^n a_i g_i b_i$ con $g_i ∈ G$ y $\lm(a_i g_i b_i) ≤ \lm(f)$.

      Como $\lm(a_i g_i b_i) ≤ \lm(f)$, si o si tiene que pasar para algún $j$ que $\lm(a_j g_j b_j) = \lm(f)$ y para este $g_j$ vale que $\lm(g_j) | \lm(f)$.
    \end{description}

  \end{description}
  Con esto se termina la prueba.
\end{proof}


\section{Algoritmo de Buchberger}

En esta sección se explica un primer algoritmo para calcular bases de Gröbner llamado Algoritmo de Buchberger.

Antes de poder calcular bases de Gröbner algo que vamos a querer hacer es poder computar si un conjunto es una Base de Gröbner, porque ninguna de las equivalencias de \cref{thm:equivalencias de base de Gröbner} es directamente calculable porque hacen cuantificaciones sobre conjuntos infinitos.

Para eso vamos a definir algo llamado S-polinomio de dos polinomios y usarlos para enunciar un teorema que nos da una forma computable de determinar si un conjunto es una Base de Gröbner.

Esos S-polinomios entre dos polinomios lo que hacen es básicamente multiplicar cada polinomio por monomios a cada lado y por un escalar de forma que los monomios principales se cancelen. O sea para polinomios $f$ y $g$ vamos a tener una cuenta $k a f b - c g d$ de forma que se cancelen los monomios principales.

Antes definimos las ambigüedades que representan los polinomios para los cuales puede pasar eso sin que todo lo importante esté en los monomios $a$, $b$, $c$ y $d$.

\begin{definition}
  Sean $m, m', a, b, c, d ∈ ⟨X⟩$. Se define
  \[ (a, b, c, d, m, m')\text{ es una ambigüedad} ⇔ amb = cm'd ∧ |a|, |b| < |m'| ∧ |c|, |d| < |m| \text{.}\]

  La ambigüedad $(a, b, c, d, m, m')$ se define como:
  \begin{itemize}
    \item De superposición $⇔ a = ε = d ∨ b = ε = c$.
    \item De inclusión $⇔ a = ε = b ∨ c = ε = d$.
    \item Relevante $⇔$ es de superposición o de inclusión.
  \end{itemize}

  Además, si $f, f' ∈ K⟨X⟩$ se define que $(a, b, c, d, f, f')$ es una ambigüedad si y solo si $(a, b, c, d, \lm_≤{(f)}, \lm_≤{(f')})$ es una ambigüedad y lo mismo para ambigüedades de superposición, de inclusión y relevantes.

  Si además $F ⊆ K⟨X⟩$ se definen:
  \begin{itemize}
    \item $\amb(f, f') = \{(a, b, c, d, f, f') : a, b, c, d ∈ ⟨X⟩ ∧ (a, b, c, d, f, f')\text{ es una ambigüedad relevante}\}$.
    \item $\amb(F) = ⋃_{f, f' ∈ F - \{0\}}{\amb(f, f')}$.
  \end{itemize}

\end{definition}

\begin{definition}
  Sean $a, b, c, d ∈ ⟨X⟩, f, f' ∈ K⟨X⟩$ y $α = (a, b, c, d, f, f')$ una ambigüedad. Se define el S-polinomio de $α$ como
  \[ \S(α) = \frac{afb}{\lc_≤{(f)}} - \frac{cgd}{\lc_≤{(f')}} \text{.}\]
\end{definition}

Notar que por como es la definición de ambigüedad, cuando es relevante siempre es como que hay una parte de $m$ y una parte de $m'$ que son iguales y que no están en los monomios $a$, $b$, $c$ y $d$.

Por ejemplo, si $f = 2 cba + 3 dbcda$ y $f' = b + bcd$, entonces una ambigüedad de inclusión sería

\[α = (ε, ε, d, a, f, f') \text{.} \]

\noindent Siendo su S-polinomio

\[S(α) = \frac{f}{3} - d f' a = \frac{2}{3} cba - dba  \text{.} \]

\noindent Y una ambigüedades de superposición sería

\[ α' = (bc, ε, ε, bcda, f, f') \text{.} \]

\noindent Siendo su S-polinomio

\[ S(α') = \frac{bc f}{3} - f' bcda = bbcda - \frac{2}{3} bccba \text{.} \]

\

Ahora vamos a probar que efectivamente los monomios principales se cancelan y después vamos a dar el teorema para calcular si un conjunto es una base de Gröbner.

\begin{theorem}\label{thm:lm ambs}
  Sean $a, b, c, d ∈ ⟨X⟩, f, g ∈ K⟨X⟩$ y $α = (a, b, c, d, f, g)$ una ambigüedad. Entonces:
  \[ \lm_≤{(afb)} = \lm_≤{(cgd)} \text{.}\]
\end{theorem}
\begin{proof}\
  \begin{DispWithArrows*}
    &\lm_≤{(afb)} \Arrow{Definición de ambigüedad para polinomios} \\
    & = a\lm_≤{(f)}b \Arrow{Definición de ambigüedad} \\
    & = c\lm_≤{(g)}d \Arrow{Definición de ambigüedad para polinomios} \\
    & = \lm_≤{(cgd)}
  \end{DispWithArrows*}
\end{proof}

Eso permite definir el monomio principal de una ambigüedad.

\begin{definition}
  Sean $a, b, c, d ∈ ⟨X⟩, f, g ∈ K⟨X⟩$ y $α = (a, b, c, d, f, g)$ una ambigüedad. Se define el monomio principal de $α$ como
  \[ \lm_≤(α) = \lm_≤(afb) \text{.}\]
\end{definition}

Notar que por el \cref{thm:lm ambs} también vale que $\lm_≤(α) = \lm_≤(cgd)$.

Lo de que los monomios principales se cancelen se enuncia en el siguiente teorema.

\begin{theorem}
  Sean $a, b, c, d ∈ ⟨X⟩, f, g ∈ K⟨X⟩$ y $α = (a, b, c, d, f, g)$ una ambigüedad. Entonces
  \[ \lm_≤(\S(α)) < \lm_≤(α) \text{.}\]
\end{theorem}
\begin{proof}
  Esto es porque en la resta $\frac{afb}{\lc_≤(f)} - \frac{cgd}{\lc_≤(g)}$ los monomios principales se cancelan.
\end{proof}

Otra cosa importante es que estos S-polinomios son cerrados en el ideal, es decir, que el S-polinomio de dos elementos de un ideal, por ejemplo en particular del conjunto generador, es un elemento del ideal.

\begin{lemma}\label{lemma:S es cerrado en ideal}
  Sean $I ⊆ K⟨X⟩$ un ideal $a, b, c, d ∈ ⟨X⟩, f, g ∈ I$ y $α = (a, b, c, d, f, g)$ una ambigüedad. Entonces
  \[ \S(α) ∈ I \text{.}\]
\end{lemma}
\begin{proof}
  En la definición de $\S$ se ve que es una combinación lineal de $f$ y $g$ con elementos de $K⟨X⟩$. En particular con los elementos $\frac{a}{\lc_≤{(f)}}$, $b$, $\frac{c}{\lc_≤{(g)}}$ y $d$. Como $f, g ∈ I$ y $I$ es un ideal, esa combinación pertenece a $I$.
\end{proof}

Ahora si a continuación el anunciado teorema para computar si un conjunto es una base de Gröbner.

\begin{theorem}[Condición de Buchberger]\label{thm:condición de Buchberger}
  Sean $I$ un ideal de $K⟨X⟩$ y $G ⊆ K⟨X⟩$. Entonces son equivalentes:
  \begin{enumerate}
    \item $G$ es una base de Gröbner de $I$.
    \item $∀α ∈ \amb(G) : \S(α) →^*_{≤, G} 0$.
  \end{enumerate}
  \qed
\end{theorem}
% TODO: Agregar prueba. De cualquier manera, Hof no lo prueba a esto

Con esto ya se puede explicar la idea del algoritmo de Buchberger.

Cuando estamos chequeando si un conjunto es una base de Gröbner usando el \cref{thm:condición de Buchberger}, si nos encontramos con un S-polinomio $f$ que se reduce a un polinomio $g$ distinto de $0$ lo que podemos hacer es agregar $g$ al conjunto y con eso $f$ sí se va a reducir a $0$.

Se podría pensar que agregar $g$ por ahí cambia el ideal generado, pero por el teorema \cref{lemma:S es cerrado en ideal} sabemos que $g$ es un elemento del ideal, por el \cref{thm:→ mantiene pertenencia a ideal} eso implica que $f$ también pertenece al ideal y por ende por el \cref{lemma:gen G = gen G U a con a ∈ gen G} se puede agregar al conjunto y que el ideal generado siga siendo el mismo.

El problema con eso de agregar un nuevo polinomio es que si bien hay una S-polinomio que pasa a si reducirse a $0$ también aparecen nuevas ambigüedades. Eso sin embargo no es tanto problema porque lo se hace es hacer eso hasta que en algún momento todas las ambigüedades se reducen a $0$ y si eso no pasa nunca simplemente el algoritmo no termina nunca (lo que refleja la no decidibilidad del problema de pertenencia a un ideal).

Ese proceso de agregar polinomios infinitamente lo definimos matemáticamente con la siguiente definición.

\begin{definition}
  Sean $G ⊆ K⟨X⟩$ y $e_≤$ un reductor. Se definen las bases de Buchberger como:
  \begin{itemize}
    \item $\B_{e_≤}^0(G) = G$.
    \item $\B_{e_≤}^{i + 1}(G) = \B_{e_≤}^i(G) ∪ \{e_≤(\B_{e_≤}^i(G))(\S(α)) : α ∈ \amb(\B_{e_≤}^i(G))\}$.
    \item $\B_{e_≤}(G) = ⋃_{i = 0}^∞ \B_{e_≤}^i(G)$.
  \end{itemize}
\end{definition}

El nombre bases de Buchberger es inventado para esta tesis, otros autores no le han dado ningún nombre concreto a esos conjuntos.

Con el siguiente teorema se enuncia que esos conjuntos realmente hacen lo que dijimos arriba y además que si hay una base de Gröbner finita entonces en algún momento se llega.

\begin{theorem}\label{thm:Buchberger correctitud}
  Sean $G ⊆ K⟨X⟩$ y $e_≤$ un reductor. Entonces
  \begin{enumerate}
    \item $\B_{e_≤}(G)$ es una base de Gröbner de $(G)$.
    \item $(G)$ tiene una base de Gröbner finita $⇒ ∃i ∈ ℕ ∪ \{0\} : (\B_{e_≤}^i(G))$ es una base de Gröbner.
  \end{enumerate}
\end{theorem}

Con lo que dijimos antes tiene sentido que (1) sea cierto, pero que (2) sea cierto no es para nada obvio. Para probar ambos items primero vamos a probar algunos lemas en el contexto de este teorema (o sea con $G$ y $e_≤$ fijados).

\begin{lemma}\label{lemma:Buchberger correctitud:3}
  $∀i ∈ ℕ ∪ \{0\} : \B_{e_≤}^{i}(G) ⊆ (G)$.
\end{lemma}
\begin{proof}
  Procedamos por inducción sobre $i$. El caso base se cumple porque $G ⊆ (G)$. Para el caso inductivo, supongamos que la afirmación es cierta para $i$ y probemos que también vale para $i + 1$:

  Tomemos $f ∈ \B_{e_≤}^{i + 1}(G)$ y probemos que $f ∈ (G)$.

  Por la definición recursiva de $\B_{e_≤}^{i + 1}$ vale que $f ∈ \B_{e_≤}^i(G) ∨ ∃α ∈ \amb(\B_{e_≤}^i(G)) : f = e_≤(\B_{e_≤}^i(G))(\S(α))$.

  Dividamos en casos según ese $∨$

  \begin{description}
    \item[Caso $f ∈ \B_{e_≤}^i(G)$:] Es valido por ser exactamente la hipótesis inductiva.
    \item[Caso $∃α ∈ \amb(\B_{e_≤}^i(G)) : f = e_≤(\B_{e_≤}^i(G))(\S(α))$:] En tal caso, por el \cref{lemma:e mantiene pertenencia a ideal} vale que $f ∈ \B_{e_≤}^i(G)$ y por la hipótesis inductiva que $f ∈ (G)$.
  \end{description}

\end{proof}

\begin{lemma}\label{lemma:Buchberger correctitud:4}
  $∀i ∈ ℕ : \B_{e_≤}(G) ⊆ (G)$.
\end{lemma}
\begin{proof}
  Esto es consecuencia directa de el \cref{lemma:Buchberger correctitud:3} y la definición de $\B_{e_≤}$.
\end{proof}

\begin{lemma}\label{lemma:Buchberger correctitud:5}
  $(\B_{e_≤}(G)) = (G)$.
\end{lemma}
\begin{proof}
  Por la definición de $\B_{e_≤}^i$ tenemos $G ⊆ \B_{e_≤}(G)$ y por ende $G ⊆ (\B_{e_≤}(G))$ y con el \cref{lemma:Buchberger correctitud:4} aplicando el \cref{lemma:sub gen y sub gen ⇒ eq} tenemos lo que queremos probar.
\end{proof}

\begin{lemma}\label{lemma:Buchberger correctitud:6}
  $∀α ∈ \amb(\B_{e_≤}(G)) : \S(α) →^*_{≤, \B_{e_≤}(G)} 0$.
\end{lemma}
\begin{proof}
  Tomemos $α = (a, b, c, d, f, g) ∈ \amb(\B_{e_≤}(G))$. Por definición de $\amb$, se cumple que $f, g ∈ \B_{e_≤}(G)$. Sea $i ∈ ℕ  ∪ \{0\}$ el mínimo tal que $f ∈ \B_{e_≤}^i(G)$, $j ∈ ℕ  ∪ \{0\}$ el mínimo tal que $g ∈ \B_{e_≤}^j(G)$ y $k = \max(i, j)$.

  Por definición de $\amb$ vale que $f, g ∈ \B_{e_≤}(G)$, así que sea $i ∈ ℕ$ el mínimo tal que $f ∈ \B_{e_≤}^i(G)$, $j ∈ ℕ$ el mínimo tal que $g ∈ \B_{e_≤}^j(G)$ y $k = \max(i, j)$.

  Por definición de $\amb$ vale que $α ∈ \amb(\B_{e_≤}^k(G))$, por ende por definición de $\B_{e_≤}^i$ vale que $e_≤(\B_{e_≤}^k(G))(\S(α)) ∈ \B_{e_≤}^{k + 1}(G)$. Esto implica que $e_≤(\B_{e_≤}^k(G))(\S(α)) →_{≤, \B_{e_≤}^{k + 1}(G)} 0$ y por ende que $e_≤(\B_{e_≤}^k(G))(\S(α)) →_{≤, \B_{e_≤}(G)} 0$.

  Combinando eso con que por definición de reductor vale que $\S(α) →^*_{≤, \B_{e_≤}^k(G)} e_≤(\B_{e_≤}^k(G))(\S(α))$ y por ende $\S(α) →^*_{≤, \B_{e_≤}(G)} e_≤(\B_{e_≤}^k(G))(\S(α))$ vale que $\S(α) →^*_{≤, \B_{e_≤}(G)} 0$.
\end{proof}

\begin{proof}[Demostración del \cref{thm:Buchberger correctitud}]
  Por el \cref{lemma:Buchberger correctitud:5} y el \cref{lemma:Buchberger correctitud:6} vale la equivalencia de base de Gröbner de la condición de Buchberger (\cref{thm:condición de Buchberger}), así que (1), o sea, que $G$ es una base de Gröbner queda probado.

  Para probar (2), asumamos el antecedente, o sea, que $(G)$ tiene una base de Gröbner finita, y tomemos una base finita $\{g_1, …, g_n\}$. Ahora tenemos que probar el consecuente, o sea, que $∃i ∈ ℕ ∪ \{0\} : (\B_{e_≤}^i(G))$ es una base de Gröbner.

  Para cada $i ∈ \{1, …, n\}$ sean:
  \begin{itemize}
    \item $g_{i, 1}, …, g_{i, k_i} ∈ \B_{e_≤}(G), a_{i, 1}, …, a_{i, k_i}, b_{i, 1}, …, b_{i, k_i} ∈ ⟨X⟩$ con $\lm(a_{i, j} g_{i, j} n_{i, j}) ≤ \lm(g_i)$ tales que $g_i = ∑_{j = 1}^{k_i} a_{i, j} g_{i, j} b_{i, j}$.

    Los cuales existen por por (5) del \cref{thm:equivalencias de base de Gröbner}.

    \item $G' = \{g_{i, j} : i ∈ \{1, …, n\}, j ∈ \{1, …, k_i\}\}$.

    \item $k ∈ ℕ$ el mínimo tal que $G' ⊆ \B_{e_≤}^k(G)$.

    Notar que $k$ está bien definido porque $G'$ es finito.
  \end{itemize}

  Ahora vamos a probar que $\B_{e_≤}^k(G)$ es una base de Gröbner de $(G)$.

  Por (5) del \cref{thm:equivalencias de base de Gröbner} alcanza con probar:
  \[ ∀f ∈ (G) - \{0\} : ∃g_1, …, g_n ∈ \B_{e_≤}^k(G), a_1, …, a_n, b_1, …, b_n ∈ ⟨X⟩ : \lm(a_i g_i b_i) ≤ \lm(f) : f = ∑_{i = 1}^n a_i g_i b_i \text{.}\]

  Tomemos $f ∈ (G) - \{0\}$ y escribamoslo de esa forma.

  Por (5) del \cref{thm:equivalencias de base de Gröbner} y el hecho de que $\{g_1, …, g_n\}$ es una base de Gröbner, sean $i'_1, …, i'_{n'} ∈ {1, …, n}, a'_1, …, a'_{n'}, b'_1, …, b'_{n'} ∈ ⟨X⟩$ tales que $\lm(a'_i g_{i'_i} b'_i) ≤ \lm(f)$ y $f = ∑_{i = 1}^{n'} a'_i g_{i'_i} b'_i$.

  \begin{DispWithArrows*}
    &f = ∑_{i = 1}^{n'} a'_i g_{i'_i} b'_i \Arrow{Condición de $g_{i, j}$} \\
    & = ∑_{i = 1}^{n'} a'_i (∑_{j = 1}^{k_{i'_i}} a_{i', j} g_{i, j} b_{i', j}) b'_i \\
    & = ∑_{i = 1}^{n'} ∑_{j = 1}^{k_i'} a'_i a_{i', j} g_{i, j} b_{i', j} b'_i
  \end{DispWithArrows*}

  Sabemos además que $g_{i',j} ∈ \B_{e_≤}^k(G)$ y que $g_{i', j} ≤ g_{i'_i} ≤ f$.

  Con esto queda completada la prueba.

\end{proof}

Con estos conjuntos podemos de forma directa dar un seudocódigo que los calcula:

\begin{algorithm}[H] % La H es para que se quede acá, porque se iba a otra página. Estaría bueno hacerlo global
  \caption{Algoritmo de Buchberger}\label{alg:Buchberger}
  \KwData{$G ⊆ K⟨X⟩$, $e_≤$ un reductor}
  \KwResult{$B ⊆ K⟨X⟩$ una base de Gröbner de $(G)$ si es que termina}
  $B ← G$

  \Loop{} {
    $ambs ← \amb(B)$

    $B' ← B$

    \For{$α ∈ ambs$} {
      $f ← e_≤(B)(\S(α))$

      \If{$f ≠ 0$} {
        $B' ← B' ∪ \{f\}$
      }
    }

    \If{$B' = B$} {
      \Break
    }

    $B ← B'$
  }
  \Return{$B$}
\end{algorithm}

El algoritmo así, si bien es una implementación tal cual de la definición de los conjuntos $\B_{e_≤}^i$ que probamos que es correcta, es muy lenta y necesita un cambio para que pase a andar bien.

Ese cambio es en lugar de para las reducciones usar el conjunto anterior usar también los nuevos polinomios que ya fueron agregados, es decir, llamar a $e_≤$ con $B'$ en lugar de con $B$ en la línea 6. % Se podrá hacer con un label esto?

Por lo que habíamos visto antes el cambio tiene sentido que sea correcto y la prueba es similar a la del \cref{thm:Buchberger correctitud}.

En la implementación el cambio ese hace una diferencia inmensa en la eficiencia, pasa de no andar rápido casi nunca a andar rápido en un montón de casos. El motivo teórico detrás de que hacer eso haga que pase a ser tanto mas rápido el autor no lo sabe ni lo pudo encontrar en la literatura.

Hay otras dos optimizaciones que se pueden hacer: una es para descartar algunas ambigüedades sin tener que reducirlas y la otra es para ir sacando algunos polinomios de la base. En la \cref{secton:optimizaciones} se aborda este tema.

\section{Algoritmo F4}

En esta sección se explica el algoritmo F4, que es otro algoritmo para calcular Bases de Gröbner.

La idea de F4 es agarrar varias ambigüedades y reducir todos sus S-polinomios al mismo tiempo, tanto reduciendo con los elementos que ya están en la base como reduciendo entre los propios S-polinomios que se están considerando. Para esto lo que se hace es convertir el problema en una reducción por filas de una matriz, codificando cada monomio como una columna y cada polinomio como una fila.

Para hacer esto de codificar los polinomios como filas de una matriz vamos a necesitar algunas definiciones.

\begin{definition}
  Sea $F ⊆ K⟨X⟩$. Se define
  \[ \spn_K(F) = \{∑_{i = 1}^n c_i f_i : n ∈ ℕ, c_i ∈ K, f_i ∈ F\} \text{.} \]
\end{definition}

\begin{definition}
  Sean $M = \{m_1, …, m_n\} ⊆ ⟨X⟩$ con $m_1 > ⋯ > m_n$. Se definen:

  \begin{itemize}
    \item $\poli_T : K^n → \spn_K(M)$.
    \item $\poli_T(v) = ∑_{i = 1}^n v_i m_i$.
  \end{itemize}
\end{definition}

\begin{definition}
  Sean $M = \{m_1, …, m_n\} ⊆ ⟨X⟩$ con $m_1 > ⋯ > m_n$. Se define $\mat_T$ aplicado a un polinomio como la inversa de $\poli_T$:

  \begin{itemize}
    \item $\mat_T : \spn_K(M) → K^n$.
    \item $\mat_T(f) = \poli_T^{-1}(f)$.
  \end{itemize}

  Para una lista de polinomios la función $\mat_T$ produce una matriz:

  \begin{itemize}
    \item $\mat_T(f_1, …, f_m) = \begin{pmatrix} \mat_T(f_1) \\ ⋮ \\ \mat_T(f_m) \end{pmatrix}$.
  \end{itemize}
\end{definition}

Estas definiciones nos permiten hablar de pasar de polinomios a matrices y viceversa.

Lo que antes en la reducción era restar un polinomio multiplicado por un escalar y un monomio a cada lado, la idea es que ahora pase a ser restarle una fila multiplicada por un escalar a otra en la matriz, así que hay a tener que empezar ya teniendo en la matriz los polinomios a restar ya multiplicados por los monomios a cada lado. Para eso definimos el algoritmo de preprocesamiento simbólico que dado el conjunto por el cual se reduce $G$ y el conjunto a reducir $F$ calcula todos los polinomios que van a ser necesarios:

\begin{algorithm}[H] % La H es para que se quede acá, porque se iba a otra página. Estaría bueno hacerlo global
  \caption{Preprocesamiento simbólico}\label{alg:Preprocesamiento simbólico}
  \KwData{$G, F ⊆ K⟨X⟩$ conjuntos finitos}
  \KwResult{$G' ⊆ \{agb : a, b ∈ ⟨X⟩, g ∈ G\}$}
  $G' ← ∅$

  $conciderados ← \{\lm(g) : g ∈ P\}$

  $T ← ⋃_{g ∈ P} \sop(\tail(g))$

  \While{$T ≠ ∅$} {
    elegir $m ∈ T$

    $T ← T - \{m\}$

    \For{$g ∈ G$} {
      \If{$\lm(g) | m$} {
        calcular $a$ y $b$ tales que $m = a \lm(g) b$

        $G' ← G' ∪ {agb}$

        $nuevos ← \{m' ∈ \sop(\tail(agb)) : m' ∉ conciderados\}$

        $T ← T ∪ nuevos$

        $conciderados ← conciderados ∪ nuevos$
      }
    }
  }

  \Return{$G'$}
\end{algorithm}

Ahora lo que hay que hacer es, al reducir, elegir varios polinomios para reducir, poner los polinomios elegidos y el resultado del preprocesamiento simbólico en una matriz, hacer la reducción por filas de esa matriz y después de alguna manera detectar que filas son los polinomios elegidos reducidos para agregar solo esos polinomios a la base.

Para detectar esas filas lo que se hace es guardarse cuales son los monomios principales de elementos del resultado del preprocesamiento simbólico y después de hacer la reducción por filas agarrar solo las filas que tengan un uno principal en una columna que no corresponda a uno de esos monomios principales.

El siguiente seudocódigo hace eso:

\begin{algorithm}[H] % La H es para que se quede acá, porque se iba a otra página. Estaría bueno hacerlo global
  \caption{Multireducción}\label{alg:Multireducción}
  \KwData{$G, F ⊆ K⟨X⟩$ conjuntos finitos}
  \KwResult{$F' ⊆ K⟨X⟩$}
  $G' ←$ Preprocesamiento simbólico$(G, F)$

  $monomios ← \{\lm(g) : g ∈ G'\}$

  $M ← \{m : m ∈ \sop(g), g ∈ G' ∪ F\}$

  $mat ← \mat_M(G' ∪ F)$

  hacer reducción por filas de $mat$

  $F' ← \{\poli_M(v) : v ∈ \filas(mat) : v ≠ 0 ∧ \lm(\poli_M(v)) ∉ monomios\}$

  \Return{$F'$}
\end{algorithm}

Y con esto se puede escribir el algoritmo F4:

\begin{algorithm}[H] % La H es para que se quede acá, porque se iba a otra página. Estaría bueno hacerlo global
  \caption{F4}\label{alg:F4}
  \KwData{$G ⊆ K⟨X⟩$, $e_≤$ un reductor}
  \KwResult{$B ⊆ K⟨X⟩$ una base de Gröbner de $(G)$ si es que termina}
  $B ← G$

  \Loop{} {
    $ambs ← \amb(B)$

    $B' ← B$

    \While{$ambs ≠ ∅$} {
      elegir $Α ⊆ ambs$

      $ambs ← ambs - Α$

      $P ← \{\S(α) : α ∈ Α\}$

      $P ←$ Multireducción$(B', P)$

      $B' ← B' ∪ P$
    }

    \If{$B' = B$} {
      \Break
    }

    $B ← B'$
  }

  \Return{$B$}
\end{algorithm}

Este algoritmo no especifica cuáles ambigüedades elegir, pero una buena estrategia es elegir todas las de manor grado. Otra opción es elegir todas, pero eso puede hacer que las matrices se vuelvan muy grandes más rápido.

En \cite{thesis:Hof20} se da un algoritmo muy parecido a este (con una diferencia mínima) y se demuestra que es correcto.

Las optimizaciones mencionadas en la sección de Buchberger para descartar ambigüedades y para sacar polinomios de la base también se pueden hacer acá. Otra optimización que se puede hacer acá es usar un algoritmo de reducción de matrices que anda más rápido en las matrices de este problema en particular, llamado eliminación Faugère-Lachartre. Esto también es abordado en la \cref{secton:optimizaciones}.

\chapter{Librería}

Como se dijo al principio, para esta tesis se implementaron los algoritmos de Buchberger y F4 en C++ junto con estructuras para manejar polinomios no conmutativos en una librería \texttt{ncgb}. En este capítulo se explica cómo usar esa librería en su versión actual.

Esa librería está en GitHub en el repositorio \href{https://github.com/IvanRenison/Non-commutative-Grobner-Bases}{\texttt{Non-commutative-Grobner-Bases}}.

\section{Estructura de las carpetas}

El repositorio tiene varias carpetas, pero hay dos que son las más relevantes para el o la usuaria.

La primera es la carpeta \texttt{ncgb}, que es en donde está alojada la librería en sí. Tiene muchos archivos \texttt{.hpp} que son los que hay que incluir para usar la librería y que tienen la implementación ahí mismo.

En algunas librerías a veces hay archivos \texttt{.h} con las declaraciones y archivos \texttt{.cpp} con las implementaciones, pero acá eso no es así por el uso de templates.

La otra carpeta relevante es la carpeta \texttt{mains}, que tiene varios archivos \texttt{.cpp} con funciones \texttt{main} que hacen distintas cosas usando la librería, cada uno con una pequeña explicación de qué hace. Estos archivos son útiles para ver ejemplos de cómo usar la librería.

\section{Estructura general de los códigos} % Capás este no es el mejor nombre para este sección, pero no se me ocurre otro

Todo el código de la librería está en el namespace \texttt{ncgb}, así que para usar cualquier cosa hay que abrir \texttt{ncgb} o usar con \texttt{ncgb::}. Para las explicaciones vamos a ver todo como con el namespace abierto.

Los polinomios como los definimos en el \cref{cap:Preliminares} están asociados a un cuerpo y un conjunto de variables, y además muchas cosas de los polinomios (y de otras definiciones) dependen de un orden monomial.

Para que al usar el código también se puedan variar esas tres cosas todas las funciones y estructuras están hechas como templates con esas tres cosas como argumentos. Por ejemplo, la definición de polinomio es así:

\begin{minted}{C++}
  template<typename K, typename X = __uint8_t, class ord = DegLexOrd<X>>
  struct Poly {
    // … Implementación
  };
\end{minted}

¿Que qué son los templates en C++? Recordatorio para quienes no lo recuerdan bien: los templates en C++ permiten escribir código genérico que funciona para diferentes tipos, configuraciones o comportamientos sin necesidad de duplicar el código a mano. Escribir código con templates es equivalente a escribir una copia del código por cada combinación de parámetros de los templates.

El tipo \texttt{K} tiene que tener implementadas todas las operaciones de cuerpo. En particular, se asume que se pueden usar las siguientes cosas:

\begin{itemize}
 \item \texttt{+}, \texttt{-} (tanto unario como binario), \texttt{*} y \texttt{/} junto con sus versiones de asignación \texttt{+=}, \texttt{-=}, \texttt{*=} y \texttt{/=}, pero no se usa el resultado de estos operadores, solo el que modifican el operando izquierdo, así que pueden devolver \texttt{void}.
 \item \texttt{==} (y también \texttt{!=}, pero ese a partir de C++20 está automáticamente con \texttt{==}).
 \item Se asume que se puede construir un \texttt{K} con \texttt{0} y con \texttt{1}, o sea, que se puede hacer \texttt{K(0)} y \texttt{K(1)}.
 \item Para imprimir los polinomios se asume \texttt{<<} y para leerlos \texttt{>>}.
\end{itemize}

En \texttt{extras/ModularArithmetic.hpp} se puede ver una implementación de aritmética modular que tiene todo lo que \texttt{K} tiene que tener (y algunas cosas extra). Para los números racionales el tipo \texttt{mpq\_class} de la librería GMP también tiene todo lo que \texttt{K} tiene que tener \cite{lib:gmp}. Esos dos tipos son los que se usan en los mains de ejemplo de la carpeta \texttt{mains}.

El tipo \texttt{X} es lo que se usa para representar las variables como números y la idea es que se use algún tipo de números sin signo. Usar otras cosas podría funcionar pero no está probado. Por defecto se usa \texttt{\_\_uint8\_t} que permite tener hasta 256 variables. Para usar más variables hay que cambiarlo a un entero sin signo más grande como \texttt{\_\_uint32\_t}.

\texttt{ord} es el orden monomial que se usa y está puesto por defecto \texttt{DegLexOrd} que es el orden lexicográfico por grado y ya está definido junto con la definición de monomios. Para usar otro orden hay que definirlo definiendo una estructura que tenga definido un operador \texttt{()} que sea el orden. Algo así podría ser el código si es una implementación para un \texttt{X} genérico:

\begin{minted}{C++}
  template<typename X>
  struct Orden {
    bool operator()(const Monomial<X>& a, const Monomial<X>& b) const {
      // … Implementación
    }
  };
\end{minted}

Esta es la forma estándar de definir órdenes para ser parámetros de template en C++.

Tanto para \texttt{X} como para \texttt{ord} los valores por defecto están puestos solo en las estructuras de monomios y polinomios, porque en las otras funciones y estructuras no hace falta porque para usarlas hay que ya tener un monomio o un polinomio (que se pase como argumento) y de ahí C++ infiere el \texttt{X} y el \texttt{ord} automáticamente.

\section{Monomios}

Los monomios están definidos en el archivo \texttt{nc\_monomial.hpp} y lo más importante de su funcionalidad se puede resumir así:

\begin{minted}{C++}
  template<typename X = __uint8_t>
  struct Monomial {
    Monomial();
    Monomial(const std::vector<X>& vals);

    bool operator==(const Monomial& m) const;
    Monomial operator*(const Monomial& m) const;
    void operator*=(const Monomial& m);
    size_t size() const;
    bool empty() const;

    // Returns the positions of m.vals where this monomial divides m
    std::vector<size_t> divide_indexes(const Monomial& m) const;

    // Does this monomial divides m?
    bool divides(const Monomial& m) const;

    // Make all possible divisions of m by this monomial
    std::vector<std::pair<Monomial, Monomial>> divide(const Monomial& m) const;

    friend std::ostream& operator<<(std::ostream& os, const Monomial& m);
    friend std::istream& operator>>(std::istream& is, Monomial& m);

    void nice_print(std::ostream& os = std::cout) const;
    static Monomial nice_read(std::istream& is = std::cin);
  };
\end{minted}

Acá el único argumento del template es \texttt{X}, porque no hace falta nada del cuerpo ni tampoco un orden monomial (de hecho no tendría sentido que haga falta un orden monomial para definir monomios).

El tipo \texttt{Monomial<X>} tiene implementada la multiplicación con \texttt{operator*} (y \texttt{operator*=}), la igualdad con \texttt{operator==}, un método \texttt{size} que devuelve el grado, un método \texttt{empty} que devuelve \texttt{true} si y solo si el monomio es el monomio vacío y varios métodos relacionados a la división y a la representación de los monomios como strings.

Relacionado a la división el método más importante es \texttt{divide\_indexes} que se llama como \texttt{m0.divide\_indexes(m1)} y devuelve un vector de \texttt{size\_t} que indica las posiciones de \texttt{m1} donde empieza una sub-palabra igual a \texttt{m0}.

Respecto a la representación como strings, hay dos pares de métodos. Por un lado están \texttt{operator>>} y \texttt{operator<<} que leen y escriben en un formato numérico que es cómodo para usar en código. Por otro lado \texttt{nice\_read} y \texttt{nice\_print} que leen y escriben en el formato que se suele usar al escribir a mano los monomios.

El formato numérico consiste en un número entero no negativo $n$ seguido de $n$ números $x_1$, …, $x_n$ que son los números de variables. Cuando $n = 0$ no hay ningún $x_i$. Se puede representar así el formato:

\begin{lstlisting}[escapechar=+]
  +$n$+ +$x_1$+ +$⋯$+ +$x_n$+
\end{lstlisting}

Por ejemplo, acá hay un monomio en el formato como se suelen escribir a mano:

\begin{lstlisting}
  adbda
\end{lstlisting}

\noindent Y acá está el mismo monomio en el formato numérico:

\begin{lstlisting}
  5 0 3 1 3 0
\end{lstlisting}

Por ejemplo, para declarar un monomio con \texttt{X = \_\_uint8\_t}, leerlo en formato numérico e imprimirlo en formato bonito se puede hacer así:

\begin{minted}{C++}
  Monomial m;
  std::cin >> m;
  m.nice_print();
\end{minted}

\section{Polinomios}

Los polinomios están definidos en el archivo \texttt{nc\_polynomial.hpp} y lo más importante de su funcionalidad se puede resumir así:

\begin{minted}{C++}
  template<typename K, typename X = __uint8_t, class ord = DegLexOrd<X>>
  struct Poly {
    Poly();
    Poly(const Monomial<X>& m, K c = K(1));
    Poly(std::vector<std::pair<Monomial<X>, K>> p);

    bool operator==(const Poly& p) const;

    Poly operator+(const Poly& p) const;
    Poly operator-(const Poly& p) const;
    Poly operator*(const Poly& p) const;
    Poly operator*(Monomial<X> m) const;
    Poly operator*(K c) const;
    Poly operator/(K c) const;
    Poly operator-() const;
    void operator+=(const Poly& p);
    void operator-=(const Poly& p);
    void operator*=(const Poly& p);
    void operator*=(Monomial<X> m);
    void operator*=(K c);

    K coeff(const Monomial<X>& m) const;

    const Monomial<X>& lm() const;
    K lc() const;
    Poly lt() const;
    bool monic() const;
    bool isZero() const;

    friend std::ostream& operator<<(std::ostream& os, const Poly& p);
    friend std::istream& operator>>(std::istream& is, Poly& p);

    void nice_print(std::ostream& os = std::cout) const;
    static Poly nice_read(std::istream& is = std::cin);
  };
\end{minted}

Para construir un polinomio hay tres constructores: el constructor vacío que produce el polinomio $0$, un constructor que toma un monomio $m$ y un elemento del cuerpo $c$ y produce el polinomio $cm$, y un constructor que toma un vector de pares monomio-coeficiente y produce el polinomio que es la suma de cada coeficiente multiplicado con su monomio.

El tipo \texttt{Poly} tiene implementadas todas las operaciones entre polinomios (como la suma) con los operadores correspondientes (como \texttt{operator+} y \texttt{operator+=}, por ejemplo), métodos algunas de las cosas definidas en la \cref{def:cosas de polinomios} (\texttt{coeff}, \texttt{lm}, \texttt{lc} y \texttt{lt}) y métodos relacionados a la representación de los polinomios como strings.

Al igual que con los monomios, para la representación como strings, hay dos pares de métodos: \texttt{operator>>} y \texttt{operator<<} para formato numérico, y \texttt{nice\_print} y \texttt{nice\_read} para formato visualmente bonito.

El formato numérico consiste en, primero, un número entero no negativo $m$ (la cantidad de términos), seguido de la descripción de $m$ términos. La descripción de cada término consiste en primero el coeficiente y después la descripción del monomio. Se puede representar así el formato:

\begin{lstlisting}[escapechar=+]
  +$m$+
  +$c_1$+ +$n_1$+ +$x_{1, 1}$+ +$⋯$+ +$x_{1, n_1}$+
  +$⋮$+
  +$c_m$+ +$n_m$+ +$x_{m, 1}$+ +$⋯$+ +$x_{m, n_m}$+
\end{lstlisting}

Por ejemplo, acá hay un polinomio en el formato como se suelen escribir a mano:

\begin{lstlisting}
  3 aaa - 5 bcc + adbda
\end{lstlisting}

\noindent Y acá está el mismo polinomio en el formato numérico:

\begin{lstlisting}
  3
  3 3 0 0 0
  -5 3 1 2 2
  1 5 0 3 1 3 0
\end{lstlisting}

Por ejemplo, para declarar un polinomio con \texttt{K = mpq\_class}, leerlo en formato numérico e imprimirlo en formato bonito se puede hacer así:

\begin{minted}{C++}
  Poly<mpq_class> p;
  std::cin >> p;
  p.nice_print();
\end{minted}

\section{Buchberger y F4}

Los algoritmos de Buchberger y F4 están ambos hechos de una forma que se usan similar, así que por eso están explicados juntos.

Como ambos algoritmos tienen el problema de que pueden no terminar, no conviene hacer directamente una función que tome el conjunto generador y devuelva una base de Gröbner porque podría no terminar (o por lo menos no que sea la única forma de usar los algoritmos).

Se podría hacer una función que además del conjunto generador tome un número que sea la cantidad de pasos a ejecutar y que además de devolver un conjunto devuelva si se llegó a una base de Gröbner o no. Esta opción tiene el problema de que si se quiere después hacer más pasos de la ejecución habría que volver a empezar, llamando a la función con un número más grande.

Para evitar ese problema lo que se hizo es definir una estructura que el constructor tome a los polinomios y tenga un método \texttt{next} que calcula un paso más del cálculo de la base de Gröbner y en caso de haber terminado lo dice y un método \texttt{fullBase} para usar solo en el caso de que se sepa que hay una base finita, que hace las llamadas a \texttt{next} hasta que termina y devuelve la base.

Para Buchberger la estructura está en el archivo \texttt{Buchberger.hpp} y se llama \texttt{BuchbergerIncremental} y para F4 la estructura está en el archivo \texttt{F4.hpp} y se llama \texttt{F4Incremental}. En el caso de Buchberger el método \texttt{next} devuelve un \texttt{std::optional<Poly<K, ord>>} que es vacío solo en el caso de que ya se haya llegado a una base de Gröbner y si no tiene un polinomio de la base de Gröbner. En el caso de F4 el método \texttt{next} devuelve un \texttt{std::vector<Poly<K, ord>>} que es vacío solo en el caso de que ya se haya llegado a una base de Gröbner y si no tiene varios polinomios de la base de Gröbner. En ambos casos devolver vacío es la forma de decir que ya terminó el algoritmo.

La interfaz de Buchberger podría describirse así:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  struct BuchbergerIncremental {
    BuchbergerIncremental(const std::vector<Poly<K, X, ord>>& G);
    std::optional<Poly<K, X, ord>> next();
    std::vector<Poly<K, X, ord>> fullBase();
  };
\end{minted}

Y la de F4 así:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  struct F4Incremental {
    F4Incremental(const std::vector<Poly<K, X, ord>>& G);
    std::vector<Poly<K, X, ord>> next();
    std::vector<Poly<K, X, ord>> fullBase();
  };
\end{minted}

En ambos casos el conjunto generador se toma como un vector porque usar un set de C++ sería más costoso innecesariamente.

Para ambos algoritmos hay además una función \texttt{inIdeal} que toma un conjunto generador, un polinomio y una cantidad de pasos y trata de decir si el polinomio está en el ideal generado por el conjunto generador haciendo esa cantidad de llamadas a \texttt{next}. Esa función devuelve un elemento del siguiente tipo (con los significados que están comentados):

\begin{minted}{C++}
  enum IdealMembershipStatus {
    InIdeal,    // The element is definitely in the ideal
    NotInIdeal, // The element is definitely not in the ideal
    Unknown     // More steps are needed to determine if the element is in the ideal
  };
\end{minted}

\noindent Y está declarada así:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  IdealMembershipStatus
  inIdeal(const std::vector<Poly<K, X, ord>>& G, Poly<K, X, ord> f, size_t st = 20);
\end{minted}

Esa función sería básicamente la que trata de responder a la pregunta que planteamos al comienzo de si un polinomio está en el ideal generado por un conjunto generador.

\section{Construcción de la respuesta}\label{section:construcción de la respuesta (librería)}

La función \texttt{inIdeal} sólo decide la pertenencia al ideal generado, pero en caso afirmativo no da una forma de escribirlo como combinación lineal de elementos del conjunto generador con polinomios como coeficientes.

Con las bases de Gröbner pasa lo mismo, los algoritmos dan la base pero no dan una forma de escribir los elementos de la base usando los elementos del conjunto generador.

Sin embargo esa información se puede calcular y ese cálculo está implementado para Buchberger en el mismo archivo \texttt{Buchberger.hpp} en unas funciones y estructuras con nombres que terminan en \texttt{Reconstruct} (esta terminación es porque van guardando la información que permite \emph{reconstruir} la respuesta).

Estas funciones devuelven elementos del tipo \texttt{InIdealPoly} definido en el archivo \texttt{nc\_polynomial\_inIdeal.hpp}. Este tipo representa un polinomio como una combinación lineal de elementos del conjunto generador. En particular, guarda un vector de elementos $m$, $i$, $m'$, $c$ de forma que, si los $g_i$ son el conjunto generador, el polinomio representado es $∑ m g_i m' c$.

Para F4 esto no está implementado (en el siguiente capítulo se explica más del porqué).


\section{Comparación de bases}

Por último, en el archivo \texttt{cmpBases.hpp} está la función \texttt{cmpBases} que toma dos conjuntos generadores y, asumiendo que son bases de Gröbner, dice si generan el mismo ideal o no.

\section{Ejemplo}

Ahora un pequeño ejemplo de uso de la librería, con un ejemplo similar al del archivo \texttt{mains/base\_Buchberger.cpp}:

\begin{minted}{C++}
  #include <bits/stdc++.h>
  #include <gmpxx.h>
  #include "ncgb/Buchberger.hpp"
  using namespace std;
  using namespace ncgb;

  typedef Poly<mpq_class> P;

  int main() {
    size_t n;
    cin >> n;
    vector<P> G(n);
    for (size_t i = 0; i < n; ++i) cin >> G[i];

    BuchbergerIncremental<P> bi(G);
    vector<P> base = bi.fullBase();

    cout << base.size() << endl;
    for (P& f : base) f.nice_print();
  }
\end{minted}

El código de este ejemplo lo que hace es, trabajando sobre los racionales, leer un conjunto generador en formato numérico, calcularle una base de Gröbner asumiendo que tiene una base finita e imprime la base de forma bonita.


\chapter{Implementación}

En este capítulo se explican los detalles de cómo está hecha la implementación.

\section{Monomios}

Los monomios, o sea los elementos de $⟨X⟩$, como son palabras se implementaron usando vectores de \texttt{X}. La base de la implementación es así:

\begin{minted}{C++}
  template<typename X = __uint8_t>
  struct Monomial {
    std::vector<X> vals;
    // … Métodos
  };
\end{minted}

Con esto, usando \texttt{X = \_\_uint8\_t}, que tiene 8 bits, se pueden tener hasta 256 variables. Cuando los monomios se imprimen o leen de forma bonita solo hay 26 variables, correspondiendo el 0 con la \texttt{a}. Si se quiere imprimir de forma bonita un monomio que usa variables mayores o iguales a 26 salta una aserción.

Esta estructura tiene implementadas las operaciones y métodos que se describieron en el capítulo anterior. La mayoría tienen una implementación directa. Las únicas no directas son las relacionadas a la división, porque chequear divisibilidad es chequear si una palabra es sub-palabra de otra, lo cual la forma directa de hacerlo lleva tiempo cuadrático (en el largo de las palabras), pero se puede hacer en tiempo lineal.

Hay muchas formas de hacerlo en tiempo lineal, la que se usó es la función Z (que en el código está en el archivo \texttt{Zfunc.hpp}). En \cite{web:cp-algo:Zfunc} se explica la función Z y cómo usarla para chequear si una palabra es sub-palabra de otra.
% También quizás tiene sentido aclarar que no es la misma que la función ζ de Riemann, que es lo primero que aparece al buscar en google "función Z"

Junto con la implementación de los monomios, en el archivo \texttt{nc\_monomial.hpp} está la implementación del orden lexicográfico por grado, que también es directa.

\section{Polinomios}

Los polinomios, o sea los elementos de $K⟨X⟩$ están implementados usando un vector de pares monomio-coeficiente que siempre se mantiene ordenado por el orden monomial. La base de la implementación es así:

\begin{minted}{C++}
  template<typename K, typename X = __uint8_t, class ord = DegLexOrd<X>>
  struct Poly {
    std::vector<std::pair<Monomial<X>, K>> terms;
    // … Métodos
  };
\end{minted}

Con esta estructura para los polinomios todas las operaciones se hacen de forma directa.

El orden de los polinomios está definido como \texttt{template<typename K, typename X, class ord> struct PolyOrd}.

\section{Reducción}

La reducción están implementadas en el archivo \texttt{reductions.hpp}, en particular en la función \texttt{reduce} que toma un polinomio y un vector de polinomios y reduce el polinomio con los polinomios del vector. La reducción se hace modificando el propio argumento. Esa función sería una implementación de un reductor concreto.

Esa reducción concreta lo que hace es siempre tratar de reducir por los primeros elementos del vector primero y tratar de reducir primero los monomios más grandes del polinomio.

En el archivo también hay una función \texttt{reduce} que además toma un vector de booleanos que tiene que ser del mismo largo que el vector de polinomios y hace la reducción solo con los polinomios que tengan un \texttt{true} en la misma posición en el vector de booleanos. Esta función está para poder implementar la optimización de ir eliminando algunos elementos de la base sin tener que estar modificando un vector.

\section{Ambigüedades}

Las ambigüedades, que son necesarias para el algoritmo de Buchberger, están implementadas en el archivo \texttt{ambiguities.hpp} y consisten en una estructura así:

\begin{minted}{C++}
  template<typename X>
  struct Amb {
    const Monomial<X>& p, q;
    enum Type { Inclusion, Overlap };
    Type type;
    size_t pos; // position where q starts in p
    Monomial<X> a, b;

    size_t size() const;
    Monomial<X> lm() const;
  };
\end{minted}

Los campos de la ambigüedad son:

\begin{itemize}
  \item \texttt{p} y \texttt{q} son referencias a los monomios sobre los cuales es la ambigüedad. El motivo por el cual se guardan en la estructura es para poder implementar una de las optimizaciones.
  \item \texttt{type} indica si la ambigüedad es de inclusión o de sobreposición.
  \item \texttt{pos} es la posición de \texttt{p} donde empieza el pedazo que es en común con \texttt{q} y por el cual existe la ambigüedad.
  \item \texttt{a} y \texttt{b} son los monomios que hacen que en el caso de las de inclusión \texttt{p} sea igual a \texttt{aqb} y en el caso de las de sobreposición \texttt{ap} sea igual a \texttt{qb}.
\end{itemize}

En el archivo también está la función \texttt{ambiguities} que toma dos monomios y devuelve un vector de \texttt{Amb} con todas las ambigüedades entre esos dos monomios. En esta función, al igual que en la divisibilidad de monomios, se usa la función Z para evitar tener que hacer algo cuadrático en los largos de los monomios.

Después, en el archivo está la función \texttt{S\_poly} que toma una ambigüedad y dos polinomios que deberían ser los polinomios correspondientes y devuelve el S-polinomio correspondiente.

Y por último está la función \texttt{checkDeletionCriteria} que implementa la optimización que permite descartar algunas ambigüedades sin tener que reducirlas.

\section{Buchberger}

Como ya se dijo antes, por el tema de que el algoritmo puede no terminar, se hace lo de usar una estructura con un método \texttt{next}. En esa estructura ya están implementadas las optimizaciones antes mencionadas, pero en esta sección primero se explica cómo sería la estructura sin esas optimizaciones y después en la sección \cref{secton:optimizaciones} se explica algo de cómo se agregan.

La base de esa estructura es así:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  struct BuchbergerIncremental {
    std::vector<Poly<K, X, ord>> G;
    std::queue<std::tuple<Amb<X>, size_t, size_t>> ambs;
    size_t t = 0;
  };
\end{minted}

Los campos de esta estructura guardan lo siguiente:

\begin{itemize}
  \item \texttt{G} es la base de Gröbner que se está construyendo. Al principio se inicializa con los polinomios con los que se llama al constructor.
  \item \texttt{ambs} son las ambigüedades que todavía no se procesaron junto con los índices en \texttt{G} de los polinomios a los que corresponde la ambigüedad. Se usa una cola para procesar siempre la que hace más tiempo está esperando.
  \item La variable \texttt{t} está porque como la base de Gröbner incluye a los polinomios originales, las primeras llamadas a \texttt{next} tienen que devolver esos polinomios y \texttt{t} lo que hace es indicar cuántos de esos ya se devolvieron. Cuando \texttt{t} es igual al tamaño de \texttt{G} es porque ya se devolvieron todos esos.
\end{itemize}

La estructura además de tener los métodos \texttt{next} y \texttt{fullBase} tiene varios métodos que se usan internamente. Entre todos los métodos son los siguientes:

\begin{itemize}
  \item \texttt{add\_amb}: Este método simplemente toma una ambigüedad y dos índices y los agrega a la cola de ambigüedades. Siempre que hay que agregar una ambigüedad se hace con este método. El motivo por el cual esto está en un método propio es para después, al implementar la optimización de descartar ambigüedades, poder hacerlo solo en este método.
  \item \texttt{add\_poly}: Este método agrega un polinomio a \texttt{G} agregando además todas las nuevas ambigüedades que aparecen a la cola de ambigüedades. Cada vez que hay que agregar un nuevo polinomio se usa este método, tanto al comienzo cuando se agregan los polinomios iniciales como cuando se agrega un S-polinomio reducido.
  \item \texttt{next}: Este es el método que corre el algoritmo. Lo principal que hace (después de usar la variable \texttt{t} para ver si tiene que devolver un elemento de \texttt{G}) es sacar ambigüedades de la cola hasta encontrar una que no se reduzca a $0$ y cuando la encuentra la agrega a la base (con \texttt{add\_poly}) y la devuelve. Si se acaban las ambigüedades devuelve vacío (recordar que devuelve \texttt{std::optional<Poly<K, ord>>}).
  \item \texttt{fullBase}: Simplemente lo que hace es llamar a \texttt{next} en un ciclo hasta que devuelva vacío y después devuelve la base.
\end{itemize}

Lo de las optimizaciones se explica más adelante en la \cref{secton:optimizaciones}.


\section{F4}

F4 es un poco más complejo que Buchberger y tiene más partes, así que esta sección está dividida en varias subsecciones para las distintas partes.

\subsection{Reducción por filas de matrices}

Para F4 hace falta hacer la reducción por filas de una matriz. Para esto, en el archivo \texttt{matrix.cpp} se definió un tipo matriz como un vector de vectores y una función \texttt{rref} que hace una eliminación gaussiana directamente y que funciona para cualquier \texttt{K}. Sin embargo, la reducción por filas de una matriz es un tema ya muy analizado y hay algunas librerías que lo hacen para distintos tipos \texttt{K} mucho más rápido.

Una librería que hace la reducción por filas es FLINT, que lo hace para el tipo \texttt{mpq\_class} de la librería GMP (que son números racionales) \cite{lib:flint, lib:gmp}. Para usarla, en el archivo \texttt{matrix\_mpq\_class.hpp} hay una especialización de \texttt{rref} para \texttt{mpq\_class}.

¿Qué que es una especialización en C++? Recordatorio para quienes no lo recuerdan bien: Es cuando hay código definido con un template y se hace una definición aparte para alguna combinación particular de parámetros del template.

Algo malo que tiene esa especialización es que las matrices de racionales con las que trabaja FLINT no están definidas como vectores de vectores, sino que son su propio tipo \texttt{fmpq\_mat\_t}, así que lo que la función hace es primero copiar la matriz a una matriz de FLINT, después hacer la reducción por filas y después copiar el resultado de vuelta a la matriz original.

El motivo por el cual primero se hizo eso es que esas matrices de FLINT se usan de una forma muy particular y complicada. Por ejemplo, para asignarle un valor \texttt{x} de tipo \texttt{mpq\_class} a una posición de la matriz hay que hacer \texttt{fmpq\_set\_mpq(fmpq\_mat\_entry(mat, i, j), x.get\_mpq\_t());}. Esto hace que si se quisiera directamente en F4 trabajar con las matrices de FLINT no se podría hacer el código genérico para cualquier \texttt{K}.
% Quizás se podría decir que el motivo por el cual lo de FLINT es como es probablemente sea porque es una librería de C?

El otro lugar en el que hay diferencia, y mucha, es en \texttt{next}. En F4 este método lo que hace es, agarrar todas las ambigüedades de menor grado, poner todos los S-polinomios correspondientes en un vector, hacer la reducción de esos polinomios con la función \texttt{multiReduction} y, si hay alguno que queda no nulo, agregar los polinomios reducidos a la base, agregando también las nuevas ambigüedades que aparecen y devolver esos nuevos polinomios, y si quedan todos nulos pasar a las ambigüedades de grado siguiente. Si no quedan más grados de ambigüedades se devuelve vacío.

\subsection{Preprocesamiento simbólico}

El preprocesamiento simbólico (\cref{alg:Preprocesamiento simbólico}) está implementado en el propio archivo \texttt{F4.hpp} en la función \texttt{symbolicPreprocessing}. La implementación es directa, así que no hay mucho para comentar.

\subsection{Reducción}

Como en F4 se reducen varios polinomios al mismo tiempo, en el código se implementó una función \texttt{multiReduction} que toma dos vectores de polinomios, la base actual y los polinomios a reducir y hace lo de reducirlos todos a la vez con la matriz.

En el \cref{alg:F4} esto se hizo dentro del propio algoritmo de F4 pero en la implementación se hizo aparte porque son bastantes cosas las que tiene que hacer el código.

La implementación lo que hace es construir la matriz (con \texttt{toMatrix}), después llamar a la función \texttt{rref} (que ya se explicó), después marcar que columnas corresponden a monomios principales del vector por el cual se está reduciendo (esto para poder saber cuales polinomios son los reducidos) y por último se convierten a polinomios las filas que su uno coeficiente principal (el primero no nula de izquierda a derecha) no está marcada como que es un monomio principal del vector por el cual se está reduciendo.

\subsection{El propio F4}

La primera diferencia tiene que ver con la selección de ambigüedades. En Buchberger solo había que elegir una ambigüedad, pero acá hay que elegir varias. Podría haber muchas formas de elegir, pero, como ya se dijo antes, una estrategia que funciona bien es elegir siempre todas las de menor grado. Para eso lo que se hizo es que las ambigüedades estén guardadas por grado en un vector, así:

\begin{minted}{C++}
  std::vector<std::vector<std::tuple<Amb<X>, size_t, size_t>>> ambs_per_deg;
\end{minted}

El otro lugar en el que hay diferencia, y mucha, es en \texttt{next}. En F4 este método lo que hace es, agarrar todas las ambigüedades de menor grado, poner todos los S-polinomios correspondientes en un vector, hacer la reducción de esos polinomios con la función \texttt{multiReduction} y, si hay alguno que queda no nulo, agregar los polinomios reducidos a la base, agregando también las nuevas ambigüedades que aparecen y devolver esos nuevos polinomios, y si quedan todos nulos pasar a las ambigüedades de grado siguiente. Si no quedan más grados de ambigüedades se devuelve vacío.

\section{Optimizaciones}\label{secton:optimizaciones}

Como ya se dijo varias veces, hay algunas optimizaciones que se pueden hacer en los algoritmos de Buchberger y F4, que están explicadas en la Sección 4.5 de \cite{thesis:Hof20}. Acá solo se van a explicar por arriba las optimizaciones y se va a explicar como están implementadas.

\subsection{Descartar ambigüedades}

La primer optimización consiste en que se pueden descartar algunas ambigüedades, sin tener que reducir su S-polinomio, según si pasan ciertas cosas con otros polinomios que ya están en la base.

En particular algunos teoremas de \cite{thesis:Hof20} permiten definir una función con la siguiente signatura:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  bool checkDeletionCriteria(std::vector<Poly<K, X, ord>>& G, Amb<X>& amb, size_t i, size_t j)
\end{minted}

Que toma la base, la ambigüedad y a que polinomios corresponde y devuelve \texttt{true} si y solo si esa ambigüedad se puede descartar sin reducirla.

Con eso lo que se hizo en el código es en el método \texttt{add\_amb} tanto de Buchberger como de F4, antes de agregar la ambigüedad, se llama a \texttt{checkDeletionCriteria} y solo se agrega la ambigüedad si da \texttt{false}.

\subsection{Eliminación de polinomios}

La segunda optimización consiste en que cuando se procesa una ambigüedad de inclusión se puede borrar de la base al polinomio grande de la ambigüedad.

Para recordar, una ambigüedad de inclusión entre los polinomio $f$ y $g$ es cuando se tiene que $\lm(f) = a \lm(g) b$ con $a, b ∈ ⟨X⟩$. Lo que esta optimización permite es, en esas ambigüedades borrar $f$ de la base.

Esta optimización se implementó en Buchberger pero no en F4, porque por algún motivo F4 anda más lento con esta optimización. No se investigó por qué pasa eso.

Para implementar esta optimización en Buchberger lo que se hizo no es directamente borrar el polinomio de \texttt{G}, porque para eso habría que también cambiar los índices que están guardados junto con las ambigüedades, sino que se agrego un vector de booleanos \texttt{removed} a la estructura, que tiene siempre el mismo largo que \texttt{G} y vale \texttt{true} solo en las posiciones de polinomios que se borraron. O sea que lo que se está haciendo es más bien marcar como borrados los polinomios.

Las funciones que se habían explicado que toman a \texttt{G} como parámetro y se usan en Buchberger lo que se hizo es agregarles un parámetro extra que sea el vector de booleanos y que solo trabaje con los polinomios de posiciones que no están con \texttt{true}.

\subsection{Reducción más eficiente en F4}

Como ya se dijo antes, para F4 también hay otra optimización que consiste en hacer la reducción por filas de la matriz de forma más eficiente aprovechando la estructura particular que tienen las matrices de F4. Esta reducción se llama reducción Faugère-Lachartre y está explicada en \cite{thesis:Hof20}.

Esta optimización no se implementó principalmente por el poco soporte para trabajar con matrices de un cuerpo arbitrario, o de una implementación específica de $ℚ$, que hay en C++. Sin embargo, queda como trabajo futuro en el \cref{cap:trabajos futuros}.

\section{Construcción de la respuesta}

Como se dijo en el capítulo anterior, la construcción de la respuesta se implementó solo para Buchberger, y no para F4. El motivo de esto es que, como para F4 de cualquier manera no está implementada la optimización de hacer la reducción más eficiente, se va a tener que seguir trabajando en la parte de la reducción y cuando se tenga lista la versión más eficiente es el momento de implementar la construcción de la respuesta.

Como se explicó en la \cref{section:construcción de la respuesta (librería)}, para la construcción de la respuesta se usa el tipo \texttt{InIdealPoly} que guarda polinomios de un ideal como combinación lineal de elementos de la base.

La estructura de la implementación de este tipo es así:

\begin{minted}{C++}
  template<typename K, typename X = __uint8_t, class ord = DegLexOrd<X>>
  struct InIdealPoly {
    std::vector<std::tuple<Monomial<X>, size_t, Monomial<X>, K>> terms;
    // … Métodos
  };
\end{minted}

Y tiene definidas las operaciones de suma, resta y producto entre polinomios, al igual que con los polinomios normales. Además tiene un método \texttt{add} para agregar una tupla monomio, índice, monomio, coeficiente al polinomio.

A diferencia de los polinomios comunes, acá \texttt{terms} no se mantiene ordenado de ninguna manera.

Con \texttt{InIdealPoly} se hizo la estructura \texttt{BuchbergerIncrementalReconstruct}, que es muy parecida a \texttt{BuchbergerIncremental} pero con algunos agregados, y se hicieron versiones \texttt{Reconstruct} de las funciones que se usan.

Para las reducciones se hizo una función con esta signatura:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  InIdealPoly<K, X, ord>
  reduceReconstruct(Poly<K, X, ord>& f, const std::vector<Poly<K, X, ord>>& G, const std::vector<InIdealPoly<K, X, ord>>& g_rec);
\end{minted}

La cual modifica a \texttt{f} y devuelve qué habría que restarle a la \texttt{f} original para obtener la \texttt{f} reducida.

También se hizo una versión de la función de los S-polinomios:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  std::tuple<Poly<K, X, ord>, std::tuple<Monomial<X>, Monomial<X>, K>, std::tuple<Monomial<X>, Monomial<X>, K>>
  S_polyReconstruct(const Amb<X>& amb, const Poly<K, X, ord>& f, const Poly<K, X, ord>& g);
\end{minted}

Que devuelve, además del S-polinomio, qué hay que multiplicarle a \texttt{f} y a \texttt{g}.

Con esto, en la estructura \texttt{BuchbergerIncrementalReconstruct} se agregó (con respecto a \texttt{BuchbergerIncremental}) un campo \texttt{std::vector<InIdealPoly<K, ord>> G\_rec} que guarda la construcción como elementos de la base de cada polinomio.

A los elementos iniciales simplemente se agregan como \texttt{{Monomial(), i, Monomial(), K(1)}} y a los que vienen de un S-polinomio se construye el \texttt{InIdealPoly} con los \texttt{InIdealPoly} de los polinomios que forman la ambigüedad multiplicados por los monomios y coeficientes que devuelve \texttt{S\_polyReconstruct} y con los de la reducción.

\chapter{Paralelismo}

Como ya se dijo, uno de los objetivos fue tratar de paralelizar el cálculo de bases de Gröbner no conmutativas para que sea más rápido.

Cuando se va a paralelizar algo siempre es importante primero tratar de optimizar la versión no paralela lo más posible. En este caso lo mejor no paralelo es el algoritmo F4, pero como no está implementada la optimización de la reducción eficiente, no está todavía optimizada al máximo la versión no paralela. De cualquier manera, como la reducción de matrices es solo una parte del algoritmo, se puede pensar cómo se podría paralelizar el resto.

El método \texttt{add\_poly} que ya se explicó que lo que hace es agregar un polinomio a la base y agregar las nuevas ambigüedades correspondientes, sin paralelizar se implementó así:

\begin{minted}{C++}
  void add_poly(const Poly<K, X, ord>& f) {
    G.push_back(f);
    for (size_t k = 0; k < G.size() - 1; k++) {
      for (auto& amb : ambiguities(G[k].lm(), f.lm())) {
        add_amb(amb, k, G.size() - 1);
      }
      for (auto& amb : ambiguities(f.lm(), G[k].lm())) {
        add_amb(amb, G.size() - 1, k);
      }
    }
  }
\end{minted}

Y el método \texttt{add\_amb} se implementó así:

\begin{minted}{C++}
  void add_amb(Amb<X>& amb, size_t i, size_t j) {
    if (checkDeletionCriteria(G, amb, i, j)) {
      return;
    }
    ambs.push({amb, i, j});
  }
\end{minted}

El primer \texttt{for} de \texttt{add\_poly} así como está casi que se podría paralelizar, lo único que lo impide es que \texttt{add\_amb} hace un \texttt{ambs.push} que sería problemático si llega a ejecutar al mismo tiempo por más de un hilo. Para solucionar eso lo que se hizo es guardar las ambigüedades en varios vectores, uno por cada polinomio de \texttt{G} y después al final de la función agregar todas las ambigüedades a \texttt{ambs}. Así:

\begin{minted}{C++}
  void add_poly(Poly<K, X, ord>& f) {
    G.push_back(f);
    size_t lim = G.size() - 1;
    std::vector<std::vector<std::tuple<Amb<X>, size_t, size_t>>> to_add(lim);

    for (size_t k = 0; k < lim; k++) {
      for (auto& amb : ambiguities(G[k].lm(), G.back().lm())) {
        if (!checkDeletionCriteria(G, amb, k, lim)) {
          to_add[k].push_back({amb, k, lim});
        }
      }
      for (auto& amb : ambiguities(G.back().lm(), G[k].lm())) {
        if (!checkDeletionCriteria(G, amb, lim, k)) {
          to_add[k].push_back({amb, lim, k});
        }
      }
    }

    for (size_t k = 0; k < lim; k++) {
      for (auto& [amb, i, j] : to_add[k]) {
        size_t d = amb.size();
        while (ambs_per_deg.size() <= d) {
          ambs_per_deg.push_back({});
        }
        ambs_per_deg[d].push_back({amb, i, j});
      }
    }
  }
\end{minted}

Con esto el primer \texttt{for} se puede paralelizar directamente.

\section{Uso de OpenMP para paralelizar}

Para paralelizar se usó la librería OpenMP \cite{lib:openmp}. Con OpenMP para que un \texttt{for} como el de \texttt{add\_poly} se corra en paralelo solo hace falta agregar en la línea anterior \texttt{\#pragma omp parallel for}, así que eso fue lo que se hizo. En el siguiente capítulo se dan los datos de qué tal anda.


\chapter{Benchmarks}

% Agregar datos de que tal anda la paralización y comparado con lo de Hof

\chapter{Trabajos futuros}\label{cap:trabajos futuros}

% Agregar

\chapter{Bibliografía}

\printbibliography[heading=none]

\end{document}
