\documentclass[12pt]{report}

\usepackage{fontspec}
\usepackage{polyglossia}
\setdefaultlanguage{spanish}

\usepackage{ccicons}

\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[bookmarks]{hyperref}
\defaultfontfeatures{Renderer=Basic,Ligatures={TeX}}
\usepackage[math-style=ISO,bold-style=ISO]{unicode-math}
\setmathfont{Asana Math}
\usepackage{witharrows}
\WithArrowsOptions{displaystyle,i,tikz={font={\small\normalfont}},ygap=0.5em,wrap-lines} % Agregar fleqn para que se ponga a la izquierda

\usepackage{titlesec}
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries}  % Estilo del título
  {Capítulo \thechapter}                 % Número del capítulo
  {2pt}                        % Espacio entre el número y el título
  {}                            % Formato del título
\titlespacing*{\chapter}{0pt}{-15pt}{25pt}  % Ajusta los márgenes

\DeclareEmphSequence{\bfseries}

\usepackage[a4paper, margin=2cm, top=1cm, bottom=1cm]{geometry} % Adjust the margin values as desired
\hfuzz=4pt % Las figuras de tikz dicen que tienen overfull hbox, pero poniendo los margenes (con showframe) se ve que no sobresale nada. Así que pongo esto para que no aparezcan los warnings

\usepackage{float}
\usepackage[ruled, vlined, linesnumbered, spanish]{algorithm2e}
\SetKw{Break}{break}
\SetKwFor{Loop}{loop}{}{end loop}

\usepackage{listings}
\lstset{basicstyle=\ttfamily} % Cambia la fuente a monospace
\usepackage{minted}
\setminted{autogobble}

\usepackage[capitalise, nameinlink]{cleveref}
\crefname{section}{Sección}{Secciones} % Para que use "Sección" en vez de "Apartado"
\crefname{subsection}{Subsección}{Subsecciones} % Para que use "Subsección" en vez de "Subapartado"

\usepackage[
  backend = biber,
  language = spanish,
  autolang = hyphen,
  bibencoding = inputenc,
  isbn = false,
  uniquename = false,
  style = alphabetic,
  abbreviate = false,
  %clearlang = true,
  date = iso
]{biblatex}
\addbibresource{biblio.bib}
% Página para buscar citas: zbmath.org

\setlist[enumerate,1]{label={\em{(\arabic*)}}}

\usepackage{fancyhdr}
\setlength{\footskip}{0.6cm} % Evita que los números de página se salgan de la página

\usepackage{pgfplots} % Para los gráficos

\newtheoremstyle{customstyle} % <name>
{} % <Space above>
{} % <Space below>
{\slshape} % <Body font> % Preguntar que opinan, y porque hay un warning
{} % <Indent amount>
{\bfseries} % <Theorem head font>
{.} % <Punctuation after theorem head>
{.5em} % <Space after theorem head>
{} % <Theorem head spec (can be left empty, meaning `normal`)

\theoremstyle{customstyle}
\newtheorem{theorem}{Teorema}[chapter]
\newtheorem{definition}[theorem]{Definición}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{colorary}[theorem]{Colorario}
\newtheorem{problem}{Problema}
\newtheorem{observation}[theorem]{Observación}
\addto\captionsspanish{\renewcommand{\proofname}{Demostración}}
\renewenvironment{proof}[1][\proofname]{{\noindent \bfseries #1: }}{\qed} % Cambiar estilo del título de la demostración

\newtheoremstyle{factstyle} % <name>
{} % <Space above>
{} % <Space below>
{\normalfont} % <Body font> % Preguntar que opinan, y porque hay un warning
{1.5em} % <Indent amount>
{\bfseries} % <Theorem head font>
{.} % <Punctuation after theorem head>
{.5em} % <Space after theorem head>
{} % <Theorem head spec (can be left empty, meaning `normal`)

\theoremstyle{factstyle}
\newtheorem{fact}{Afirmación}[theorem]


\newcommand\cpp{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\relsize{-3}{\textbf{++}}}\xspace}
\newcommand\cppXX{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\relsize{-3}{\textbf{++}}}20\xspace}

\DeclareMathOperator{\sop}{sop}
\DeclareMathOperator{\lm}{lm}
\DeclareMathOperator{\lc}{lc}
\DeclareMathOperator{\lt}{lt}
\DeclareMathOperator{\tail}{tail}
\DeclareMathOperator{\amb}{amb}
\renewcommand{\S}{\text{S}}
\DeclareMathOperator{\B}{B}
\DeclareMathOperator{\spn}{span} % span está ocupado y re rompe todo si lo re-declaro
\DeclareMathOperator{\mat}{mat}
\DeclareMathOperator{\poli}{poli}
\DeclareMathOperator{\filas}{filas}

% Convenciones de uso de letras:
% X: Alfabeto
% K: Cuerpo
% R: Anillo
% I: Ideal
% G: Conjunto generador
% F: Conjunto de polinomios (que no sea un conjunto generador)
% f, g: Polinomios
% m: Monomios
% M: Conjunto de monomios
% c, k: Coeficiente de K
% a, b, c, d: Coeficientes de monomios
% α: Ambigüedades
% n: Límite de sumatorias
% i, j: Índices

% Metadatos del PDF
\hypersetup{
  pdftitle = {Implementación de bases de Gröbner no conmutativas en C++ con un poquito de paralelismo},    % Título del documento
  pdfauthor = {Iván Ariel Renison},        % Autor del documento
  pdfsubject = {Computación simbólica y computación algebraica},         % Asunto o tema del documento
  pdfkeywords = {
    Polinomios no conmutativos,
    Bases de Gröbner no conmutativas,
    Algoritmo de Buchberger no conmutativo,
    Algoritmo F4 no conmutativo,
    Sistemas de reescritura,
    Implementación
  }, % Palabras clave
  % pdfcreator = {XeLaTeX},               % Herramienta de creación
  % pdfproducer = {XeLaTeX con hyperref}  % Productor del PDF
}


\begin{document}

\def\tituloTesis{Implementación de bases de Gröbner no conmutativas en \cpp con un poquito de paralelismo} % Repito el título porque en los metadatos tengo que poner C++ y acá \cpp
\input{Carátula}

\section*{Resumen}

Esta tesis presenta una implementación en \cpp para el cálculo de las llamadas bases de Gröbner no conmutativas. Ya existían previamente algunas implementaciones de dicho cálculo, pero la que se presenta en esta tesis es la primera en \cpp y además incluye un poquito de paralelismo.

En los primeros capítulos se proporciona una explicación matemática completa del tema, para que la tesis sea autocontenida y en los restantes se explica la librería de \cpp donde se implementó el cálculo (junto con las estructuras y algoritmos auxiliares necesarios) y se la compara con una de las implementaciones anteriores.

\section*{Abstract}

This thesis presents an implementation in \cpp for the computation of the so called noncommutative Gröbner bases. Some implementations of this computation already exist previously, but the one presented in this thesis is the first one in \cpp and it also includes a little bit of parallelism.

In the first chapters a complete mathematical explanation of the subject is provided, so that the thesis is self contained, and in the remaining chapters the \cpp library where the calculation was implemented (together with the necessary structures and auxiliary algorithms) is explained and compared with one of the previous implementations.

\vfill

\subsection*{Clasificación (MSC2010)}

68W30: Computación simbólica y computación algebraica

\subsection*{Palabras clave}

\begin{itemize}
  \item Polinomios no conmutativos
  \item Bases de Gröbner no conmutativas
  \item Algoritmo de Buchberger no conmutativo
  \item Algoritmo F4 no conmutativo
  \item Sistemas de reescritura
  \item Implementación
\end{itemize}


\newpage

\section*{Reconocimientos}

% La idea de esta sección es listar a quienes ayudaron en algo. No se si está es la mejor forma de hacerla

A continuación una lista de quienes ayudaron en algo con este trabajo: % Además de yo, no se si eso lo debería decir y cómo

\begin{itemize}
  \item Mis directores Cristian Vay y Miguel Maria Pagano ayudaron un montón, tanto en la parte de aprender los temas, como en la parte de hacer el código y en la parte de escribir.
  \item Lucía Martinez Gavier leyó la tesis e hizo bastantes sugerencias.
  \item Álvaro Roy Schachner me ayudó con cosas de la computadora Atom y además leyó la tesis e hizo bastantes sugerencias.
  \item Felipe Clariá Dambolena leyó la tesis e hizo bastantes sugerencias.
  \item Agustín Garcia Iglesias como miembro del tribunal leyó la tesis e hizo algunas sugerencias.
  \item Ana María Cingolani leyó el resumen y la introducción e hizo algunas sugerencias.
\end{itemize}

Además se usaron recursos computacionales de la computadora Atom de FAMAF.


\tableofcontents

\chapter{Introducción}

Usted posiblemente haya escuchado hablar de los polinomios de varias variables, esos como $5 + 3 y - 2 x y + x^3 y^5$, y que todo el conjunto de los polinomios sobre el cuerpo $K$ y con variables en el alfabeto $X$ se denota $K[X]$. En esos polinomios el producto entre las variables conmuta; por ejemplo, el polinomio $x y$ es igual al polinomio $y x$. Sin embargo, se pueden considerar otros polinomios en los que el producto entre las variables no conmuta, así que por ejemplo, el polinomio $x y$ es distinto del polinomio $y x$; pero el producto de las variables con los coeficientes sí sigue conmutando, por ejemplo el polinomio $x 3 y$ es igual al polinomio $3 x y$. Estos nuevos polinomios, llamados polinomios no conmutativos, cuando son sobre el cuerpo $K$ y con variables en el alfabeto $X$ se denotan $K⟨X⟩$, y consisten en combinaciones lineales de palabras del alfabeto $X$ con coeficientes en $K$. Algunos ejemplos de polinomios no conmutativos en $ℚ⟨x, y, z⟩$ son los siguientes:
\begin{align*}
  f_0 &= x \text{,} \\
  f_1 &= xy + yz \text{,} \\
  f_2 &= 3 xyy - 2 xzxy + \frac{4}{3} yzzx \text{.}
\end{align*}

\noindent Notar que $f_1 ≠ xy + zy$ ya que el producto es no conmutativo.

Un problema de decisión que existe sobre los polinomios no conmutativos es: dado un conjunto $G$ de polinomios no conmutativos y un polinomio no conmutativo $f$, decidir si $f$ se puede escribir como combinación lineal de elementos de $G$ con coeficientes en $K⟨X⟩$. O escrito formalmente:

\begin{problem}\label{problem:principal}
  Dados $G ⊆ K⟨X⟩$ y $f ∈ K⟨X⟩$, determinar si vale que
  \[ ∃n ∈ ℕ ∪ \{0\}, g_1, …, g_n ∈ G, f_1, …, f_n, f'_1, …, f'_n ∈ K⟨X⟩ : f = ∑_{i = 1}^n f_i g_i f'_i \text{.}\]
\end{problem}

Acá los $f_i$ y $f'_i$ se llaman cofactores. Por ejemplo con $K = ℚ$ y $X = \{x, t, z\}$, si:
\begin{align*}
  g_0 &= xy + yz \text{,} \\
  g_1 &= yx + zx \text{,} \\
  G &= \{g_0, g_1\} \text{,} \\
  f_0 &= xyx - yyx \text{,} \\
  f_1 &= xyz + zyx \text{.}
\end{align*}

\noindent Tenemos que para $f_0$ la respuesta al \cref{problem:principal} es afirmativa porque
\begin{align*}
  g_0 x - y g_1 &= (xy + yz)x - y(yx + zx) \\
    &= xyx + yzx - yyx - yzx \\
    &= xyx - yyx \\
    &= f_0
\end{align*}

\noindent Pero para $f_1$ es negativa porque no hay ninguna forma de escribirlo como combinación lineal de $g_0$ y $g_1$ con coeficientes en $ℚ⟨x, y, z⟩$. Más adelante se explicará el código que se hizo que permite a veces responder al \cref{problem:principal} y en particular determinar que para $f_1$ la respuesta es negativa.

El \cref{problem:principal} no es decidible, lo que significa que no hay un algoritmo que decide si vale la expresión o no. Sin embargo sí es semi-decidible, lo que significa que hay un algoritmo que en los casos en los que sí vale la expresión termina y en los casos en los que no vale la expresión puede no terminar.

Este problema no es decidible porque el llamado problema de la palabra se reduce a este. En \cite{web:wiki:WordProblem} se puede ver qué es el problema de la palabra y en la Sección 1.3 de \cite{article:MORA1994131} cómo es la reducción. En esta tesis no se abordará el tema.

Para trabajar en algoritmos que aborden el \cref{problem:principal} se considera el conjunto denotado como $(G)$ de todos los polinomios no conmutativos que satisfacen ese $∃$, de forma que el \cref{problem:principal} se convierte en determinar si $f ∈ (G)$. Para responder a esto se usa el siguiente resultado: si $G = \{g_1, …, g_n\} ⊆ K⟨X⟩$, $f ∈ K⟨X⟩$, $a$ y $b$ son polinomios que consisten en una sola palabra y $c ∈ K$, entonces
\[ f ∈ (G) ⇔ f + c a g_i b ∈ (G) \text{.} \]

Teniendo en cuenta este resultado se intenta cancelar los términos de $f$ sumando distintos $c a g_i b$, y si en algún momento se llega a $0$ entonces vale que $f ∈ (G)$ porque el $0$ es la sumatoria vacía y porque todas las sumas mantienen la pertenencia a $(G)$.

El problema es que a veces puede pasar que ya no queden más términos para cancelar y no se haya llegado a $0$ pero aun así valga que $f ∈ (G)$. Para solucionar este problema lo que se hace es agregar elementos a $G$ para obtener lo que se llama una base de Gröbner de $(G)$, con la cual siempre que el polinomio está en $(G)$ se puede llegar a $0$ cancelando términos. El único problema de esto es que la base de Gröbner no siempre es finita, lo que refleja la no decidibilidad del problema. De cualquier manera, en los casos en los que es infinita, es computacionalmente enumerable, y el poder enumerarla también puede ser útil para abordar el \cref{problem:principal}. % Pongo computacionalmente enumerable y no solo enumerable porque enumerable no dice nada

Para calcular o enumerar las bases de Gröbner hay dos algoritmos destacados, llamados el algoritmo de Buchberger y el algoritmo F4. La diferencia entre los dos algoritmos es que Buchberger va calculando la base de Gröbner de a un elemento por vez mientras que F4 usa herramientas de álgebra lineal para calcular de a muchos elementos al mismo tiempo.

Si la explicación sobre los algoritmos no se entendió no importa porque en el \cref{cap:Preliminares} se explica bien.

Estos algoritmos, al igual que mucha de la teoría, están inspirados en el caso análogo para polinomios conmutativos y muchos de los nombres de las cosas se usaron primero para el caso conmutativo y después se usó el mismo nombre para el caso no conmutativo. Por ejemplo, ``el algoritmo de Buchberger'' en otros contextos puede referirse al algoritmo para el caso conmutativo. En esta tesis siempre se estará hablando del caso no conmutativo. Para una explicación del caso conmutativo ver \cite{book:ideals-varieties-algorithms}.

Existían previamente varias implementaciones del algoritmo de Buchberger en el caso no conmutativo, por ejemplo \cite{lib:GBNP, lib:DGPS, lib:NCAlgebra}. El algoritmo F4 para el caso no conmutativo fue primero adaptado por Xiu Xingqiang en \cite{phdthesis:XiuXingqiang12} y está implementado en el sistema algebraico computacional Magma. Luego, Clemens Hofstadler hizo en \cite{thesis:Hof20} su propia implementación en Python con SageMath, que es otro sistema algebraico computacional que al mismo tiempo es librería de Python. De acuerdo a mi conocimiento esas son las únicas dos implementaciones de F4 para el caso no conmutativo.

El objetivo de este trabajo fue implementar en \cpp ambos algoritmos y si es posible con un poco de paralelismo. La implementación en \cpp se logró, aunque para el algoritmo F4 hay una optimización que ayuda bastante que quedó pendiente. El paralelismo también se logró, pero solo para una parte y para la parte de la optimización que quedó pendiente tampoco se hizo nada de paralelismo. Por eso es que tiene un ``poquito'' de paralelismo.

Por la optimización que quedó pendiente en los casos muy grandes la implementación no le logra ganar a la implementación en Python con SageMath. Esto no significa que todo el trabajo no haya servido para nada, porque se podría continuar con el desarrollo y lograr mejorar suficientemente la implementación para igualar o llegar a ser mejor que la de Python con SageMath.

Alguien podría preguntar para qué sirve intentar resolver el \cref{problem:principal} y calcular bases de Gröbner no conmutativas. En el caso conmutativo hay un montón de aplicaciones prácticas importantes, por ejemplo en robótica porque el caso conmutativo tiene mucho significado geométrico; en cambio en el caso no conmutativo las aplicaciones son más limitadas y específicas.

Algo para lo que las bases de Gröbner no conmutativas son muy útiles es para trabajar con álgebras presentadas por generadores y relaciones. En este contexto, los generadores son las variables y las relaciones están dadas por los polinomios generadores del ideal. Un problema que las bases de Gröbner ayudan a resolver es el cálculo de la dimensión de este tipo de álgebras. Por ejemplo, algunos investigadores de FAMAF (incluido Cristian Vay, el director de este trabajo) se enfrentan a este problema trabajando con (las que llaman) álgebras de Nichols, que están explicadas en \cite{book:introNichols}. Usaremos algunos ejemplos de estas álgebras en el \cref{cap:Benchmarks}.

Como ya se dijo, el problema de la palabra se puede reducir al \cref{problem:principal}, así que los algoritmos eficientes para bases de Gröbner no conmutativas también pueden ser útiles para el problema de la palabra.

También hay algunas aplicaciones de bases de Gröbner no conmutativas y conmutativas en criptografía. En \cite{article:crypto_gb} se habla de eso.

Esta tesis está organizada de la siguiente forma. En el \cref{cap:Preliminares} se explican los contenidos matemáticos mínimos necesarios para que la exposición sea autocontenida. En el \cref{cap:Librería} se explica cómo usar la librería que se desarrolló. En el \cref{cap:Implementación} se describe cómo están hechas las implementaciones. En el \cref{cap:Tests} se describe brevemente qué tests se hicieron para eliminar los errores y darle robustez a la librería. En el \cref{cap:Benchmarks} se muestran algunos resultados de benchmarks que se hicieron, comparando cosas dentro de la librería y comparando con la implementación de Python con SageMath. Por último en el \cref{cap:Conclusiones} se destacan algunos puntos positivos del trabajo y se mencionan algunos trabajos futuros que se podrían hacer.

\chapter{Preliminares}\label{cap:Preliminares}

En este capítulo se desarrolla toda la parte matemática, basada principalmente en la exposición de \cite{thesis:Hof20}. Primero se explican sistemas de reescritura, después se introduce el álgebra libre, se prueba que es un anillo, se dan algunas definiciones y propiedades de anillos y se definen las bases de Gröbner. Por último, se explican y dan seudocódigos para los algoritmos de Buchberger y F4, que son los dos algoritmos importantes para calcular bases de Gröbner no conmutativas.

El capítulo incluye demostraciones de muchos de los resultados expuestos, pero leer las demostraciones no es necesario para entender la tesis.

\section{Sistemas de reescritura}

Sistemas de reescritura trata básicamente sobre el estudio de relaciones entre objetos con la idea de usar las relaciones para convertir un objeto en otro. En general usaremos relaciones denotadas con algún símbolo con forma de flecha porque se tiene la idea de que se \textsl{usa} la relación para convertir el objeto de la base de la flecha en el objeto de la punta de la flecha. Como recordatorio, una relación entre $A$ y $B$ es un subconjunto de $A × B$ y una relación sobre $A$ es un subconjunto de $A^2$.

Para toda esta sección fijemos un conjunto $A$. Primero recordamos algunas operaciones muy comunes sobre relaciones.

\begin{definition}\label{def:operaciones relaciones}
  Dadas relaciones $→$ y $⟿$ sobre $A$ se define:
  \begin{itemize}
    \item $→ ∘ ⟿\ = \{(x, z) ∈ A^2 : ∃y ∈ A : x → y ∧ y ⟿ z\}$.
    \item $→^0\ = \{(x, x) : x ∈ A\}$.
    \item $→^{i + 1}\ =\ →^i ∘ →$.
    \item $→^*\ = ⋃_{i = 0}^∞ →^i$.
    \item $←\ = \{(y, x) ∈ A^2 : (x, y) ∈\ →\}$.
    \item $↔\ =\ → ∪ ←$.
  \end{itemize}

  Con esta definición se define automáticamente también $↔^*$ que se llama la clausura reflexo-transitiva de $→$.
\end{definition}

Con $←$ y $↔$ podría haber problema para usarlas con una relación denotada por un símbolo que no tenga forma de flecha, pero en esta tesis las relaciones que se usan siempre se denotan con un símbolo con forma de flecha.

Para el resto de la sección fijemos una relación $→$ sobre $A$. Tenemos los siguientes dos lemas.

\begin{lemma}\label{lemma:→* como ∃}
  Sean $a, b ∈ A$. Entonces
  \[ a →^* b ⇔ ∃n ∈ ℕ ∪ \{0\}, x_0, …, x_n ∈ A : x_0 = a ∧ x_n = b ∧ ∀i ∈ \{1, …, n\} : x_{i-1} → x_i \text{.}\]
  \qed
\end{lemma}

\begin{lemma}\label{lemma:↔* min equiv que contiene a →}
  $↔^*$ es la mínima relación de equivalencia que contiene a $→$.
  \qed
\end{lemma}

Como querremos hablar de usar $→$ para transformar un objeto en otro, es útil la próxima definición que permite hablar de una forma normal de un objeto como un elemento al que se llega aplicando $→$ hasta que no se pueda más.

\begin{definition}\label{def:forma normal}
  Dados $a, b ∈ A$ se define:
  \begin{itemize}
    \item $a$ está en forma normal $⇔ ∄x ∈ A : a → x$.
    \item $b$ es forma normal de $a ⇔ a →^* b ∧ b$ está en forma normal.
    \item $a$ tiene forma normal $⇔ ∃x ∈ A : x$ es forma normal de $a$.
    \item $a ↓ b ⇔ ∃x ∈ A : a →^* x ∧ b →^* x$.
  \end{itemize}
\end{definition}

Con $↓$ pasa lo mismo que con $←$ y $↔$ de que podría ser problemático usarla para una relación denotada con un símbolo que no tenga forma de flecha, pero en esta tesis eso no pasa.

También necesitamos definir las siguientes propiedades de las relaciones.

\begin{definition} Se define que
  \begin{itemize}
    \item $→$ es confluente $⇔ ∀x, y, z ∈ A : x →^* y ∧ x →^* z ⇒ y ↓ z$.
    \item $→$ es Church-Rosser $⇔ ∀x, y ∈ A : x ↔^* y ⇔ x ↓ y$.
    \item $→$ es normalizante $⇔ ∀x ∈ A : x$ tiene forma normal.
    \item $→$ es terminante $⇔ ∄X ∈ A^ℕ : ∀i ∈ ℕ : X_i → X_{i + 1}$.
  \end{itemize}
\end{definition}

Las siguientes propiedades referidas a relaciones serán de utilidad.

\begin{theorem}\label{thm:terminante ⇒ normalizante}
  $→$ es terminante $⇒ →$ es normalizante.
  \qed
\end{theorem}

\begin{theorem}\label{thm:confluente ⇔ Church-Rosser}
  $→$ es confluente $⇔ →$ es Church-Rosser.
  \qed
\end{theorem}

Si $→$ es confluente y normalizante entonces todos los elementos tienen una única forma normal, y si además es terminante tenemos la siguiente observación.

\begin{observation}\label{obs:→ confluente y terminante}
  Si $→$ es confluente y terminante entonces calcular la forma normal de $a$ y de $b$ permite decidir si $a ↔ ^* b$.
  \qed
\end{observation}

Con esto ya está todo lo necesario sobre sistemas de re-escritura. Para aprender más sobre el tema se puede leer \cite{book:term-rewriting}.

\section{Álgebra libre}

El álgebra libre es básicamente el conjunto de los polinomios no conmutativos con sus operaciones. En los polinomios no conmutativos los monomios son palabras y la única operación interna de los monomios (entre monomios y que devuelve monomios) es la multiplicación, que equivale a la concatenación de palabras.

\begin{definition}
  Sea $X$ un alfabeto finito. Se define la estructura $(⟨X⟩, ·)$ de la siguiente manera:
  \begin{itemize}
    \item $⟨X⟩$ es el conjunto de palabras finitas sobre $X$.
    \item $· : ⟨X⟩^2 → ⟨X⟩$ es la concatenación.
  \end{itemize}
  Al par $(⟨X⟩, ·)$ se lo llama el monoide libre sobre $X$, a los elementos de $⟨X⟩$ se los llama monomios libres sobre $X$ y a $·$ el producto de $⟨X⟩$.

  Si $X = \{x_1, …, x_n\}$ escribimos $⟨x_1, …, x_n⟩$ en lugar de $⟨X⟩$.
\end{definition}

Por ejemplo, si $X = \{x, y, z\}$, algunos monomios son:

\begin{align*}
  m_0 &= xyyzy \text{,} \\
  m_1 &= yyz \text{,} \\
  m_2 &= yzy \text{,} \\
  m_3 &= ε \text{,} \\
  m_1 · m_2 &= yzzyzy \text{.}
\end{align*}

El $ε$ de $m_3$ es la palabra vacía. Notar que $m_1 ≠ m_2$ ya que el producto es no conmutativo.

\

Para todo el resto de la tesis fijemos un alfabeto finito $X$.

\

En estos monomios, al igual que en los conmutativos, se puede hablar de que un monomio divida a otro, pero acá que un monomio divida a otro es equivalente a que sea una sub-palabra.

\begin{definition}
  Sean $m, m' ∈ ⟨X⟩$. Se define que $m$ divida a $m'$, denotado como $m | m'$ de la siguiente forma:
  \[ m | m' ⇔ ∃a , b ∈ ⟨X⟩ : m' = a m b \text{.} \]
\end{definition}

En el ejemplo de antes tenemos que $m_1 | m_0$ ya que $m_0 = a m_1 b$.

Hablar del resultado de la división es un poco más complicado acá porque tendría que haber dos resultados, el $a$ y el $b$ de la definición, así que en ningún momento hablaremos de dividir un monomio en otro ni escribiremos divisiones entre monomios.

Más adelante será necesario tener un orden entre los elementos de $⟨X⟩$. Se podría fijar uno concreto definiendo directamente $≤$ para $⟨X⟩$, pero es mejor enunciar las mínimas propiedades necesarias y trabajar con cualquier orden que las satisfaga. Eso se logra con la siguiente definición.

\begin{definition}\label{def:buen orden monomial}
  Sea $≤$ un orden total sobre $⟨X⟩$. Se define que $≤$ es un buen orden monomial si y solo si:
  \begin{enumerate}
    \item $∀m, m', a, b ∈ ⟨X⟩ : m ≤ m' ⇒ a m b ≤ a m' b$.
    \item $∀S ⊆ ⟨X⟩ : S ≠ ∅ ⇒ S$ tiene mínimo elemento con respecto a $≤$.
  \end{enumerate}
\end{definition}

Un ejemplo de orden que cumple con esta definición es el siguiente.

\begin{definition}\label{def:orden lexicográfico por grado}
  Fijemos $X = \{x_1, …, x_n\}$ y un orden total sobre $X$: $x_1 ≤ … ≤ x_n$, el cual se extiende (como es usual) de forma lexicográfica a $⟨X⟩$. El orden lexicográfico por grado $ ≤_{deglex}$ sobre $⟨X⟩$ se define así:
  \[ m ≤_{deglex} m' ⇔ |m| < |m'| ∨ (|m| = |m'| ∧ m ≤ m') \text{.}\]
\end{definition}

O sea, el orden lexicográfico por grado ordena primero por cardinalidad, también llamado grado, y desempata con el orden lexicográfico. Por ejemplo, tenemos que $bc ≤_{deglex} abb$, $aabbc ≤_{deglex} abbcc$ y $ε ≤_{deglex} a$. Se puede probar fácilmente que este orden es un buen orden monomial.

\

A partir de ahora fijamos un buen orden monomial $≤$ y usaremos $<$, $≥$ y $>$ como se usan habitualmente.

La siguiente propiedad es consecuencia directa de la definición de buen orden monomial.

\begin{theorem}\label{thm:≤ no sucesiones dec inf}
  La relación $≤$ no tiene sucesiones estrictamente decrecientes infinitas.
  \qed
\end{theorem}

Ahora pasemos a hablar de sumar monomios entre sí para tener polinomios no conmutativos.

\begin{definition}
  Sea $R$ un anillo conmutativo. Se define la $R$-álgebra libre sobre $X$ como el conjunto
  \[ R⟨X⟩ = \{∑_{i = 1}^n c_i m_i : c_1, …, c_n ∈ R, m_1, …, m_n ∈ ⟨X⟩\}\]
  con las siguientes operaciones: la suma de los elementos de $R⟨X⟩$ como definido unir las $∑$, el producto por escalares definido como
  \[ c (∑_{i = 1}^n c_i m_i) = ∑_{i = 1}^n c c_i m_i \text{,}\]
  y el producto entre elementos de $R⟨X⟩$ definido como
  \[ (∑_{i = 1}^n c_i m_i) · (∑_{i = 1}^m c'_i m'_i) = ∑_{i = 1}^n ∑_{j = 1}^m c_i c'_j m_i m'_j \text{.}\]

  A los elementos de $R⟨X⟩$ se los llama polinomios no conmutativos. Cuando tengamos una lista de variables $x_1, …, x_n$ escribimos $R⟨x_1, …, x_n⟩$ en lugar de $R⟨\{x_1, …, x_n\}⟩$.
\end{definition}

Recordemos los ejemplos de polinomios no conmutativos en $ℚ⟨x, y, z⟩$ dados en la introducción:
\begin{align*}
  f_0 &= x \text{,} \\
  f_1 &= xy + zy \text{,} \\
  f_2 &= 3 xyy - 2 xzxy + \frac{4}{3} yzzx \text{.}
\end{align*}

\noindent Recordar que $f_1 ≠ xy + xz$ ya que el producto es no conmutativo.

Sobre los polinomios no conmutativos se hacen las siguientes definiciones.

\begin{definition}\label{def:cosas de polinomios}
  Sean $R$ un anillo conmutativo, $f ∈ R⟨X⟩$, $c_1, …, c_n ∈ R - \{0\}$, $m_1, …, m_n, m ∈ ⟨X⟩$, $f = ∑_{i = 1}^n c_i m_i$ y $≤$ un buen orden monomial. Se definen:
  \begin{itemize}
    \item el coeficiente de $m$ en $f$ es $f_m = \begin{cases} c_0&\text{si }m = m_0 \\ ⋮ & \\ c_n&\text{si }m = m_n \\ 0&\text{en otro caso} \end{cases} $.
    \item el soporte de $f$ es el conjunto $\sop(f) = \{m_1, …, m_n\}$.
    \item el monomio principal de $f$ es $\lm_≤(f) = \max_≤(\sop(f))$.
    \item el coeficiente principal de $f$ es $\lc_≤(f) = f_{\lm_≤(f)}$.
    \item el término principal de $f$ es $\lt_≤(f) = \lc_≤(f) · \lm_≤(f)$.
    \item $\tail_≤(f) = f - \lt_≤(f)$.
  \end{itemize}

  Los nombres lm, lc, lt y tail vienen del inglés leading monomial, leading coefficient, leading term y tail respectivamente.
\end{definition}

Siguiendo con el ejemplo tenemos:
\begin{align*}
  {f_2}_{xyy} &= 3 \text{,} \\
  \sop(f_2) &= \{xyy, xxxy, yxxx\} \text{,} \\
  \lm_{≤_{deglex}}(f_2) &= yxxx \text{,} \\
  \lc_{≤_{deglex}}(f_2) &= 4 \text{,} \\
  \lt_{≤_{deglex}}(f_2) &= 4 yxxx \text{,} \\
  \tail_{≤_{deglex}}(f_2) &= 3 xyy - 2 xxxy \text{.}
\end{align*}

Sobre estas definiciones valen muchas propiedades fáciles de probar, como por ejemplo $\lm_≤(f) \lm_≤(f') = \lm_≤(f f')$, que durante el resto de la tesis usaremos mucho, pero no las probamos una por una porque sería tedioso y aburrido.

Más adelante será necesario comparar no solo monomios sino también polinomios, así que el orden $≤$ se extiende a $K⟨X⟩$ así:

\begin{definition}\label{def:orden polinomial}
  Sean $R$ un anillo conmutativo y $f, g ∈ R⟨X⟩$. Se define que $f < g$ si y solo si vale alguna de las siguientes:
  \begin{enumerate}
    \item $f = 0 ∧ g ≠ 0$.
    \item $\lm_≤(f) < \lm_≤(g)$.
    \item $\lm_≤(f) = \lm_≤(g) ∧ \tail_≤(f) < \tail_≤(g)$.
  \end{enumerate}
  Y como es usual $f ≤ g$ si y solo si $f < g ∨ f = g$.
\end{definition}

Dicho en palabras, el orden en los polinomios es orden lexicográfico con el polinomio visto como una lista de monomios, sin coeficientes, ordenada de mayor a menor. Por ejemplo, tenemos estas desigualdades en $K⟨X⟩$:
\begin{align*}
  x &< xy \text{,} \\
  yzz + z &< yzz + xyy \text{,} \\
  xz &< xz + xx \text{.}
\end{align*}

Si bien a este $<$ lo estamos llamando (y lo continuaremos llamando) orden, en realidad no es un orden sino un preorden porque cuando solo cambian los coeficientes entre un polinomio y otro, ninguno de los polinomios es menor que el otro.

\begin{theorem}
  Sea $R$ un anillo conmutativo. Entonces
    \[ \text{la relación }<\text{ en }R⟨X⟩\text{ es un preorden parcial.} \]
  \qed
\end{theorem}

Además, el polinomio $0$ es el mínimo.

\begin{lemma}\label{lemma:0 es mínimo}
  $0$ es el mínimo de $<$.
  \qed
\end{lemma}

En este orden, al igual que para los monomios, vale que no hay sucesiones estrictamente decrecientes infinitas.

\begin{lemma}\label{lemma:≤ en KX no sucesiones dec inf}
  Sea $R$ un anillo conmutativo. Entonces
    \[ \text{la relación }≤\text{ en }R⟨X⟩\text{ no tiene sucesiones estrictamente decrecientes infinitas.} \]
\end{lemma}
\begin{proof}
  Supongamos que existen sucesiones estrictamente decrecientes infinitas. Tomemos una sucesión estrictamente decreciente infinita $P$, o sea que valga $P_1 > P_2 > P_3 > …$, que minimice $\lm_≤(P_1)$. Tomar este mínimo es posible por el \cref{thm:≤ no sucesiones dec inf} que dice que no hay sucesiones estrictamente decrecientes infinitas en los monomios.

  Notemos que:

  \begin{fact}\label{fact:≤ en KX no sucesiones dec inf:1}
    $∀i ∈ ℕ : \lm_≤(P_i) = \lm_≤(P_1)$.
  \end{fact}
  En efecto, no puede ser $\lm_≤(P_i) < \lm_≤(P_1)$ porque entonces $P_i, P_{i + 1}, …$ sería una sucesión estrictamente decreciente infinita que rompería la minimalidad de $\lm_≤(P_1)$ y no puede ser $\lm_≤(P_i) > \lm_≤(P_1)$ porque eso implicaría $P_i > P_1$ y eso contradice que $P$ sea decreciente.

  \begin{fact}\label{fact:≤ en KX no sucesiones dec inf:2}
    $\tail_≤(P_1), \tail_≤(P_2), …$ es una sucesión estrictamente decreciente infinita.
  \end{fact}
  Esto vale porque al aplicar la definición del orden polinomial sobre $P_1 > P_2 > P_3 > …$ y usar la \cref{fact:≤ en KX no sucesiones dec inf:1} queda $\tail_≤(P_1) > \tail_≤(P_2) > \tail_≤(P_3) > …$

  Como por el \cref{lemma:0 es mínimo} $0$ es un mínimo:

  \begin{fact}\label{fact:≤ en KX no sucesiones dec inf:3}
    $P_1 ≠ 0$.
  \end{fact}

  Sin embargo, la \cref{fact:≤ en KX no sucesiones dec inf:2} contradice la minimalidad de $\lm_≤(P_1)$ ya que por la \cref{fact:≤ en KX no sucesiones dec inf:3} vale que $\lm_≤(\tail_≤(P_1)) < \lm_≤(P_1)$.

\end{proof}

La estructura del álgebra libre es un anillo, lo cual es muy útil por muchas definiciones y teoremas que ya existen sobre los anillos.

\begin{theorem}
  Sea $R$ un anillo conmutativo. Entonces
  \[ (R⟨X⟩, +, ·)\text{ es un anillo} \text{.}\]
  \qed
\end{theorem}
% Tendría que decir algo de que no se incluye la demostración porque es fácil?

Las definiciones y teoremas sobre anillos necesarios están a continuación.

\begin{definition}\label{def:ideal}
  Sean $R$ un anillo e $I ⊆ R$. Se define que $I$ es un ideal de $R$ si y solo si:
  \begin{enumerate}
    \item $I ≠ ∅$.
    \item $∀a, b ∈ I : a + b ∈ I$.
    \item $∀a ∈ I, r, r' ∈ R : r a r' ∈ I$.
  \end{enumerate}
\end{definition}

Los ideales son básicamente conjuntos cerrados por la suma y por el producto por cualquier elemento del anillo. Si se tiene un conjunto que no es un ideal se puede agregar todo lo mínimo necesario para convertirlo en un ideal. La siguiente definición define eso y el siguiente teorema dice que efectivamente se obtiene un ideal.

\begin{definition}\label{def:ideal gen}
  Sean $R$ un anillo y $G ⊆ R$. Se define el ideal generado por $G$, denotado $(G)$, como
  \[ (G) = \{∑_{i = 1}^n c_i g_i c_i' : n ∈ ℕ ∪ \{0\}, g_1, …, g_n ∈ G, c_1, …, c_n, c_1', …, c_n' ∈ R\} \text{.}\]
  A los $c_i, c'_i$ se los llama cofactores. % ¿Está bien poner esto así?
\end{definition}

\begin{theorem}
  Sean $R$ un anillo y $G ⊆ R$. Entonces
  \[ (G)\text{ es un ideal de }R \text{.}\]
  \qed
\end{theorem}

Sobre los ideales generados por un conjunto valen los siguientes dos lemas básicos.

\begin{lemma}\label{lemma:gen G = gen G U a con a ∈ gen G}
  Sean $R$ un anillo, $G ⊆ R$ y $a ∈ (G)$. Entonces
  \[ (G) = (G ∪ \{a\}) \text{.}\]
  \qed
\end{lemma}

\begin{lemma}\label{lemma:sub gen y sub gen ⇔ eq}
  Sean $R$ un anillo y $G, G' ⊆ R$. Entonces
  \[ G ⊆ (G') ∧ G' ⊆ (G) ⇔ (G) = (G') \text{.}\]
  \qed
\end{lemma}


Cada ideal define una clase de equivalencia en el anillo, la cual se llama congruencia módulo el ideal.

\begin{definition}\label{def:congruencia mod ideal}
  Sean $R$ un anillo e $I ⊆ R$. Se define la relación $≡_I$ en $R$, llamada congruencia módulo $I$, así:
  \[ a ≡_I b ⇔ a - b ∈ I \text{.}\]
\end{definition}

\begin{theorem}\label{thm:congruencia mod ideal es equivalencia}
  Sean $R$ un anillo e $I ⊆ R$ un ideal. Entonces
  \[ ≡_I \text{es una relación de equivalencia} \text{.}\]
  \qed
\end{theorem}

Esta congruencia es parecida a la de los enteros módulo un natural (de hecho, la congruencia de los enteros módulo un natural es un subcaso de ésta tomando como ideal a todos los múltiplos del módulo). Vale que la clase de equivalencia del $0$ es el propio ideal:

\begin{lemma}\label{lemma:en ideal ⇔ congruente 0}
  Sean $R$ un anillo, $I ⊆ R$ un ideal y $a ∈ R$. Entonces
  \[ a ∈ I ⇔ a ≡_I 0 \text{.}\]
  \qed
\end{lemma}

% Estaría bueno citar algún libro para aprender más sobre anillos

Ahora volvemos al álgebra libre y a partir de ahora fijamos un cuerpo $K$.

\begin{observation}
  Con las definiciones que tenemos ahora el \cref{problem:principal} se puede escribir como: dado un conjunto finito $G ⊆ K⟨X⟩$ y un elemento $f ∈ K⟨X⟩$, determinar si $f ∈ (G)$.
\end{observation}

Esta observación es tan importante que de hecho el \cref{problem:principal} se llama `problema de pertenencia al ideal'.

Si bien $(G)$ para un anillo general está definido usando combinaciones lineales con coeficientes en el anillo, para el álgebra libre es útil la siguiente equivalencia que dice que alcanza con considerar solo monomios y elementos del cuerpo como cofactores.

\begin{lemma}\label{lemma:(G) equiv}
  Sea $G ⊆ K⟨X⟩$. Entonces
  \[ (G) = \{∑_{i = 0}^n c_i m_i g_i m'_i : n ∈ ℕ ∪ \{0\}, c_1, …, c_n ∈ K, m_1, …, m_n, m'_1, …, m'_n ∈ ⟨X⟩, g_1, …, g_n ∈ G\} \text{.}\]
\end{lemma}
\begin{proof} Probemos que $f$ está en uno si y solo si está en el otro.
  \begin{description}
    \item[Ida ($⇒$):] Supongamos $f ∈ (G)$. Sean:

    \begin{itemize}
      \item $g_1, …, g_n ∈ G, f_1, …, f_n, f'_1, …, f'_n ∈ K⟨X⟩$ tales que $f = ∑_{i = 1}^n f_i g_i f'_i$, los cuales existen porque estamos suponiendo $f ∈ (G)$ y por la definición de $(G)$.
      \item Para cada $i ∈\{1, …, n\}$:
      \begin{itemize}
        \item $c_{i, 1}, …, c_{i, n_i} ∈ K, m_{i, 1}, …, m_{i, n_i} ∈ ⟨X⟩$ tales que $f_i = ∑_{j = 1}^{n_i} c_{i, j} m_{i, j}$.
        \item $c'_{i, 1}, …, c'_{i, n'_i} ∈ K, m'_{i, 1}, …, m'_{i, n'_i} ∈ ⟨X⟩$ tales que $f'_i = ∑_{j = 1}^{n'_i} c'_{i, j} m'_{i, j}$.
      \end{itemize}
    \end{itemize}

    Con esto tenemos:
    \begin{DispWithArrows*}
      f &= ∑_{i = 1}^n f_i g_i f'_i \\
        &= ∑_{i = 1}^n (∑_{j = 1}^{n_i} c_{i, j} m_{i, j}) g_i (∑_{j = 1}^{n'_i} c'_{i, j} m'_{i, j}) \\
        &= ∑_{i = 1}^n ∑_{j = 1}^{n_i} ∑_{j' = 1}^{n'_i} c_{i, j} c'_{i, j'} m_{i, j} g_i m'_{i, j'} \text{.}
    \end{DispWithArrows*}

    Esto último es una expresión que se transforma a la forma que queremos. % ¿Está bien esto?

    \item[Vuelta ($⇐$):] La vuelta es cierta porque, como $c_i m_i, m'_i ∈ K⟨X⟩$, es un caso particular de la definición.
  \end{description}
\end{proof}

\section{Reducción de polinomios}

Ahora definiremos una relación de reducción en los polinomios no conmutativos la cual depende del orden monomial y de un conjunto generador $G$, cuya clausura reflexo-transitiva es igual a $≡_{(G)}$. La relación que definiremos siempre es terminante, pero no siempre confluente. Para los casos en los que sí sea confluente la \cref{obs:→ confluente y terminante} nos permitirá chequear si dos polinomios son equivalentes y en particular, por el \cref{lemma:en ideal ⇔ congruente 0}, si un polinomio está en $(G)$. A los casos en los que no sea confluente después trataremos de convertirlos en confluentes.

\begin{definition}\label{def:reducciones}
  Sean $G ⊆ K⟨X⟩$ y $f, f' ∈ K⟨X⟩$. Se define la relación $→_{≤, G}$ del siguiente modo:
  \[ f →_{≤, G} f' ⇔ ∃a, b ∈ ⟨X⟩, g ∈ G : \lm_≤(agb) ∈ \sop(f) ∧ f' = f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb \text{.} \]
  Cuando vale $f →_{≤, G}f'$ se dice que $f$ se reduce a $f'$ (vía $G$).
\end{definition}

Por ejemplo, si tenemos $f = 2 zyx + 3 xyzxx$, $g_0 = x - yzx$ y $G = \{g_0\}$ tenemos:

\[f →_{≤, G} f + 3x g_0 x = 3 xxx + 2 zyx \text{.}\]

\

Como teníamos la \cref{def:operaciones relaciones} y la \cref{def:forma normal} quedan definidas automáticamente las relaciones $→^*_{≤, G}$, $↔^*_{≤, G}$ y $↓_{≤, G}$.

A continuación la prueba de que esta relación efectivamente achica.

\begin{theorem}\label{thm:→ achican}
  Sean $G ⊆ K⟨X⟩$ y $f, f' ∈ K⟨X⟩$. Entonces
  \[ f →_{≤, G} f' ⇒ f' < f \text{.} \]
\end{theorem}
\begin{proof}
  Supongamos el antecedente. Por definición de reducciones tomemos $a, b ∈ ⟨X⟩, g ∈ G$ tales que:
  \begin{enumerate}[label=(\roman*)]
    \item $\lm_≤(agb) ∈ \sop(f)$. \label{thm:→ achican:i}
    \item $f' = f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb$. \label{thm:→ achican:ii}
  \end{enumerate}

  Sean:
  \begin{itemize}
    \item $c_1, …, c_n ∈ K, m_1, …, m_n ∈ ⟨X⟩$ con $m_1 > m_2 > ⋯ > m_n$ tales que $f = ∑_{i = 1}^n c_i m_i$.
    \item $i$ tal que $m_i = \lm_≤(agb)$, el cual existe por \ref{thm:→ achican:i}.
  \end{itemize}

  Notar también que:

  \begin{enumerate}[label=(\roman*)]
    \setcounter{enumi}{2}
    \item $m_i = \lm_≤(\frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb)$. \label{thm:→ achican:iii}
  \end{enumerate}

  \ref{thm:→ achican:ii} y \ref{thm:→ achican:iii} implican que los términos $c_1 m_1, c_2 m_2, …, c_{i-1}, m_{i-1}$ son iguales en $f$ y en $f'$ y no hay nada más en el medio, porque $f'$ es $f$ con términos menores o iguales a $\lm_≤(\frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb)$ restadas (por \ref{thm:→ achican:ii}).

  Además, por \ref{thm:→ achican:iii} y \ref{thm:→ achican:ii} vale que $f'_{m_i} = 0$, por ende, el término que sigue después de $m_{i-1}$ (si es que hay) es menor que $m_i$.

  Combinando que los primeros $i - 1$ términos son iguales y el $i$ es menor en $f'$ que en $f$, por la definición de $<$ para polinomios vale que $f' < f$.

\end{proof}

La relación $→_{≤, G}$ tiene varias propiedades útiles que se prueban a continuación.

\begin{lemma}\label{lemma:suma →↓}
  Sean $G ⊆ K⟨X⟩$ y $f_0, f_1, f ∈ K⟨X⟩$. Entonces
  \[ f_0 →_{≤, G} f_1 ⇒ f_0 + f ↓_{≤, G} f_1 + f \text{.}\]
\end{lemma}
\begin{proof}
  Supongamos el antecedente $f_0 →_{≤, G} f_1$.

  Sean $g ∈ G, a, b ∈ K⟨X⟩$ tales que $f_1 = f_0 - \frac{{f_0}_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb $, los cuales existen por definición de $→_{≤, G}$.

  Dividamos la demostración en casos según la pertenencia de $\lm_≤(agb)$ a $\sop(f)$:

  \begin{description}
    \item[Caso $\lm_≤(agb) ∉ \sop(f)$:] Partiendo de la condición de $a, g, b$ hacemos lo siguiente:
    \begin{DispWithArrows*}
      &f_1 = f_0 - \frac{{f_0}_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} \\
      & ⇒ f_1 + f = f_0 - \frac{{f_0}_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} + f \Arrow{Definición de $→_{≤, G}$}\\
      & ⇒ f_0 + f →_{≤, G} f_1 + f \Arrow{Definición de $↓$, ambos se reducen a $f_1 + f$}\\
      & ⇒ f_0 + f ↓_{≤, G} f_1 + f \text{.}
    \end{DispWithArrows*}

    \item[Caso $\lm_≤(agb) ∈ \sop(f)$:]\
    \begin{description}
      \item[Subcaso $f_{\lm_≤(agb)} = -{f_0}_{\lm_≤(agb)}$]

      En este caso $\lm_≤(agb)$ se cancela en la suma $f_0 + f$. Y como además $\lm_≤(agb) ∉ \sop(f_1)$ por el antecedente, tenemos:
      \begin{DispWithArrows*}
        &f_1 + f →_{≤, G} f_1 + f - \frac{f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \Arrow{Subcaso} \\
        & ⇒ f_1 + f →_{≤, G} f_1 + f + \frac{{f_0}_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \Arrow{Condición de $g$, $a$ y $b$} \\
        & ⇒ f_1 + f →_{≤, G} f_0 + f \\
        & ⇒ f_1 + f ↓_{≤, G} f_0 + f \text{.}
      \end{DispWithArrows*}

      \item[Subcaso $f_{\lm_≤(agb)} ≠ -{f_0}_{\lm_≤(agb)}$:] En este caso $\lm_≤(agb)$ no se cancela en la suma $f_0 + f$, así que podemos aplicar $→_{≤, G}$ con $a$, $g$, $b$:
      \begin{DispWithArrows*}
        &f_0 + f →_{≤, G} f_0 + f - \frac{(f_0 + f)_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \\
        & ⇒ f_0 + f →_{≤, G} f_0 + f - \frac{{f_0}_{\lm_≤(agb)} + f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \\
        & ⇒ f_0 + f →_{≤, G} f_0 + f - \frac{{f_0}_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb - \frac{f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \Arrow{Condición de $g$, $a$ y $b$}\\
        & ⇒ f_0 + f →_{≤, G} f_1 + f - \frac{f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb  \text{.}
      \end{DispWithArrows*}
        Llamemos (i) a este último resultado.

      Además por el caso y el hecho de que $\lm_≤(agb) ∉ \sop(f_1)$ también tenemos $\lm_≤(agb) ∈ \sop(f_1 + f)$, entonces:
      \begin{DispWithArrows*}
        &f_1 + f →_{≤, G} f_1 + f - \frac{(f_1 + f)_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \\
        & ⇒ f_1 + f →_{≤, G} f_1 + f - \frac{{f_1}_{\lm_≤(agb)} + f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \Arrow{$\lm_≤(agb) ∉ \sop(f_1)$ porque $f_0 →_{≤, G} f_1$ y definición de $→_{≤, G}$}\\
        & ⇒ f_1 + f →_{≤, G} f_1 + f - \frac{0 + f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \\
        & ⇒ f_1 + f →_{≤, G} f_1 + f - \frac{f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb \text{.}
      \end{DispWithArrows*}
        Llamemos (ii) a este último resultado.

      Por (i) e (ii) tenemos $f_0 + f ↓_{≤, G} f_0' + f$. % No se como hacer esto con labels
    \end{description}
  \end{description}
\end{proof}

\begin{lemma}\label{lemma:prod mon →}
  Sean $G ⊆ K⟨X⟩$, $f, f' ∈ K⟨X⟩$, $c ∈ K$ y $m, m' ∈ ⟨X⟩$. Entonces
  \[ f →_{≤, G} f' ⇒ c m f m' →_{≤, G} c m f' m' \text{.}\]
\end{lemma}
\begin{proof} Supongamos el antecedente.

  Sean $g ∈ G$ y $a, b ∈ ⟨X⟩$ tales que $f' = f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb$, los cuales existen por definición de $→_{≤, G}$. Tenemos:
  \begin{DispWithArrows*}
    &f' = f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb \\
    & ⇒ c m f' m' = c m (f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb) m' \\
    & ⇒ c m f' m' = c m f m' - \frac{{(c m f m')}_{\lm_≤(c ma g bm')}}{\lc_≤(g)}c ma g bm'  \Arrow{Definición $→_{≤, G}$} \\
    & ⇒ c m f' m' →_{≤, G} c m f m' \text{.}
  \end{DispWithArrows*}

\end{proof}

Ahora el ya anunciado teorema de que la clausura reflexo transitiva de $→_{≤, G}$ es una congruencia.

\begin{theorem}\label{theorem:→^* = ≡}
  Sea $G ⊆ K⟨X⟩$. Entonces
  \[ ↔^*_{≤, G}\ =\ ≡_{(G)} \text{.}\]
\end{theorem}
\begin{proof} Probemos las dos inclusiones.
  \begin{description}
    \item[Prueba de $↔^*_{≤, G}\ ⊆\ ≡_{(G)}$:] Como $↔^*_{≤, G}$ y $≡_{(G)}$ son relaciones de equivalencia y además por el \cref{lemma:↔* min equiv que contiene a →} $↔^*_{≤, G}$ es la mínima relación de equivalencia que contiene a $→_{≤, G}$, alcanza con probar $→_{≤, G}\ ⊆\ ≡_{(G)}$. Para eso supongamos $f →_{≤, G} f'$ y probemos $f ≡_{(G)} f'$.

    Sean $g ∈ G, a, b ∈ K⟨X⟩$ tales que $\lm_≤(agb) ∈ f$ y $f' = f - \frac{f_{\lm_≤(agb)}}{g_{\lm_≤(agb)}} agb$, los cuales existen por definición de $→_{≤, G}$. Tenemos:
    \begin{DispWithArrows*}
      &f ≡_{(G)} f' \\
      & ⇔ f - f' ∈ (G) \\
      & ⇔ f - (f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb) ∈ (G) \\
      & ⇔ \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb ∈ (G) \text{.}
    \end{DispWithArrows*}
    Y esto último es claramente cierto por la definición de (G) (\cref{def:ideal gen}).

    \item[Prueba de $≡_{(G)}\ ⊆\ ↔^*_{≤, G}$:] Supongamos $f ≡_{(G)} f'$ y probemos $f ↔^*_{≤, G} f'$. Sean:
    \begin{itemize}
      \item $g = f - f'$.
      \item $c_1, …, c_n ∈ K, m_1, …, m_n, m_1', …, m_n' ∈ ⟨X⟩$, $g_1, …, g_n ∈ G$ tales que $g = ∑_{i = 1}^n c_i m_i g_i m_i'$, los cuales existen porque por definición de $≡_G$ tenemos $g ∈ (G)$ y por el \cref{lemma:(G) equiv}.
      \item $f_0 = f$.
      \item Para cada $i ∈ \{1, …, n\}$: $f_i = f_{i - 1} - c_i m_i g_i m_i'$.
    \end{itemize}

    Tenemos entonces:
    \begin{DispWithArrows*}
      &∀i ∈ \{1, …, n\} : g_i →_{≤, G} 0 \Arrow{\Cref{lemma:prod mon →}} \\
      & ⇒ ∀i ∈ \{1, …, n\} : c_i m_i g_i m_i' →_{≤, G} 0 \Arrow{\Cref{lemma:suma →↓}} \\
      & ⇒ ∀i ∈ \{1, …, n\} : c_i m_i g_i m_i' + f_i ↓_{≤, G} 0 + f_i \Arrow{Definición de los $f_i$}\\
      & ⇒ ∀i ∈ \{1, …, n\} : f_{i - 1} ↓_{≤, G} f_i \\
      & ⇒ ∀i ∈ \{1, …, n\} : f_{i - 1} ↔^*_{≤, G} f_i \\
      & ⇒ f_0 ↔^*_{≤, G} f_n \Arrow{$f_n = f - g = f'$} \\
      & ⇒ f ↔^*_{≤, G} f_n \text{.}
    \end{DispWithArrows*}

  \end{description}
\end{proof}

\begin{lemma}\label{thm:→ mantiene pertenencia a ideal}
  Sean $G ⊆ K⟨X⟩, f, f' ∈ K⟨X⟩$. Entonces
  \[ f →^*_{≤, G} f' ⇒ (f ∈ (G) ⇔ f' ∈ (G)) \text{.}\]
\end{lemma}
\begin{proof}
  Si asumimos $f →^*_{≤, G} f'$, tenemos por el \cref{theorem:→^* = ≡} que $f ≡_{(G)} f'$ y entonces por el \cref{lemma:en ideal ⇔ congruente 0} vale que $f ∈ (G) ⇔ f' ∈ (G)$.
\end{proof}

\begin{theorem}
  Sea $G ⊆ K⟨X⟩$. Entonces
  \[ →_{≤, G} \text{ es terminante.} \]
\end{theorem}
\begin{proof}
  Por contradicción. Supongamos que $→_{≤, G}$ no es terminante. Por definición podemos tomar una sucesión $P ∈ K⟨X⟩^ℕ$ tal que:

  \[ ∀i ∈ ℕ : P_i →_{≤, G} P_{i+1}. \]

  \noindent Por el \cref{thm:→ achican} tenemos que:

  \[ ∀i ∈ ℕ : P_i > P_{i+1}. \]

  \noindent Pero esto contradice el \cref{lemma:≤ en KX no sucesiones dec inf} que dice que no hay sucesiones estrictamente decrecientes infinitas en $K⟨X⟩$.
\end{proof}

El siguiente teorema caracteriza las formas normales de $→$.

\begin{theorem}
  Sean $G ⊆ K⟨X⟩, f ∈ K⟨X⟩$. Entonces
  \[ f\text{ está en forma normal con respecto a} →_{≤, G} ⇔ ∄g ∈ G, m ∈ \sop(f) : \lm_≤(g) | m \text{.}\]
\end{theorem}
\begin{proof} Probemos ida y vuelta por separado.
  \begin{description}
    \item[Ida ($⇒$):] Por contradicción, supongamos el antecedente y que tenemos $g ∈ G, m ∈ \sop(f)$ tales que $\lm_≤(g) | m$.

    \noindent Sean $a, b ∈ ⟨X⟩$ tales que $m = agb$, los cuales existen por la definición de divisibilidad.

    \noindent Entonces tenemos $f →_{≤, G} f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb$ y por ende $f$ no está en forma normal.

    \item[Vuelta ($⇐$):] Por contrarrecíproca, supongamos que $f$ no está en forma normal con respecto a $→_{≤, G}$ y probemos existe $g$ que satisface el $∃$. Sean:
    \begin{itemize}
      \item $f' ∈ K⟨X⟩$ tal que $f →_{≤, G} f'$, el cual existe por el antecedente.
      \item $a, b ∈ ⟨X⟩, g ∈ G$ tales que $\lm_≤(agb) ∈ \sop(f)$ y $f' = f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb$, los cuales existen por la definición de $→_{≤, G}$.
    \end{itemize}

    Con esto es claro que $g$ satisface el $∃$.
  \end{description}
\end{proof}

Probamos que $→_{≤, G}$ es terminante, pero no que sea confluente, ya que (como habíamos anticipado) no siempre lo es. Por ejemplo, siguiendo con el mismo ejemplo de antes en el que ya teníamos $f = 2 zyx + 3 xyzxx$, $g_0 = x - yzx$, si ahora agregamos $g_1 = y - zxx$ y $G = \{g_0, g_1\}$ tenemos, como antes:

\[f →_{≤, G} f + 3x g_0 x = 3 xxx + 2 zyx = f' \text{.}\]

\noindent Y también:

\[f →_{≤, G} f - 3xy g_1 = 3 xyy + 2 zyx = f'' \text{.}\]

\noindent Pero ningún monomio principal de $G$ divide a ningún monomio de $f'$ o $f''$ así que $f'$ y $f''$ no se pueden reducir más.

La siguiente definición nos permite hablar más cómodamente de una forma normal de un elemento (tanto en las definiciones y teoremas como en los algoritmos).

\begin{definition}\label{def:reductor}
  Sea $e_≤ : 𝒫(K⟨X⟩) → K⟨X⟩ → K⟨X⟩$. Se define que
  \begin{align*}
    & e_≤\text{ es un reductor }\\
    &\ \ ⇔ ∀G ⊆ K⟨X⟩, f ∈ K⟨X⟩ : e_≤(G)(f)\text{ es forma normal de }f\text{ con respecto a }→_{≤, G} \text{.}
  \end{align*} % Esto lo hice así para que no se salga del margen, no se si es lo mejor o no
\end{definition}

Un ejemplo de reductor podría calcularse con el siguiente seudocódigo.

\begin{algorithm}[H] % La H es para que se quede acá, porque se iba a otra página. Estaría bueno hacerlo global
  \caption{Ejemplo de reductor}\label{alg:reductor}
  \KwData{$G = \{g_1, …, g_n\} ⊆ K⟨X⟩, f ∈ K⟨X⟩$}
  \KwResult{$f' ∈ K⟨X⟩$}
  $f' ← f$

  $i ← 1$

  \While{$i ≤ n$} {
    $r ← False$

    \For{$m ∈ \sop(f')$} {
      \If{$\lm_≤(g_i) | m$} {
        calcular $a, b ∈ ⟨X⟩$ tales que $m = a \lm_≤(g_i) b$
        $f' ← f' - \frac{f'_{\lm_≤(g_i)}}{\lc_≤(g_i)}a g_i b$

        $r ← True$

        \Break
      }
    }

    \If{$r$} {
      $i ← 1$
    } \Else {
      $i ← i + 1$
    }
  }
  \Return{$f'$}
\end{algorithm}

Este algoritmo consiste básicamente en siempre buscar entre los elementos de $G$ si hay alguno con el cual reducir, y parar cuando ya no hay ninguno.

Una propiedad sobre los reductores que necesitaremos es que mantienen la pertenencia a ideales (lo cual tiene mucho sentido por la definición).

\begin{lemma}\label{lemma:e mantiene pertenencia a ideal}
  Sean $e_≤$ un reductor, $G ⊆ K⟨X⟩$ y $f ∈ (G)$. Entonces
  \[ e_≤(G)(f) ∈ (G) \text{.}\]
\end{lemma}
\begin{proof}
  Es consecuencia directa de la definición y del \cref{thm:→ mantiene pertenencia a ideal}.
\end{proof}

\section{Bases de Gröbner}

Que $→_{≤, G}$ fuera siempre confluente sería muy útil porque significaría que para cualquier clase de equivalencia de $≡_{(G)}$ se podría siempre llegar a una misma forma normal y así determinar si dos elementos son equivalentes. En particular se podría determinar si un elemento está en el ideal viendo si se llega a $0$ como forma normal. Los casos en los que sí es confluente se llaman bases de Gröbner y después algo que haremos es calcularle una base de Gröbner de un ideal generado por un conjunto que no es base de Gröbner.

\begin{definition}\label{def:base de Gröbner}
  Sean $I$ un ideal de $K⟨X⟩$ y $G ⊆ K⟨X⟩$. Se define que
  \[G\text{ es una base de Gröbner de }I ⇔ (G) = I ∧ →_{≤, G}\text{ es confluente} \text{.} \]
  Además se dice que ``$G$ es una base de Gröbner'' si lo es de algún ideal.
\end{definition}

Una consecuencia directa de la definición es el siguiente lema.

\begin{lemma}\label{lemma:→ gröbner es Church-Rosser}
  Sea $G$ una base de Gröbner. Entonces
  \[→_{≤, G}\text{ es Church-Rosser.}\]
\end{lemma}
\begin{proof}
  Es una aplicación directa del \cref{thm:confluente ⇔ Church-Rosser}.
\end{proof}

Las bases de Gröbner se definieron por la propiedad más importante que queremos que tengan, pero las siguientes equivalencias las harán mucho más cómodas de trabajar.

\begin{theorem}\label{thm:equivalencias de base de Gröbner}
  Sean $I$ un ideal de $K⟨X⟩$ y $G ⊆ K⟨X⟩$. Las siguientes afirmaciones son equivalentes:
  \begin{enumerate}
    \item $G$ es una base de Gröbner de $I$. \label{thm:egb:1}

    \item $∀f ∈ K⟨X⟩ : f ∈ I ⇔ f →^*_{≤, G} 0$. \label{thm:egb:2}

    \item $(G) = I ∧ ∀f ∈ K⟨X⟩ : f ∈ I ⇒ f →^*_{≤, G} 0$. \label{thm:egb:3}

    \item $(G) = I ∧ ∀f ∈ I - \{0\} : ∃g ∈ G : \lm_≤(g) | \lm_≤(f)$. \label{thm:egb:4}

    \item $∀f ∈ I - \{0\} : ∃c_1, …, c_n ∈ K, g_1, …, g_n ∈ G, a_1, …, a_n, b_1, …, b_n ∈ ⟨X⟩ : \lm_≤(a_i g_i b_i) ≤ \lm_≤(f) ∧ f = ∑_{i = 1}^n c_i a_i g_i b_i$.  \label{thm:egb:5}
  \end{enumerate}
\end{theorem}
\begin{proof} Probaremos \ref{thm:egb:1} $⇒$ \ref{thm:egb:2}, \ref{thm:egb:2} $⇒$ \ref{thm:egb:3}, \ref{thm:egb:3} $⇒$ \ref{thm:egb:1}, \ref{thm:egb:4} $⇒$ \ref{thm:egb:3}, \ref{thm:egb:2} $⇒$ \ref{thm:egb:5} y \ref{thm:egb:5} $⇒$ \ref{thm:egb:4}.
  \begin{description}

    \item[\ref{thm:egb:1} $⇒$ \ref{thm:egb:2}:] Supongamos que $G$ es una base de Gröbner de $I$ y tomemos $f ∈ K⟨X⟩$. Tenemos que probar $f ∈ I ⇔ f →^*_{≤, G} 0$. Vamos de un lado para el otro:
    \begin{DispWithArrows*}
      &f ∈ I \Arrow{\Cref{lemma:en ideal ⇔ congruente 0}} \\
      & ⇔ f ≡_I 0 \Arrow{\Cref{theorem:→^* = ≡}} \\
      & ⇔ f ↔^*_{≤, G} 0 \Arrow{Por \cref{lemma:→ gröbner es Church-Rosser}, $→_{≤, G}$ es Church-Rosser, definición de Church-Rosser} \\
      & ⇔ f ↓_{≤, G} 0 \Arrow{Definición de $↓$} \\
      & ⇔ ∃f' ∈ K⟨X⟩ : f →^*_{≤, G} f' ∧ 0 →^*_{≤, G} f' \Arrow{\Cref{thm:→ achican} y \cref{lemma:0 es mínimo}} \\
      & ⇔ ∃f' ∈ K⟨X⟩ : f →^*_{≤, G} f' ∧ f' = 0 \\
      & ⇔ f →^*_{≤, G} 0 \text{.}
    \end{DispWithArrows*}

    \item[\ref{thm:egb:2} $⇒$ \ref{thm:egb:3}:] Supongamos el antecedente $∀f ∈ K⟨X⟩ : f ∈ I ⇔ f →^*_{≤, G} 0$. Tenemos que probar que $(G) = I ∧ ∀f ∈ K⟨X⟩ : f ∈ I ⇒ f →^*_{≤, G} 0$. Probemos cada término del $∧$ por separado:

    \begin{description}
      \item[Prueba de $(G) = I$:] Es cierto porque \ref{thm:egb:2} $⇒$ \ref{thm:egb:1} y $(G) = I$ es parte de la definición de base de Gröbner.
      \item[Prueba de $∀f ∈ K⟨X⟩ : f ∈ I ⇒ f →^*_{≤, G} 0$:] Es cierto por \ref{thm:egb:2}.
    \end{description}

    \item[\ref{thm:egb:3} $⇒$ \ref{thm:egb:1}:] Supongamos el antecedente $(G) = I ∧ ∀f ∈ K⟨X⟩ : f ∈ I ⇒ f →^*_{≤, G} 0$. Tenemos que probar que $G$ es una base de Gröbner de $I$, es decir $(G) = I ∧ →_{≤, G}$ es confluente. La parte de $(G) = I$ es válida porque es parte de \ref{thm:egb:3}.

    Para la otra parte, por definición de confluente, alcanza con probar $∀f, f_0, f_1 ∈ K⟨X⟩ : f →^*_{≤, G} f_0 ∧ f →^*_{≤, G} f_1 ⇒ f_0 ↓_{≤, G} f_1$. En tal caso, vale por lo siguiente:
    \begin{DispWithArrows*}
      &f →^*_{≤, G} f_0 ∧ f →^*_{≤, G} f_1 \\
      & ⇒ f_0 ↔^*_{≤, G} f_1 \Arrow{\Cref{theorem:→^* = ≡}} \\
      & ⇒ f_0 ≡_{(G)} f_1 \Arrow{Definición de $≡_G$} \\
      & ⇒ f_0 - f_1 ∈ (G) \Arrow{Antecedente} \\
      & ⇒ f_0 - f_1 →^*_{≤, G} 0 \Arrow{\Cref{lemma:suma →↓}} \\
      & ⇒ (f_0 - f_1) + f_1 ↓_{≤, G} 0 + f_1 \\
      & ⇒ f_0 ↓_{≤, G} f_1 \text{.}
    \end{DispWithArrows*}

    \item[\ref{thm:egb:4} $⇒$ \ref{thm:egb:3}:] Supongamos el antecedente $(G) = I ∧ ∀f ∈ I - \{0\} : ∃g ∈ G : \lm_≤(g) | \lm_≤(f)$. Tenemos que probar $(G) = I ∧ ∀f ∈ K⟨X⟩ : f ∈ I ⇒ f →^*_{≤, G} 0$. La parte de $(G) = I$ es válida porque es parte de \ref{thm:egb:4}. Para la otra parte probémoslo por contradicción. En particular tomemos el mínimo $f$ tal que $f ∈ I$ pero no se cumple que $f →^*_{≤, G} 0$.

    Por \ref{thm:egb:4} sea $g ∈ G$ tal que $\lm_≤(g) | \lm_≤(f)$ y sean también:
    \begin{itemize}
      \item $a, b ∈ ⟨X⟩$ tales que $a \lm_≤(g) b = \lm_≤(f)$.
      \item $f' = f - \frac{f_{\lm_≤(agb)}}{\lc_≤(g)}agb$.
    \end{itemize}

    Notar que:

    \begin{itemize}
      \item $f' ∈ I$ ya que $f ∈ I$ y $g ∈ (G) = I$,
      \item $f →_{≤, G} f'$ por definición de $→_{≤, G}$ y
      \item $f' < f$ por el \cref{thm:→ achican}.
    \end{itemize}

    \noindent Ahora como no vale $f →^*_{≤, G} 0$, tampoco puede valer $f' →^*_{≤, G} 0$. Sin embargo, esto contradice que $f$ sea mínimo.

    \item[\ref{thm:egb:2} $⇒$ \ref{thm:egb:5}:] Supongamos \ref{thm:egb:2} y tomemos $f ∈ I - \{0\}$.

    Por \ref{thm:egb:2} tenemos que $f →^*_{≤, G} 0$.

    Sean:
    \begin{itemize}
      \item $n ∈ ℕ, f_0, …, f_n ∈ K⟨X⟩$ tales que $f_0 = f$, $f_n = 0$ y $f_0 →_{≤, G} f_1 →_{≤, G} … →_{≤, G} f_n$, los cuales existen porque $f →^*_{≤, G} 0$ y por el \cref{lemma:→* como ∃}.
      \item Para cada $i ∈ \{1, …, n\}$, $g_i ∈ G, a_i, b_i ∈ X$ tales que $\lm_≤(a_i g_i b_i) ∈ \sop(f_{i-1})$ y $f_i = f_{i-1} - \frac{(f_{i-1})_{\lm_≤(a_i g_i b_i)}}{\lc_≤(g_i)}a_i g_i b_i$ los cuales existen por definición de $→_{≤, G}$.
    \end{itemize}

    Probaremos que el $∃$ de \ref{thm:egb:5}, se satisface con $c_i = \frac{(f_{i-1})_{\lm_≤(a_i g_i b_i)}}{\lc_≤(g_i)}, g_i, a_i, b_i$. O sea, tenemos que probar $\lm_≤(a_i g_i b_i) ≤ \lm_≤(f)$ y $f = ∑_{i = 1}^n \frac{(f_{i-1})_{\lm_≤(a_i g_i b_i)}}{\lc_≤(g_i)} a_i g_i b_i$.

    \begin{description}
      \item[Prueba de $\lm_≤(a_i g_i b_i) ≤ \lm_≤(f)$ fijando $i$:] Por el \cref{thm:→ achican} y por transitividad de $<$ tenemos $f_{i-1} ≤ f$. Además tenemos $a_i g_i b_i ≤ f_{i-1}$ porque $\lm_≤(a_i g_i b_i) ∈ \sop(f_{i-1})$, así que por transitividad de $≤$ vale $\lm_≤(a_i g_i b_i) ≤ \lm_≤(f)$.
      \item[Prueba de $f = ∑_{i = 1}^n \frac{(f_{i-1})_{\lm_≤(a_i g_i b_i)}}{\lc_≤(g_i)} a_i g_i b_i$:] Es consecuencia directa de como se eligieron los $g_i, a_i, b_i$.
    \end{description}

    \item[\ref{thm:egb:5} $⇒$ \ref{thm:egb:4}:] Supongamos \ref{thm:egb:5} y probemos \ref{thm:egb:4}. Para eso tenemos que probar $(G) = I$ y $∀f ∈ I : ∃g ∈ G - \{0\} : \lm_≤(g) | \lm_≤(f)$.

    \begin{description}
      \item[Prueba de $(G) = I$:] Tomemos $f ∈ I$ y probemos $f ∈ (G)$.

      Por \ref{thm:egb:5} tenemos que $f = ∑_{i = 1}^n c_i a_i g_i b_i$ con $c_i ∈ K$, $a_i, b_i ∈ ⟨X⟩$ y $g_i ∈ G$.

      Esto encaja exactamente con la definición de $(G)$, así que queda probado $f ∈ (G)$.

      \item[Prueba de $∀f ∈ I - \{0\} : ∃g ∈ G : \lm_≤(g) | \lm_≤(f)$:] Fijemos $f ∈ I - \{0\}$.

      Por \ref{thm:egb:5} tenemos que $f = ∑_{i = 1}^n c_i a_i g_i b_i$ con $c_i ∈ K$, $a_i, b_i ∈ ⟨X⟩$, $g_i ∈ G$ y $\lm_≤(a_i g_i b_i) ≤ \lm_≤(f)$.

      Como $\lm_≤(a_i g_i b_i) ≤ \lm_≤(f)$, sí o sí tiene que pasar para algún $j$ que $\lm_≤(a_j g_j b_j) = \lm_≤(f)$ y para este $g_j$ vale que $\lm_≤(g_j) | \lm_≤(f)$.
    \end{description}

  \end{description}
  Con esto se termina la prueba.
\end{proof}

\begin{observation}
  \ref{thm:egb:2} del \cref{thm:equivalencias de base de Gröbner} nos da una manera precisa de responder al \cref{problem:principal}. Para decidir si $f$ está en el ideal generado por $G$, primero le calculamos una base de Gröbner al ideal. Luego si pudimos encontrar una base de Gröbner finita, aplicamos el \cref{alg:reductor} con esta base y si el resultado es $0$ entonces $f$ está en el ideal.
\end{observation}

Combinando esta observación con el \cref{lemma:sub gen y sub gen ⇔ eq} tenemos el siguiente colorario. % ¿Está bien escrito colorario? Porque me lo pone como error

\begin{colorary}\label{col:(G) = (G') cond}
  Sean $G, G' ⊆ K⟨X⟩$ bases de Gröbner, entonces
  \[ (G) = (G') ⇔ (∀g ∈ G : g →_{≤, G'} 0) ∧ (∀g' ∈ G' : g' →_{≤, G} 0)\]
  \qed
\end{colorary}


\section{Algoritmo de Buchberger}

En esta sección se explica el primer algoritmo para calcular bases de Gröbner llamado Algoritmo de Buchberger.

Antes de poder calcular bases de Gröbner algo que estaría bueno hacer es poder computar si un conjunto es una base de Gröbner, porque ninguna de las equivalencias del \cref{thm:equivalencias de base de Gröbner} es directamente calculable ya que hacen cuantificaciones sobre conjuntos infinitos. Para eso definiremos algo llamado S-polinomio de dos polinomios y los usaremos para enunciar un teorema que nos da una forma computable de determinar si un conjunto es una base de Gröbner. Los S-polinomios entre dos polinomios multiplican cada polinomio por monomios a cada lado y por un escalar de forma que los monomios principales se cancelen. O sea, para polinomios $f$ y $g$ tendremos una cuenta $k a f b - k' c g d$ de forma que se cancelen los monomios principales.

Primero definimos las ambigüedades, que representan los polinomios para los cuales puede pasar eso.

\begin{definition}
  Sean $m, m', a, b, c, d ∈ ⟨X⟩$. Se define
  \[ (a, b, c, d, m, m')\text{ es una ambigüedad} ⇔ amb = cm'd \text{.}\]

  La ambigüedad $(a, b, c, d, m, m')$ se define como:
  \begin{itemize}
    \item de superposición $⇔ (a = ε = d ∧ |b| < |m'| ∧ |c| < |m|) ∨ (b = ε = c ∧ |a| < |m'| ∧ |d| < |m|)$.
    \item de inclusión $⇔ a = ε = b ∨ c = ε = d$.
    \item relevante $⇔$ es de superposición o de inclusión.
  \end{itemize}

  Si $f, f' ∈ K⟨X⟩$ se define que $(a, b, c, d, f, f')$ es una ambigüedad si y solo si $(a, b, c, d, \lm_≤{(f)}, \allowbreak \lm_≤{(f')})$ es una ambigüedad y lo mismo para ambigüedades de superposición, de inclusión y relevantes. % Tube que poner \allowbreak para que no se salga del margen

  Además se define
  \[ \amb(f, f') = \{(a, b, c, d, f, f') : (a, b, c, d, f, f')\text{ es una ambigüedad relevante}\} \text{.} \]

  Finalmente para $F ⊆ K⟨X⟩$ se define
  \[ \amb(F) = ⋃_{f, f' ∈ F - \{0\}}{\amb(f, f')} \text{.} \]

\end{definition}

Notar que por cómo es la definición de ambigüedad, cuando es relevante siempre hay una parte de $m$ y una parte de $m'$ que son iguales y que no están en los monomios $a$, $b$, $c$ y $d$. Los casos relevantes son los únicos importantes, y por eso son los que se usan para definir $\amb$. No relevantes siempre hay entre cualquier par de monomios, por ejemplo tomando $a = d = ε$, $b = m'$ y $c = m$.

\begin{definition}
  Sean $a, b, c, d ∈ ⟨X⟩, f, f' ∈ K⟨X⟩$ y $α = (a, b, c, d, f, f')$ una ambigüedad. Se define el S-polinomio de $α$ como
  \[ \S(α) = \frac{afb}{\lc_≤{(f)}} - \frac{cgd}{\lc_≤{(f')}} \text{.}\]
\end{definition}

Por ejemplo, si $f = 2 cba + 3 dbcda$ y $f' = b + bcd$, entonces una ambigüedad de inclusión sería

\[α = (ε, ε, d, a, f, f') \text{.} \]

\noindent Siendo su S-polinomio

\[S(α) = \frac{f}{3} - d f' a = \frac{2}{3} cba - dba  \text{.} \]

\noindent Y una ambigüedad de superposición sería

\[ α' = (bc, ε, ε, bcda, f, f') \text{.} \]

\noindent Siendo su S-polinomio

\[ S(α') = \frac{bc f}{3} - f' bcda = bbcda - \frac{2}{3} bccba \text{.} \]

\

Ahora probaremos que efectivamente los monomios principales se cancelan y después enunciaremos el teorema para decidir si un conjunto es una base de Gröbner.

\begin{lemma}\label{lemma:lm ambs}
  Sea $α = (a, b, c, d, f, g)$ una ambigüedad. Entonces:
  \[ \lm_≤{(afb)} = \lm_≤{(cgd)} \text{.}\]
\end{lemma}
\begin{proof}\
  Las siguientes igualdades siguen por la definición de ambigüedad tanto para monomios como para polinomios.
  \[ \lm_≤{(afb)} = a\lm_≤{(f)}b = c\lm_≤{(g)}d = \lm_≤{(cgd)} \text{.} \]
\end{proof}

Eso permite definir el monomio principal de una ambigüedad.

\begin{definition}
  Sea $α = (a, b, c, d, f, g)$ una ambigüedad. Se define el monomio principal de $α$ como

  \[ \lm_≤(α) = \lm_≤(afb) \text{.}\]
\end{definition}

Notar que por el \cref{lemma:lm ambs} también vale que $\lm_≤(α) = \lm_≤(cgd)$.

Lo de que los monomios principales se cancelen produce el siguiente lema.

\begin{lemma}
  Sea $α = (a, b, c, d, f, g)$ una ambigüedad. Entonces
  \[ \lm_≤(\S(α)) < \lm_≤(α) \text{.}\]
\end{lemma}
\begin{proof}
  Esto es porque en la resta $\frac{afb}{\lc_≤(f)} - \frac{cgd}{\lc_≤(g)}$ los monomios principales se cancelan.
\end{proof}

Otra propiedad importante es que estos S-polinomios son cerrados en el ideal, es decir, que el S-polinomio de dos elementos de un ideal es un elemento del ideal.

\begin{lemma}\label{lemma:S es cerrado en ideal}
  Sean $I ⊆ K⟨X⟩$ un ideal $f, g ∈ I$ y $α = (a, b, c, d, f, g)$ una ambigüedad. Entonces
  \[ \S(α) ∈ I \text{.}\]
\end{lemma}
\begin{proof}
  En la definición de $\S$ se ve que es una combinación lineal de $f$ y $g$ con elementos de $K⟨X⟩$. Más precisamente, con los elementos $\frac{a}{\lc_≤{(f)}}$, $b$, $\frac{c}{\lc_≤{(g)}}$ y $d$. Como $f, g ∈ I$ e $I$ es un ideal, esa combinación pertenece a $I$.
\end{proof}

Ahora si enunciamos el teorema para decidir si un conjunto es una base de Gröbner.

\begin{theorem}[Condición de Buchberger]\label{thm:condición de Buchberger}
  Sean $I$ un ideal de $K⟨X⟩$ y $G ⊆ K⟨X⟩$. Entonces son equivalentes:
  \begin{enumerate}
    \item $G$ es una base de Gröbner de $I$.
    \item $∀α ∈ \amb(G) : \S(α) →^*_{≤, G} 0$.
  \end{enumerate}
  \qed
\end{theorem}

La demostración se puede encontrar por ejemplo en \cite{phdthesis:Hof23}, en donde algo similar a esto está enunciado en el Teorema 2.4.54. No lo demostramos acá porque requiere varios lemas adicionales que no son pertinentes para esta tesis.

Con esto ya se puede explicar la idea del algoritmo de Buchberger. Supongamos que tenemos un conjunto que no sabemos si es base de Gröbner o no y estamos chequeando si lo es con el \cref{thm:condición de Buchberger}. Si nos encontramos con un S-polinomio $f$ que se reduce a un polinomio $g$ distinto de $0$ podemos agregar $g$ al conjunto y con eso $f$ pasa a sí reducirse a $0$. Se podría pensar que agregar $g$ cambiaría el ideal generado, pero por el \cref{lemma:S es cerrado en ideal} sabemos que $g$ es un elemento del ideal y por el \cref{thm:→ mantiene pertenencia a ideal} eso implica que $f$ también pertenece al ideal. Por ende, por el \cref{lemma:gen G = gen G U a con a ∈ gen G} se puede agregar al conjunto y que el ideal generado siga siendo el mismo. El problema de agregar un nuevo polinomio es que si bien hay un S-polinomio que pasa a sí reducirse a $0$ también aparecen nuevas ambigüedades. Eso, sin embargo, no es tanto problema porque lo que se hace es repetir hasta que en algún momento todas las ambigüedades se reducen a $0$. Si eso no pasa nunca simplemente el algoritmo no termina nunca, lo que refleja la no decidibilidad del problema de pertenencia a un ideal.

Ese proceso de agregar polinomios infinitamente lo definimos matemáticamente del siguiente modo.

\begin{definition}
  Sean $G ⊆ K⟨X⟩$ y $e_≤$ un reductor. Se definen:
  \begin{itemize}
    \item $\B_{e_≤}^0(G) = G$.
    \item $\B_{e_≤}^{i + 1}(G) = \B_{e_≤}^i(G) ∪ \{e_≤(\B_{e_≤}^i(G))(\S(α)) : α ∈ \amb(\B_{e_≤}^i(G))\}$.
    \item $\B_{e_≤}(G) = ⋃_{i = 0}^∞ \B_{e_≤}^i(G)$.
  \end{itemize}
  A $\B_{e_≤}(G)$ lo llamamos base de Buchberger de $(G)$.
\end{definition}

El nombre base de Buchberger es inventado para esta tesis, otros autores no le han dado ningún nombre concreto a esos conjuntos.

Con el siguiente teorema se enuncia que la base de Buchberger es una base de Gröbner y que si hay una base de Gröbner finita entonces en alguna iteración finita se llega, o sea, algún $B_{e_≤}^{i}(G)$ es igual a la base de Buchberger.

\begin{theorem}\label{thm:Buchberger correctitud}
  Sean $G ⊆ K⟨X⟩$ y $e_≤$ un reductor. Entonces
  \begin{enumerate}
    \item $\B_{e_≤}(G)$ es una base de Gröbner de $(G)$. \label{thm:Buchberger correctitud:item:1}
    \item $(G)$ tiene una base de Gröbner finita $⇒ ∃i ∈ ℕ ∪ \{0\} : (\B_{e_≤}^i(G))$ es una base de Gröbner. \label{thm:Buchberger correctitud:item:2}
  \end{enumerate}
\end{theorem}

Es esperable que \ref{thm:Buchberger correctitud:item:1} sea cierto por cómo definimos la base de Buchberger, mientras que \ref{thm:Buchberger correctitud:item:2} es más sorprendente. Para demostrar ambos ítems primero probaremos algunos lemas en el contexto de este teorema, o sea con $G$ y $e_≤$ fijados.

\begin{lemma}\label{lemma:Buchberger correctitud:3}
  $∀i ∈ ℕ ∪ \{0\} : \B_{e_≤}^{i}(G) ⊆ (G)$.
\end{lemma}
\begin{proof}
  Procedamos por inducción sobre $i$. El caso base se cumple porque $G ⊆ (G)$. Para el caso inductivo, supongamos que la afirmación es cierta para $i$ y probemos que también vale para $i + 1$. Para eso tomemos $f ∈ \B_{e_≤}^{i + 1}(G)$ y probemos que $f ∈ (G)$.

  Por la definición recursiva de $\B_{e_≤}^{i + 1}$ vale que $f ∈ \B_{e_≤}^i(G) ∨ ∃α ∈ \amb(\B_{e_≤}^i(G)) : f = e_≤(\B_{e_≤}^i(G))(\S(α))$. Dividamos en casos según ese $∨$.

  \begin{description}
    \item[Caso $f ∈ \B_{e_≤}^i(G)$:] Es válido por ser exactamente la hipótesis inductiva.
    \item[Caso $∃α ∈ \amb(\B_{e_≤}^i(G)) : f = e_≤(\B_{e_≤}^i(G))(\S(α))$:] En tal caso, por el \cref{lemma:e mantiene pertenencia a ideal} vale que $f ∈ \B_{e_≤}^i(G)$ y por la hipótesis inductiva que $f ∈ (G)$.
  \end{description}

\end{proof}

\begin{lemma}\label{lemma:Buchberger correctitud:4}
  $\B_{e_≤}(G) ⊆ (G)$.
\end{lemma}
\begin{proof}
  Esto es consecuencia directa del \cref{lemma:Buchberger correctitud:3} y la definición de $\B_{e_≤}$.
\end{proof}

\begin{lemma}\label{lemma:Buchberger correctitud:5}
  $(\B_{e_≤}(G)) = (G)$.
\end{lemma}
\begin{proof}
  Por la definición de $\B_{e_≤}^i$ tenemos $G ⊆ \B_{e_≤}^i(G)$ y por ende $G ⊆ (\B_{e_≤}(G))$. Con el \cref{lemma:Buchberger correctitud:4} aplicando el \cref{lemma:sub gen y sub gen ⇔ eq} tenemos lo que queremos probar.
\end{proof}

\begin{lemma}\label{lemma:Buchberger correctitud:6}
  $∀α ∈ \amb(\B_{e_≤}(G)) : \S(α) →^*_{≤, \B_{e_≤}(G)} 0$.
\end{lemma}
\begin{proof}
  Tomemos $α = (a, b, c, d, f, g) ∈ \amb(\B_{e_≤}(G))$. Por definición de $\amb$ vale que $f, g ∈ \B_{e_≤}(G)$, así que sean:
  \begin{itemize}
    \item $i ∈ ℕ$ el mínimo tal que $f ∈ \B_{e_≤}^i(G)$,
    \item $j ∈ ℕ$ el mínimo tal que $g ∈ \B_{e_≤}^j(G)$ y
    \item $k = \max(i, j)$.
  \end{itemize}

  \noindent Por definición de $\amb$ tiene que valer que
  \[ α ∈ \amb(\B_{e_≤}^k(G)) \]
  y entonces por definición de $\B_{e_≤}^i$ vale que
  \[ e_≤(\B_{e_≤}^k(G))(\S(α)) ∈ \B_{e_≤}^{k + 1}(G) \text{.} \]
  Esto implica que
  \[ e_≤(\B_{e_≤}^k(G))(\S(α)) →_{≤, \B_{e_≤}^{k + 1}(G)} 0\]
  y por ende que
  \[ e_≤(\B_{e_≤}^k(G))(\S(α)) →_{≤, \B_{e_≤}(G)} 0 \text{.} \]
  Llamemos (i) a esto último. Por otro lado, por definición de reductor tenemos que
  \[ \S(α) →^*_{≤, \B_{e_≤}^k(G)} e_≤(\B_{e_≤}^k(G))(\S(α)) \]
  y por ende
  \[ \S(α) →^*_{≤, \B_{e_≤}(G)} e_≤(\B_{e_≤}^k(G))(\S(α)) \text{.} \]
  Llamemos (ii) a esto último. Combinando (i) y (ii) vale que $\S(α) →^*_{≤, \B_{e_≤}(G)} 0$, que es lo que queríamos probar.
\end{proof}

Con estos lemas ahora sí hagamos la demostración del \cref{thm:Buchberger correctitud}.

\begin{proof}[Demostración del \cref{thm:Buchberger correctitud}]
  Por el \cref{lemma:Buchberger correctitud:5} y el \cref{lemma:Buchberger correctitud:6} vale la equivalencia de base de Gröbner de la condición de Buchberger (\cref{thm:condición de Buchberger}), así que \ref{thm:Buchberger correctitud:item:1}, o sea, que $G$ es una base de Gröbner queda probado.

  Para probar \ref{thm:Buchberger correctitud:item:2}, asumamos el antecedente, o sea, que $(G)$ tiene una base de Gröbner finita, y tomemos una base finita $\{g_1, …, g_n\}$. Ahora tenemos que probar el consecuente, o sea, que $∃i ∈ ℕ ∪ \{0\} : (\B_{e_≤}^i(G))$ es una base de Gröbner.  Para cada $i ∈ \{1, …, n\}$ sean:
  \begin{itemize}
    \item $g_{i, 1}, …, g_{i, k_i} ∈ \B_{e_≤}(G), a_{i, 1}, …, a_{i, k_i}, b_{i, 1}, …, b_{i, k_i} ∈ ⟨X⟩$ con $\lm_≤(a_{i, j} g_{i, j} b_{i, j}) ≤ \lm_≤(g_i)$ tales que $g_i = ∑_{j = 1}^{k_i} a_{i, j} g_{i, j} b_{i, j}$. Los cuales existen por por \ref{thm:egb:5} del \cref{thm:equivalencias de base de Gröbner}.

    \item $G' = \{g_{i, j} : i ∈ \{1, …, n\}, j ∈ \{1, …, k_i\}\}$.

    \item $k ∈ ℕ$ el mínimo tal que $G' ⊆ \B_{e_≤}^k(G)$. Notar que $k$ está bien definido porque $G'$ es finito y los $g_{i, j}$ están en $\B_{e_≤}(G)$.
  \end{itemize}

  Vamos a probar que $\B_{e_≤}^k(G)$ es una base de Gröbner de $(G)$. Por \ref{thm:egb:5} del \cref{thm:equivalencias de base de Gröbner} alcanza con probar:
  \[ ∀f ∈ (G) - \{0\} : ∃g'_1, …, g'_n ∈ \B_{e_≤}^k(G), a_1, …, a_n, b_1, …, b_n ∈ ⟨X⟩ : \lm_≤(a_i g'_i b_i) ≤ \lm_≤(f) ∧ f = ∑_{i = 1}^n a_i g'_i b_i \text{.}\]

  Tomemos $f ∈ (G) - \{0\}$ y escribámoslo de esa forma. Por \ref{thm:egb:5} del \cref{thm:equivalencias de base de Gröbner} y el hecho de que $\{g_1, …, g_n\}$ es una base de Gröbner, sean $i'_1, …, i'_{n'} ∈ \{1, …, n\}, a'_1, …, a'_{n'}, b'_1, …, b'_{n'} ∈ ⟨X⟩$ tales que:
  \begin{itemize}
    \item $\lm_≤(a'_i g_{i'_i} b'_i) ≤ \lm_≤(f)$ y
    \item $f = ∑_{i = 1}^{n'} a'_i g_{i'_i} b'_i$.
  \end{itemize}

  Con esto tenemos:

  \begin{DispWithArrows*}
    &f = ∑_{i = 1}^{n'} a'_i g_{i'_i} b'_i \Arrow{Condición de $g_{i, j}$} \\
    & = ∑_{i = 1}^{n'} a'_i (∑_{j = 1}^{k_{i'_i}} a_{i', j} g_{i, j} b_{i', j}) b'_i \\
    & = ∑_{i = 1}^{n'} ∑_{j = 1}^{k_i'} a'_i a_{i', j} g_{i, j} b_{i', j} b'_i \text{.}
  \end{DispWithArrows*}

  Sabemos además que $g_{i',j} ∈ \B_{e_≤}^k(G)$ y que $g_{i', j} ≤ g_{i'_i} ≤ f$. Así que queda $f$ escrito como queríamos y la prueba queda completada.
\end{proof}

Con estos conjuntos podemos de forma directa dar un seudocódigo que los calcula:

\begin{algorithm}[H] % La H es para que se quede acá, porque se iba a otra página. Estaría bueno hacerlo global
  \caption{Algoritmo de Buchberger}\label{alg:Buchberger}
  \KwData{$G ⊆ K⟨X⟩$, $e_≤$ un reductor}
  \KwResult{$B ⊆ K⟨X⟩$ una base de Gröbner de $(G)$ si es que termina}
  $B ← G$

  \Loop{} {
    $ambs ← \amb(B)$

    $B' ← B$

    \For{$α ∈ ambs$} {
      $f ← e_≤(B)(\S(α))$

      \If{$f ≠ 0$} {
        $B' ← B' ∪ \{f\}$
      }
    }

    \If{$B' = B$} {
      \Break
    }

    $B ← B'$
  }
  \Return{$B$}
\end{algorithm}

El algoritmo así, si bien es una implementación literal de la definición de los conjuntos $\B_{e_≤}^i$ que probamos que es correcta, es muy lento y necesita mejoras. Una mejora muy importante, probablemente la más importante, consiste en usar también los nuevos polinomios que ya fueron agregados para hacer las nuevas reducciones en el paso. Es decir, llamar a $e_≤$ con $B'$ en lugar de con $B$ en la línea 6. % ¿Se podrá hacer con un label esto?

Por lo visto antes, el cambio tiene sentido que sea correcto y la prueba es similar a la del \cref{thm:Buchberger correctitud}. En la práctica, esta mejora hace una diferencia inmensa en la eficiencia, pasa de no andar rápido casi nunca a andar rápido en una gran cantidad de casos. La justificación teórica de por qué hacer eso hace que pase a ser tanto más rápido no la sé ni la pude encontrar en la literatura.

Hay otras dos optimizaciones que se pueden hacer: una es para descartar algunas ambigüedades sin tener que reducirlas y la otra es para ir sacando algunos polinomios de la base. En la \cref{secton:optimizaciones} se aborda este tema.

\section{Algoritmo F4}
En esta sección se explica el algoritmo F4, que es otro método para calcular bases de Gröbner. La idea de F4 es considerar varias ambigüedades y reducir todos sus S-polinomios al mismo tiempo, reduciendo con los elementos que ya están en la base y entre los propios S-polinomios que se están considerando. Para esto se convierte el problema en una reducción por filas de una matriz, codificando cada monomio como una columna y cada polinomio como una fila. Para codificar los polinomios como filas de una matriz hacen falta algunas definiciones.

\begin{definition}
  Sea $F ⊆ K⟨X⟩$. Se define
  \[ \spn_K(F) = \{∑_{i = 1}^n c_i f_i : n ∈ ℕ, c_i ∈ K, f_i ∈ F\} \text{.} \]
\end{definition}

\begin{definition}
  Sean $M = \{m_1, …, m_n\} ⊆ ⟨X⟩$ con $m_1 > ⋯ > m_n$. Se define la función $\poli_M : K^n → \spn_K(M)$ de la siguiente manera:
  \[ \poli_M(v) = ∑_{i = 1}^n v_i m_i \text{.} \]
\end{definition}

\begin{definition}
  Sean $M = \{m_1, …, m_n\} ⊆ ⟨X⟩$ con $m_1 > ⋯ > m_n$. Se define $\mat_M : \spn_K(M) → K^n$ como la inversa de $\poli_M$:
  \[ \mat_M(f) = \poli_M^{-1}(f) \text{.} \]
  Para una lista de polinomios se define que $\mat_M$ produzca una matriz:
  \[ \mat_M(f_1, …, f_m) = \begin{pmatrix} \mat_M(f_1) \\ ⋮ \\ \mat_M(f_m) \end{pmatrix} \text{.} \]
\end{definition}

Estas definiciones son un diccionario polinomios-matrices y matrices-polinomios en la que cada monomio se corresponde con una columna de la matriz.

Con esto, en la reducción, restar un polinomio multiplicado por un escalar y un monomio a cada lado a otro polinomio, o sea, hacer $f ← f - \frac{f_{\lm_≤(g)}}{\lc_≤(g)}a g b$ al reducir $f$ con $g$, la idea es que pase a ser restarle una fila multiplicada por un escalar a otra fila en la matriz, o sea, hacer $\text{fila}_i ← \text{fila}_i - c · \text{fila}_j$.

Para poder hacer eso, al comenzar hay que tener en la matriz los polinomios a restar ya multiplicados por los monomios a cada lado, es decir, cuando haga falta hacer $f ← f - \frac{f_{\lm_≤(g)}}{\lc_≤(g)}a g b$ hay que tener $agb$ en la matriz. Tener solo $g$ en la matriz no serviría porque no coincidirían los monomios y por ende no coincidirían las columnas de la matriz.

Para eso definimos el algoritmo de preprocesamiento simbólico que, dado el conjunto por el cual se reduce $G$ y el conjunto a reducir $F$, calcula todos los polinomios necesarios:

\begin{algorithm}[H] % La H es para que se quede acá, porque se iba a otra página. Estaría bueno hacerlo global
  \caption{Preprocesamiento simbólico}\label{alg:Preprocesamiento simbólico}
  \KwData{$G, F ⊆ K⟨X⟩$ conjuntos finitos}
  \KwResult{$G' ⊆ \{agb : a, b ∈ ⟨X⟩, g ∈ G\}$}
  $G' ← ∅$

  $considerados ← ⋃_{g ∈ P} \sop(g)$

  $T ← ⋃_{g ∈ P} \sop(\tail_≤(g))$

  \While{$T ≠ ∅$} {
    elegir $m ∈ T$

    $T ← T - \{m\}$

    \For{$g ∈ G$} {
      \If{$\lm_≤(g) | m$} {
        calcular $a$ y $b$ tales que $m = a \lm_≤(g) b$

        $G' ← G' ∪ {agb}$

        $nuevos ← \{m' ∈ \sop(\tail_≤(agb)) : m' ∉ considerados\}$

        $T ← T ∪ nuevos$

        $considerados ← considerados ∪ nuevos$
      }
    }
  }

  \Return{$G'$}
\end{algorithm}

El seudocódigo se puede entender cómo funciona analizándolo en detalle, pero lo importante es saber que calcula los polinomios que van a hacer falta que sean las filas de la matriz, y no importa demasiado entender cómo funciona por dentro.

Una vez que tenemos los monomios necesarios, armamos la matriz correspondiente a $G$ y $F$ usando la función $\mat_T$ y reducimos por fila esta matriz. Con la matriz no es obvio qué hay que hacer, porque hay que solo considerar las filas que corresponden a los polinomios reducidos, pero no necesariamente son las mismas filas que eran antes los polinomios sin reducir porque la reducción por filas puede hacer cosas como intercambiar filas. Para saber cuáles son esas filas lo que se hace es guardar cuáles son los monomios principales de elementos del resultado del preprocesamiento simbólico y después de hacer la reducción por filas agarrar solo las filas que tengan un uno principal en una columna que no corresponda a uno de esos monomios principales.

El siguiente seudocódigo hace eso:

\begin{algorithm}[H] % La H es para que se quede acá, porque se iba a otra página. Estaría bueno hacerlo global
  \caption{Multireducción}\label{alg:Multireducción}
  \KwData{$G, F ⊆ K⟨X⟩$ conjuntos finitos}
  \KwResult{$F' ⊆ K⟨X⟩$}
  $G' ←$ Preprocesamiento simbólico$(G, F)$

  $monomios ← \{\lm_≤(g) : g ∈ G'\}$

  $M ← \{m : m ∈ \sop(g), g ∈ G' ∪ F\}$

  $mat ← \mat_M(G' ∪ F)$

  reducir por filas $mat$

  $F' ← \{\poli_M(v) : v ∈ \filas(mat) : v ≠ 0 ∧ \lm_≤(\poli_M(v)) ∉ monomios\}$

  \Return{$F'$}
\end{algorithm}

Con estos dos algoritmos ya podemos escribir el algoritmo F4:

\begin{algorithm}[H] % La H es para que se quede acá, porque se iba a otra página. Estaría bueno hacerlo global
  \caption{F4}\label{alg:F4}
  \KwData{$G ⊆ K⟨X⟩$, $e_≤$ un reductor}
  \KwResult{$B ⊆ K⟨X⟩$ una base de Gröbner de $(G)$ si es que termina}
  $B ← G$

  \Loop{} {
    $ambs ← \amb(B)$

    $B' ← B$

    \While{$ambs ≠ ∅$} {
      elegir $Α ⊆ ambs$

      $ambs ← ambs - Α$

      $P ← \{\S(α) : α ∈ Α\}$

      $P ←$ Multireducción$(B', P)$

      $B' ← B' ∪ P$
    }

    \If{$B' = B$} {
      \Break
    }

    $B ← B'$
  }

  \Return{$B$}
\end{algorithm}

Este algoritmo no especifica cuáles ambigüedades elegir, pero una buena estrategia es elegir todas las de menor grado. Otra opción es elegir todas, pero eso puede hacer que las matrices se vuelvan muy grandes rápidamente.

En \cite{thesis:Hof20} se da un algoritmo muy parecido a este (con una diferencia mínima) y se demuestra que es correcto.

Las optimizaciones mencionadas en la sección de Buchberger para descartar ambigüedades y para quitar polinomios de la base también se pueden hacer acá. Otra optimización que se puede hacer es usar un algoritmo de reducción de matrices que anda más rápido en las matrices de este problema en particular, llamado eliminación Faugère-Lachartre. Esto también es abordado en la \cref{secton:optimizaciones}.

\chapter{Librería}\label{cap:Librería}

Como se dijo en la introducción, para este trabajo se implementaron los algoritmos de Buchberger y F4 en \cpp junto con estructuras para manejar polinomios no conmutativos en una librería llamada \texttt{ncgb}. La librería está disponible en el repositorio \url{https://github.com/IvanRenison/Non-commutative-Grobner-Bases}.

En este capítulo se explica cómo se usa la librería sin explicar como está implementada y en el siguiente se explica cómo está implementada. En la fecha de la escritura de esta tesis, el último commit tiene el tag \href{https://github.com/IvanRenison/Non-commutative-Grobner-Bases/tree/v0.1}{\texttt{v0.1}} y todas las explicaciones de esta tesis se refieren esa versión.

El repositorio tiene varios directorios, pero hay dos que son los más relevantes. El primero es el directorio \path{ncgb}, que es donde está alojada la librería en sí. Como la librería es genérica sobre algunos parámetros, se usan templates. Las clases y funciones genéricas se definen en archivos \path{.hpp}, donde se incluye tanto su declaración como su implementación. El otro directorio relevante es \path{mains}, que tiene varios archivos \path{.cpp} con funciones \texttt{main} que usan la librería. Estos archivos son útiles para ver ejemplos de cómo usar la librería.

Todo el código de la librería está en el namespace \texttt{ncgb}. En todos los códigos que veamos asumiremos que \texttt{ngcb} está abierto.

\section{Uso general de la librería}

Los polinomios como los definimos en el \cref{cap:Preliminares} están asociados a un cuerpo y un conjunto de variables; además, muchas operaciones y métodos de los polinomios (y de otras definiciones) dependen de un orden monomial. Para que se puedan variar esas tres cosas se usan templates para hacer las funciones y estructuras de forma genérica. Por ejemplo, el esqueleto de la definición de polinomio es así: % No se como reemplazar "cosas" acá porque son dos tipos y un orden

\begin{minted}{C++}
  template<typename K, typename X = __uint8_t, class ord = DegLexOrd<X>>
  struct Poly {
    // … Implementación
  };
\end{minted}

El tipo \texttt{K} tiene que tener implementadas todas las operaciones de cuerpo. En particular, la librería asume que tiene definidas las siguientes operaciones y construcciones:

\begin{itemize}
  \item \texttt{+}, \texttt{-} (tanto unario como binario), \texttt{*} y \texttt{/} junto con sus versiones de asignación \texttt{+=}, \texttt{-=}, \texttt{*=} y \texttt{/=}. De las versiones de asignación no se usa el resultado, solo el hecho de que modifican el operando izquierdo, así que pueden devolver \texttt{void}.
  % Miguel puso: No entiendo qué significa esto. ¿Es relevante para que yo pueda definir un nuevo cuerpo?
  % Es relevante porque significa que podés hacer que devuelvan void, y eso de hecho pasa en ModularArithmetic
  % No se como explicarlo de otra forma.
  \item \texttt{==} (y también \texttt{!=}, pero ese no hace falta definirlo a mano, porque a partir de \cppXX está automáticamente con \texttt{==}).
  % Miguel sugirió: (y también !=. Sólo es necesario si se compila con un estándar anterior a C++20 ).
  % No creo que compile con un C++ anterior al 20, pero, algo pasa es que si en C++ mayor o igual a 20 igual se define !=, se usa ese != definido a parte, no la negación de ==. No me convence lo que sugiere Miguel por que no queda claro eso.
  \item \texttt{<<} para imprimir y \texttt{>>} para leer.
  \item Las expresiones \texttt{K(0)} y \texttt{K(1)} son validas.
\end{itemize}

Y como es esperable, las operaciones tienen que cumplir todos los axiomas de cuerpos.

En este trabajo se usaron dos cuerpos distintos, los números racionales, usando el tipo \texttt{mpq\_class} de la librería GMP que cumple con todos los requisitos de \texttt{K} \cite{lib:gmp} y la aritmética modular con una implementación propia en el archivo \path{extras/ModularArithmetic.hpp}, que sirve para ver un ejemplo de un cuerpo que funciona con la librería (aunque tiene algunas operaciones extra).

El tipo \texttt{X} se usa para representar las variables como números y la idea es que se use algún tipo de números sin signo. Usar otros tipos podría funcionar pero no está probado. Por defecto se usa \texttt{\_\_uint8\_t} que permite tener hasta 256 variables. Para usar más variables hay que cambiarlo a un entero sin signo más grande como \texttt{\_\_uint32\_t}.

Por último, \texttt{ord} es el orden monomial que se usa y está puesto por defecto \texttt{DegLexOrd} que es el orden lexicográfico por grado de la \cref{def:orden lexicográfico por grado} y ya está definido junto con la definición de monomios. Para usar otro orden hay que definir una estructura que tenga un operador \texttt{()} que implemente el testeo del orden. Esta es la forma estándar de definir órdenes para usar en templates en \cpp. Algo así podría ser el código si es una implementación para un \texttt{X} genérico:

\begin{minted}{C++}
  template<typename X>
  struct Orden {
    bool operator()(const Monomial<X>& a, const Monomial<X>& b) const {
      // … Implementación
    }
  };
\end{minted}

Tanto para \texttt{X} como para \texttt{ord} los valores por defecto están puestos solo en las estructuras de monomios y polinomios, porque en las otras funciones y estructuras no hace falta ya que para usarlas hay que ya tener un monomio o un polinomio (que se pase como argumento) y de ahí \cpp infiere \texttt{X} y \texttt{ord} automáticamente.
% Miguel puso: Esto puede ir antes.
% Pero no se donde podría ir antes

La librería también tiene un Makefile con el que se pueden compilar los archivos del directorio \path{mains} y correr los tests. Este Makefile puede servir de ejemplo de cómo se compila un archivo que usa la librería.

\section{Monomios}

Los monomios están definidos en el archivo \path{ncgb/nc_monomial.hpp} y lo más importante de su funcionalidad se puede resumir así:

\begin{minted}{C++}
  template<typename X = __uint8_t>
  struct Monomial {
    Monomial();
    Monomial(const std::vector<X>& vals);

    bool operator==(const Monomial& m) const;
    Monomial operator*(const Monomial& m) const;
    void operator*=(const Monomial& m);
    size_t size() const;
    bool empty() const;

    // Returns the positions of m.vals where this monomial divides m
    std::vector<size_t> divide_indexes(const Monomial& m) const;

    // Does this monomial divides m?
    bool divides(const Monomial& m) const;

    // Make all possible divisions of m by this monomial
    std::vector<std::pair<Monomial, Monomial>> divide(const Monomial& m) const;

    friend std::ostream& operator<<(std::ostream& os, const Monomial& m);
    friend std::istream& operator>>(std::istream& is, Monomial& m);

    void nice_print(std::ostream& os = std::cout) const;
    static Monomial nice_read(std::istream& is = std::cin);
  };
\end{minted}

El único argumento del template es \texttt{X}, porque no hace falta nada del cuerpo ni tampoco un orden monomial (de hecho, no tendría sentido que haga falta un orden monomial para definir monomios).

Relacionado a la división, el método más importante es \texttt{divide\_indexes} que se llama como \texttt{m0.divide\_indexes(m1)} y devuelve un vector de \texttt{size\_t} que indica las posiciones de \texttt{m1} donde empieza una sub-palabra igual a \texttt{m0}.

Respecto a la representación como strings, hay dos pares de métodos. Por un lado están \texttt{operator>>} y \texttt{operator<<} que leen y escriben en un formato numérico que es cómodo para usar en código. Por otro lado, \texttt{nice\_read} y \texttt{nice\_print} que leen y escriben en el formato que se suele usar al escribir a mano los monomios usando las variables $a$, $b$, etc.

El formato numérico consiste en un número entero no negativo $n$ seguido de $n$ números $x_1$, …, $x_n$ que son los números de variables. Cuando $n = 0$ no hay ningún $x_i$. Se puede representar así el formato:

$\begin{array}{llll} n & x_1 & ⋯ & x_n \end{array}$

Por ejemplo, acá hay un monomio en el formato como se suelen escribir a mano:

\begin{lstlisting}
  adbda
\end{lstlisting}

\noindent Y acá está el mismo monomio en el formato numérico:

\begin{lstlisting}
  5 0 3 1 3 0
\end{lstlisting}

Para declarar un monomio con \texttt{X = \_\_uint8\_t}, leerlo en formato numérico e imprimirlo en formato bonito se puede hacer así:

\begin{minted}{C++}
  Monomial m;
  std::cin >> m;
  m.nice_print();
\end{minted}
% No se porque este snippet no tiene ningún color

En el mismo archivo \path{ncgb/nc_monomial.hpp} está definido el orden lexicográfico por grado \texttt{DegLexOrd}.

\section{Polinomios}

Los polinomios están definidos en el archivo \path{ncgb/nc_polynomial.hpp} y lo más importante de su funcionalidad se puede resumir así:

\begin{minted}{C++}
  template<typename K, typename X = __uint8_t, class ord = DegLexOrd<X>>
  struct Poly {
    Poly();
    Poly(const Monomial<X>& m, K c = K(1));
    Poly(std::vector<std::pair<Monomial<X>, K>> p);

    bool operator==(const Poly& p) const;

    Poly operator+(const Poly& p) const;
    Poly operator-(const Poly& p) const;
    Poly operator*(const Poly& p) const;
    Poly operator*(Monomial<X> m) const;
    Poly operator*(K c) const;
    Poly operator/(K c) const;
    Poly operator-() const;
    void operator+=(const Poly& p);
    void operator-=(const Poly& p);
    void operator*=(const Poly& p);
    void operator*=(Monomial<X> m);
    void operator*=(K c);

    K coeff(const Monomial<X>& m) const;

    const Monomial<X>& lm() const;
    K lc() const;
    Poly lt() const;
    bool monic() const;
    bool isZero() const;

    friend std::ostream& operator<<(std::ostream& os, const Poly& p);
    friend std::istream& operator>>(std::istream& is, Poly& p);

    void nice_print(std::ostream& os = std::cout) const;
    static Poly nice_read(std::istream& is = std::cin);
  };
\end{minted}

Para construir un polinomio hay tres constructores: el constructor vacío que produce el polinomio $0$, un constructor que toma un monomio $m$ y un elemento del cuerpo $c$ y produce el polinomio $cm$, y un constructor que toma un vector de pares monomio-coeficiente y produce el polinomio que es la suma de cada coeficiente multiplicado con su monomio.

El tipo \texttt{Poly} tiene implementadas todas las operaciones entre polinomios (como la suma) con los operadores correspondientes (como \texttt{operator+} y \texttt{operator+=}, por ejemplo), algunas de las cosas definidas en la \cref{def:cosas de polinomios} (\texttt{coeff}, \texttt{lm}, \texttt{lc} y \texttt{lt}) y métodos relacionados a la representación de los polinomios como strings. % No se porque remplazar "cosas" acá

Al igual que con los monomios, para la representación como strings, hay dos pares de métodos: \texttt{operator>>} y \texttt{operator<<} para formato numérico, y \texttt{nice\_print} y \texttt{nice\_read} para formato visualmente bonito.

El formato numérico consiste en, primero, un número entero no negativo $k$ que es la cantidad de términos, seguido de la descripción de $k$ términos. La descripción de cada término consiste en primero el coeficiente y después la descripción del monomio en el formato numérico de los monomios. Se puede representar así el formato:

$\begin{array}{llllll}
  k &&&& \\
  c_1 & n_1 & x_{1, 1} & ⋯ & x_{1, n_1} \\
  ⋮ &&&& \\
  c_m & n_m & x_{k, 1} & ⋯ & x_{k, n_m}
\end{array}$

Por ejemplo, acá hay un polinomio en el formato como se suelen escribir a mano:

\begin{lstlisting}
  3 aaa - 5 bcc + adbda
\end{lstlisting}

\noindent Y acá está el mismo polinomio en el formato numérico:

\begin{lstlisting}
  3
  3 3 0 0 0
  -5 3 1 2 2
  1 5 0 3 1 3 0
\end{lstlisting}

En el archivo \path{ncgb/nc_polynomial.hpp} también está definido el orden polinomial de la \cref{def:orden polinomial} como \texttt{PolyOrd}.

Por ejemplo, para declarar un polinomio con \texttt{K = mpq\_class}, \texttt{X = \_\_uint8\_t} y \texttt{ord = DegLexOrd<X>}, leerlo en formato numérico e imprimirlo en formato bonito se puede hacer así:

\begin{minted}{C++}
  Poly<mpq_class> p;
  std::cin >> p;
  p.nice_print();
\end{minted}

\section{Buchberger y F4}\label{section:Buchberger y F4 (librería)}

Los algoritmos de Buchberger y F4 tienen una interfaz similar, así que por eso están explicados juntos.

Como ambos algoritmos tienen el problema de que pueden no terminar, no conviene hacer directamente una función que tome el conjunto generador y devuelva una base de Gröbner porque podría no terminar. Sí tiene sentido tener una función como esa para casos en los que se sabe que hay una base de Gröbner finita, pero no conviene que sea la única forma de usar los algoritmos.

Se podría hacer una función que además del conjunto generador tome un número que sea la cantidad de pasos a ejecutar y que además de devolver un conjunto devuelva si se llegó a una base de Gröbner o no. Esta opción tiene el problema de que una vez que la función retorna no se puede seguir con el cálculo, lo cual en algunos casos podría ser deseable.

Para evitar esos problemas se definió una estructura cuyo constructor toma a los polinomios y tiene un método \texttt{next} que calcula un paso más del cálculo de la base de Gröbner y en caso de haber terminado lo indica, y un método \texttt{fullBase} para usar solo en el caso de que se sepa que hay una base finita, que hace las llamadas a \texttt{next} hasta que termina y devuelve la base.

Para Buchberger la estructura dicha está en el archivo \path{ncgb/Buchberger.hpp} y se llama \texttt{BuchbergerIncremental} y para F4 la estructura está en el archivo \path{ncgb/F4.hpp} y se llama \texttt{F4Incremental}. En el caso de Buchberger el método \texttt{next} devuelve un \texttt{std::optional<Poly<K, ord>>} que es vacío solo en el caso de que ya se haya llegado a una base de Gröbner y si no tiene un polinomio de la base de Gröbner. En el caso de F4 el método \texttt{next} devuelve un \texttt{std::vector<Poly<K, ord>>} que es vacío solo en el caso de que ya se haya llegado a una base de Gröbner y si no tiene varios polinomios de la base de Gröbner. En ambos casos devolver vacío es la forma de decir que ya terminó el algoritmo. % Redacción para evitar overfull hbox

La interfaz de Buchberger podría describirse así:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  struct BuchbergerIncremental {
    BuchbergerIncremental(const std::vector<Poly<K, X, ord>>& G);
    std::optional<Poly<K, X, ord>> next();
    std::vector<Poly<K, X, ord>> fullBase();
  };
\end{minted}

Y la de F4 así:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  struct F4Incremental {
    F4Incremental(const std::vector<Poly<K, X, ord>>& G);
    std::vector<Poly<K, X, ord>> next();
    std::vector<Poly<K, X, ord>> fullBase();
  };
\end{minted}

En ambos casos el conjunto generador se toma como un vector porque usar un set de \cpp sería más costoso innecesariamente.

Para ambos algoritmos hay además una función \texttt{inIdeal} que toma un conjunto generador, un polinomio y una cantidad de pasos y trata de decir si el polinomio está en el ideal generado por el conjunto generador haciendo esa cantidad de llamadas a \texttt{next}. Esa función devuelve un elemento del siguiente tipo (con los significados que están comentados):

\begin{minted}{C++}
  enum IdealMembershipStatus {
    InIdeal,   // The element is definitely in the ideal
    NotInIdeal,// The element is definitely not in the ideal
    Unknown    // More steps needed to determine if the element is in the ideal
  };
\end{minted}

\noindent Y está declarada así:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  IdealMembershipStatus
  inIdeal(const std::vector<Poly<K, X, ord>>& G, Poly<K, X, ord> f, size_t st);
\end{minted}

Esa función básicamente trata de responder el \cref{problem:principal} de si un polinomio está en el ideal generado por un conjunto generador.

\section{Representación con cofactores}\label{section:representación con cofactores (librería)}

La función \texttt{inIdeal} solo decide la pertenencia al ideal generado, pero en caso afirmativo no da una forma de escribirlo como combinación lineal de elementos del conjunto generador con polinomios como coeficientes.

Con las bases de Gröbner pasa lo mismo, los algoritmos dan la base pero no dan una forma de escribir los elementos de la base usando los elementos del conjunto generador.

Sin embargo, esa información se puede calcular y ese cálculo está implementado para Buchberger en el mismo archivo \path{ncgb/Buchberger.hpp} en unas funciones y estructuras con nombres que terminan en \texttt{Cofactor}. Esta terminación es porque la forma de escribir un polinomio como combinación lineal de otros se llama representación con cofactores.

Estas funciones devuelven elementos del tipo \texttt{CofactorPoly} definido en el archivo \path{ncgb/nc_cofactorPolynomial.hpp}. Este tipo representa un polinomio como una combinación lineal de elementos del conjunto generador. En particular, guarda un vector de elementos $m$, $i$, $m'$, $c$ de forma que, si los $g_i$ son el conjunto generador, el polinomio representado es $∑ m g_i m' c$.

Para F4 esto no está implementado (en el siguiente capítulo se explica más del porqué).


\section{Comparación de bases de Gröbner}\label{section:Comparación de bases de Gröbner (libreria)}

En el archivo \path{ncgb/cmpBases.hpp} está la función \texttt{cmpBases} que toma dos conjuntos generadores y, asumiendo que son bases de Gröbner, dice si generan el mismo ideal o no.

\section{Paralelismo} % O paralelización?

Como ya se dijo antes, uno de los objetivos del trabajo fue paralelizar el cálculo de bases de Gröbner para que corra en varios hilos al mismo tiempo. La paralelización se hizo parcialmente para F4, o sea, se hizo solo para una parte del código, pero al ser solo parcialmente no hace casi diferencia en el tiempo, así que no sería muy útil usarla actualmente. De cualquier manera, en esta sección se explica cómo hacer que la parte del código que puede ejecutarse en paralelo lo haga.

La paralelización está hecha con OpenMP \cite{lib:openmp}, que es una librería de \cpp de paralelización, así que para que corra en paralelo es igual que en cualquier otro código paralelizado con OpenMP y a continuación se explica.

Para compilar para que se corra en paralelo hay que usar el flag \texttt{-fopenmp}. Solo con usar ese flag sigue igual corriendo con un solo hilo. Para que corra con varios hilos hay varias formas, la más fácil es incluir a OpenMP con \texttt{\#include <omp.h>} y en el main llamar a la función \texttt{omp\_set\_num\_threads} pasándole la cantidad de hilos con los que se quiere correr.


\section{Ejemplo}

Ahora un pequeño ejemplo de uso de la librería, con un ejemplo similar al del archivo \path{mains/base_Buchberger.cpp}:

\begin{minted}{C++}
  #include <bits/stdc++.h>
  #include <gmpxx.h>
  #include "ncgb/Buchberger.hpp"
  using namespace std;
  using namespace ncgb;

  typedef Poly<mpq_class> P;

  int main() {
    size_t n;
    cin >> n;
    vector<P> G(n);
    for (size_t i = 0; i < n; ++i) cin >> G[i];

    BuchbergerIncremental<P> bi(G);
    vector<P> base = bi.fullBase();

    cout << base.size() << endl;
    for (P& f : base) f.nice_print();
  }
\end{minted}

Este código, que trabaja sobre los racionales, lee un conjunto generador en formato numérico, le calcula una base de Gröbner, asumiendo que tiene una finita, y la imprime de forma bonita.


\chapter{Implementación}\label{cap:Implementación}

En este capítulo se explican los detalles de cómo está hecha la implementación de la librería.

\section{Monomios}

Los monomios, o sea los elementos de $⟨X⟩$, como son palabras se implementaron usando vectores de \texttt{X}. La base de la implementación es así:

\begin{minted}{C++}
  template<typename X = __uint8_t>
  struct Monomial {
    std::vector<X> vals;
    // … Métodos
  };
\end{minted}

Con esto, usando \texttt{X = \_\_uint8\_t}, que tiene 8 bits, se pueden tener hasta 256 variables. Cuando los monomios se imprimen o leen de forma bonita solo hay 26 variables, correspondiendo el 0 con la \texttt{a}. Si se quiere imprimir de forma bonita un monomio que usa variables mayores o iguales a 26, salta una aserción.

Esta estructura tiene implementadas las operaciones y métodos que se describieron en el capítulo anterior. La mayoría tienen una implementación directa. Las únicas no directas son las relacionadas a la división, porque chequear divisibilidad es chequear si una palabra es sub-palabra de otra, para lo cual, la forma directa de hacerlo llevaría tiempo cuadrático en el largo de las palabras, pero se puede hacer, y se hizo, en tiempo lineal.

Hay muchas formas de hacerlo en tiempo lineal, la que se usó es la función Z, que en el código está en el archivo \path{ncgb/Zfunc.hpp}. En \cite{web:cp-algo:Zfunc} se explica la función Z y cómo usarla para chequear si una palabra es sub-palabra de otra.
% También quizás tiene sentido aclarar que no es la misma que la función ζ de Riemann, que es lo primero que aparece al buscar en google "función Z"

Junto con la implementación de los monomios, en el archivo \path{ncgb/nc_monomial.hpp} está la implementación del orden lexicográfico por grado, que también es directa.

\section{Polinomios}

Los polinomios, o sea los elementos de $K⟨X⟩$, están implementados usando un vector de pares monomio-coeficiente que siempre se mantiene ordenado por el orden monomial. El esqueleto de la implementación es así:

\begin{minted}{C++}
  template<typename K, typename X = __uint8_t, class ord = DegLexOrd<X>>
  struct Poly {
    std::vector<std::pair<Monomial<X>, K>> terms;
    // … Métodos
  };
\end{minted}

En esta estructura, \texttt{terms} es el vector de pares monomio-coeficiente mencionado. Con esta estructura para los polinomios todas las operaciones se hacen de forma directa.

\section{Reducción}

La reducción está implementada en el archivo \path{ncgb/reductions.hpp}, en particular en la función \texttt{reduce} que toma un polinomio y un vector de polinomios y reduce el polinomio con los polinomios del vector. La reducción se hace modificando el propio argumento. Esa función sería una implementación de un reductor concreto, o sea, de un $e_≤$ y trata siempre de reducir primero por los primeros elementos del vector y empezando por los monomios más grandes del polinomio.

La función \texttt{reduce} tiene además una versión alternativa que además toma un vector de booleanos que tiene que ser del mismo largo que el vector de polinomios (en \cpp se puede tener varias funciones con el mismo nombre si tienen argumentos de distinto tipo) y hace la reducción solo con los polinomios que tengan un \texttt{false} en la misma posición en el vector de booleanos. Esta función está para poder implementar la optimización de ir eliminando algunos elementos de la base sin tener que estar modificando un vector.

\section{Ambigüedades}

Las ambigüedades, que son necesarias para el algoritmo de Buchberger, están implementadas en el archivo \path{ncgb/ambiguities.hpp} y consisten en una estructura así:

\begin{minted}{C++}
  template<typename X>
  struct Amb {
    const Monomial<X>& p, q;
    enum Type { Inclusion, Overlap };
    Type type;
    size_t pos; // position where q starts in p
    Monomial<X> a, b;

    size_t size() const;
    Monomial<X> lm() const;
  };
\end{minted}

Los campos de la ambigüedad son:

\begin{itemize}
  \item \texttt{p} y \texttt{q} son referencias a los monomios sobre los cuales es la ambigüedad. El motivo por el cual se guardan en la estructura es para poder implementar una de las optimizaciones.
  \item \texttt{type} indica si la ambigüedad es de inclusión o de superposición (recordar que todas las ambigüedades que se usan en el algoritmo de Buchberger son relevantes).
  \item \texttt{pos} es la posición de \texttt{p} donde empieza el pedazo que es en común con \texttt{q} y por la cual existe la ambigüedad.
  \item \texttt{a} y \texttt{b} son los monomios que hacen que en el caso de las de inclusión \texttt{p} sea igual a \texttt{aqb} y en el caso de las de superposición \texttt{ap} sea igual a \texttt{qb}. % El producto acá es medio raro, pero no se como hacerlo mejor
\end{itemize}

En el archivo también está la función \texttt{ambiguities} que toma dos monomios y devuelve un vector de \texttt{Amb} con todas las ambigüedades entre esos dos monomios. En esta función, al igual que en la divisibilidad de monomios, se usa la función Z para evitar tener que hacer algo cuadrático en los largos de los monomios.

Después, en el archivo está la función \texttt{S\_poly} que toma una ambigüedad y dos polinomios que deberían ser los polinomios correspondientes y devuelve el S-polinomio correspondiente.

Y por último está la función \texttt{checkDeletionCriteria} que implementa la optimización que permite descartar algunas ambigüedades sin tener que reducirlas.

\section{Buchberger}

Como ya se dijo en la \cref{section:Buchberger y F4 (librería)}, por el inconveniente de que el algoritmo puede no terminar, se usa una estructura con un método \texttt{next}. En esa estructura ya están implementadas las optimizaciones antes mencionadas, pero en esta sección primero se explica cómo sería la estructura sin esas optimizaciones y después en la sección \cref{secton:optimizaciones} se explica algo de cómo se agregan.

La base de esa estructura es así:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  struct BuchbergerIncremental {
    std::vector<Poly<K, X, ord>> G;
    std::queue<std::tuple<Amb<X>, size_t, size_t>> ambs;
    size_t t = 0;
    // … Métodos
  };
\end{minted}

Los campos de esta estructura guardan lo siguiente:

\begin{itemize}
  \item \texttt{G} es la base de Gröbner que se está construyendo. Al principio se inicializa con los polinomios con los que se llama al constructor.
  \item \texttt{ambs} son las ambigüedades que todavía no se procesaron junto con los índices en \texttt{G} de los polinomios a los que corresponde la ambigüedad. Se usa una cola para procesar siempre la que hace más tiempo está esperando.
  \item La variable \texttt{t} está porque como la base de Gröbner incluye a los polinomios originales, las primeras llamadas a \texttt{next} tienen que devolver esos polinomios y \texttt{t} indica cuántos de esos ya se devolvieron. Cuando \texttt{t} es igual al tamaño de \texttt{G} es porque ya se devolvieron todos esos.
\end{itemize}

La estructura además de tener los métodos \texttt{next} y \texttt{fullBase} tiene varios métodos que se usan internamente. Entre todos los métodos son los siguientes: % Por todos los métodos me refiero a los ya mencionados y a los todavía no mencionados

\begin{itemize}
  \item \texttt{add\_amb}: Este método toma una ambigüedad y dos índices, que son los índices en \texttt{G} de los polinomios a los que corresponde la ambigüedad, y los agrega a la cola de ambigüedades. Siempre que hay que agregar una ambigüedad se hace con este método. El motivo por el cual esto está en un método propio es para después, al implementar la optimización de descartar ambigüedades, poder hacerlo solo en este método.
  \item \texttt{add\_poly}: Este método agrega un polinomio a \texttt{G} agregando además todas las nuevas ambigüedades que aparecen (llamando a \texttt{add\_amb}). Cada vez que hay que agregar un nuevo polinomio se usa este método, tanto al comienzo cuando se agregan los polinomios iniciales como cuando se agrega un S-polinomio reducido.
  \item \texttt{next}: Este es el método que corre el algoritmo. Lo principal que hace (después de usar la variable \texttt{t} para ver si tiene que devolver un elemento de \texttt{G}) es sacar ambigüedades de la cola hasta encontrar una que no se reduzca a $0$ y cuando la encuentra la agrega a la base (con \texttt{add\_poly}) y la devuelve. Si se acaban las ambigüedades devuelve vacío (recordar que devuelve \texttt{std::optional<Poly<K, ord>>}).
  \item \texttt{fullBase}: Simplemente llama a \texttt{next} en un ciclo hasta que devuelva vacío y después devuelve la base.
\end{itemize}

Lo de las optimizaciones se explica más adelante en la \cref{secton:optimizaciones}.


\section{F4}

F4 es un poco más complejo que Buchberger y tiene más partes, así que esta sección está dividida en varias subsecciones para las distintas partes.

\subsection{Reducción por filas de matrices}

Para F4 hace falta hacer la reducción por filas de una matriz. Para esto, en el archivo \path{ncgb/matrix.cpp} se definió un tipo matriz como un vector de vectores y una función \texttt{rref} que hace una eliminación gaussiana directamente y que funciona para cualquier \texttt{K}. Sin embargo, la reducción por filas de una matriz es un tema ya muy analizado y hay algunas librerías que lo hacen para distintos tipos \texttt{K} mucho más rápido.

Una librería que hace la reducción por filas es FLINT, que lo hace para el tipo \texttt{mpq\_class} de GMP que ya dijimos que son los números racionales \cite{lib:flint, lib:gmp}. Para usarla, en el archivo \path{ncgb/matrix_mpq_class.hpp} hay una especialización de \texttt{rref} para \texttt{mpq\_class} (en \cpp una especialización es cuando hay código definido con un template y se hace una definición aparte para alguna combinación particular de parámetros del template).

Algo malo que tiene esa especialización es que las matrices de racionales con las que trabaja FLINT no están definidas como vectores de vectores, sino que son su propio tipo \texttt{fmpq\_mat\_t}, así que la función tiene que primero copiar la matriz a una matriz de FLINT, después hacer la reducción por filas y por último copiar el resultado de vuelta a la matriz original.

El motivo por el cual se hizo eso es que esas matrices de FLINT se usan de una forma muy particular y complicada. Por ejemplo, para asignarle un valor \texttt{x} de tipo \texttt{mpq\_class} a una posición de la matriz hay que hacer \texttt{fmpq\_set\_mpq(fmpq\_mat\_entry(mat, i, j), x.get\_mpq\_t(\allowbreak));}. Esto hace que si se quisiera directamente en F4 trabajar con las matrices de FLINT no se podría hacer el código genérico para cualquier \texttt{K}. Se probó también hacer que las matrices sean una estructura propia y hacer una especialización de \texttt{Matrix<mpq\_class>} para que use \texttt{fmpq\_mat\_t} por dentro, pero andaba más lento, así que se descartó. % Tube que poner \allowbreak porque si no se pasaba del margen, pero no se si es la mejor forma de hacerlo

\subsection{Preprocesamiento simbólico}

El preprocesamiento simbólico (\cref{alg:Preprocesamiento simbólico}) está implementado en el propio archivo \path{ncgb/F4.hpp} en la función \texttt{symbolicPreprocessing}. La implementación es directa, así que no hay mucho para comentar.

\subsection{Reducción}

Como en F4 se reducen varios polinomios al mismo tiempo, se implementó una función \texttt{multiReduction} que toma dos vectores de polinomios, la base actual y los polinomios a reducir y los reduce todos a la vez.

En el \cref{alg:F4} esto se hizo dentro del propio algoritmo de F4 pero en la implementación se hizo aparte porque es bastante lo que tiene que hacer el código.

La implementación lo que hace es construir la matriz con una función llamada \texttt{toMatrix}, después llamar a la función \texttt{rref} que ya se explicó, después marcar qué columnas corresponden a monomios principales del vector por el cual se está reduciendo (esto para poder saber cuáles polinomios son los reducidos) y por último convertir a polinomios las filas que su coeficiente principal (el primero no nulo de izquierda a derecha) no está marcada como que es un monomio principal del vector por el cual se está reduciendo.

\subsection{El propio F4}

La primera diferencia tiene que ver con la selección de ambigüedades. En Buchberger solo había que elegir una ambigüedad, pero acá hay que elegir varias. Podría haber muchas formas de elegir, pero, como ya se dijo antes, una estrategia que funciona bien es elegir siempre todas las de menor grado. Para eso se hizo que las ambigüedades estén guardadas por grado en un vector, así:

\begin{minted}{C++}
  std::vector<std::vector<std::tuple<Amb<X>, size_t, size_t>>> ambs_per_deg;
\end{minted}

El otro lugar en el que hay diferencia, y mucha, es en \texttt{next}. En F4 este método lo que hace es, agarrar todas las ambigüedades de menor grado, poner todos los S-polinomios correspondientes en un vector, hacer la reducción de esos polinomios con la función \texttt{multiReduction} y, si hay alguno que queda no nulo, agregar los polinomios reducidos a la base, agregando también las nuevas ambigüedades que aparecen y devolver esos nuevos polinomios, y si quedan todos nulos pasar a las ambigüedades de grado siguiente. Si no quedan más grados de ambigüedades se devuelve vacío.

\section{Optimizaciones}\label{secton:optimizaciones}

Como ya se dijo varias veces, hay algunas optimizaciones que se pueden hacer en los algoritmos de Buchberger y F4, que están explicadas en la Sección 4.5 de \cite{thesis:Hof20}. Acá solo se explican superficialmente las optimizaciones y se explican cómo están implementadas.

\subsection{Descartar ambigüedades}

La primera optimización consiste en que se pueden descartar algunas ambigüedades, sin tener que reducir su S-polinomio, según si se cumplen ciertas propiedades con otros polinomios que ya están en la base.

En particular algunos teoremas de \cite{thesis:Hof20} permiten definir una función con la siguiente signatura:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  bool
  checkDeletionCriteria(
    std::vector<Poly<K, X, ord>>& G, Amb<X>& amb, size_t i, size_t j);
\end{minted}

Que toma la base, la ambigüedad y a que polinomios corresponde y devuelve \texttt{true} si y solo si esa ambigüedad se puede descartar sin reducirla.

Usando esa función, en el método \texttt{add\_amb} tanto de Buchberger como de F4, antes de agregar la ambigüedad, se agregó una llamada a \texttt{checkDeletionCriteria} para solo agregar la ambigüedad si da \texttt{false}.

\subsection{Eliminación de polinomios}

La segunda optimización consiste en que cuando se procesa una ambigüedad de inclusión se puede borrar de la base al polinomio grande de la ambigüedad.

Para recordar, una ambigüedad de inclusión entre los polinomio $f$ y $g$ es cuando se tiene que $\lm_≤(f) = a \lm_≤(g) b$ con $a, b ∈ ⟨X⟩$. Esta optimización permite en esas ambigüedades borrar $f$ de la base.

Esta optimización se implementó en Buchberger pero no en F4, porque por algún motivo cuando se probó en F4 anduvo más lento. No se investigó por qué pasó eso.

Para implementar esta optimización en Buchberger lo que se hizo no es directamente borrar el polinomio de \texttt{G}, porque para eso habría que también cambiar los índices que están guardados junto con las ambigüedades, sino que se agregó un vector de booleanos \texttt{removed} a la estructura, que tiene siempre el mismo largo que \texttt{G} y vale \texttt{true} solo en las posiciones de polinomios que se borraron. O sea que más bien se está marcando como borrados los polinomios.

A las funciones que se habían explicado, que toman a \texttt{G} como parámetro y se usan en Buchberger, se les agregó un parámetro extra que es el vector de booleanos y se hizo que solo trabajen con los polinomios de posiciones que no están en \texttt{true}.

\subsection{Reducción más eficiente en F4}\label{subsection:Reducción más eficiente en F4}

Como ya se dijo antes, para F4 también hay otra optimización que consiste en hacer la reducción por filas de la matriz de forma más eficiente aprovechando la estructura particular que tienen las matrices de F4. Esta reducción se llama reducción Faugère-Lachartre y está explicada en \cite{thesis:Hof20}.

Esta optimización no se implementó principalmente por el poco soporte para trabajar con matrices de un cuerpo arbitrario, o de una implementación específica de $ℚ$, que hay en \cpp. Sin embargo, queda como trabajo futuro en la \cref{section:trabajos futuros}.

\section{Representación con cofactores}

Como se dijo en el capítulo anterior, la representación con cofactores se implementó solo para Buchberger, y no para F4. El motivo es que, como para F4 de cualquier manera no está implementada la optimización de hacer la reducción más eficiente, se hay que seguir trabajando en la parte de la reducción y cuando se tenga lista la versión más eficiente es el momento de implementar la representación con cofactores.

Como se explicó en la \cref{section:representación con cofactores (librería)}, para la representación con cofactores se usa el tipo \texttt{CofactorPoly} que guarda polinomios de un ideal como combinación lineal de elementos de la base.

La estructura de la implementación de este tipo es así:

\begin{minted}{C++}
  template<typename K, typename X = __uint8_t, class ord = DegLexOrd<X>>
  struct CofactorPoly {
    std::vector<std::tuple<Monomial<X>, size_t, Monomial<X>, K>> terms;
    // … Métodos
  };
\end{minted}

Y tiene definidas las operaciones de suma, resta y producto entre polinomios, al igual que con los polinomios normales. Además, tiene un método \texttt{add} para agregar una tupla monomio, índice, monomio, coeficiente al polinomio.

A diferencia de los polinomios comunes, acá \texttt{terms} no se mantiene ordenado de ninguna manera.

Con \texttt{CofactorPoly} se hizo la estructura \texttt{BuchbergerIncrementalCofactor}, que es muy parecida a \texttt{BuchbergerIncremental} pero con algunos agregados, y se hicieron versiones \texttt{Cofactor} de las funciones que se usan.

Para las reducciones se implementó una función que toma, por referencia, un polinomio $f$, reduce in place a su forma normal $f^*$ y devuelve la diferencia $f - f^*$ como un \texttt{CofactorPoly}. La signatura es:

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  CofactorPoly<K, X, ord>
  reduceCofactor(
    Poly<K, X, ord>& f, const std::vector<Poly<K, X, ord>>& G,
    const std::vector<CofactorPoly<K, X, ord>>& g_rec);
\end{minted}

También se hizo una versión de la función de los S-polinomios, que toma la ambigüedad y los polinomios correspondientes $f$ y $g$ y devuelve, además del S-polinomio $p$, los monomios $a$, $b$, $a'$ y $b'$ y los elementos del cuerpo $c$ y $c'$, que hacen que $p = c a f b + c' a' f b'$.

\begin{minted}{C++}
  template<typename K, typename X, class ord>
  std::tuple<
    Poly<K, X, ord>,
    std::tuple<Monomial<X>, Monomial<X>, K>,
    std::tuple<Monomial<X>, Monomial<X>, K>>
  S_polyCofactor(
    const Amb<X>& amb, const Poly<K, X, ord>& f, const Poly<K, X, ord>& g);
\end{minted}

Con esto, en la estructura \texttt{BuchbergerIncrementalCofactor} se agregó (con respecto a \texttt{BuchbergerIncremental}) un campo \texttt{std::vector<CofactorPoly<K, ord>> G\_rec} que guarda la representación con cofactores de cada polinomio de \texttt{G}.

En este campo \texttt{G\_rec}, a los elementos iniciales simplemente se los agrega como \texttt{{Monomial(), i, Monomial(), K(1)}} y a los elementos que vienen de un S-polinomio se les construye el \texttt{CofactorPoly} con los \texttt{CofactorPoly} de los polinomios que forman la ambigüedad multiplicados por los monomios y coeficientes que devuelve \texttt{S\_polyCofactor} y con el \texttt{CofactorPoly} de la reducción que devuelve la función \texttt{reduceCofactor}.

\section{Comparación de bases de Gröbner}

La función \texttt{cmpBases} que se mencionó en la \cref{section:Comparación de bases de Gröbner (libreria)} que toma dos conjuntos generadores y, asumiendo que son bases de Gröbner, dice si generan el mismo ideal o no está implementada usando tal cual el \cref{col:(G) = (G') cond}.

\section{Paralelismo}

Cuando se paraleliza siempre es importante primero tratar de optimizar la versión no paralela lo más posible. En este caso lo mejor no paralelo es el algoritmo F4, pero como no está implementada la optimización de la reducción eficiente, no está todavía optimizado al máximo. De cualquier manera, como la reducción de matrices es solo una parte del algoritmo, se puede pensar cómo se podría paralelizar el resto.

El método \texttt{add\_poly} que ya se explicó que agrega un polinomio a la base y las nuevas ambigüedades correspondientes, sin paralelizar se implementó así:

\begin{minted}{C++}
  void add_poly(const Poly<K, X, ord>& f) {
    G.push_back(f);
    for (size_t k = 0; k < G.size() - 1; k++) {
      for (auto& amb : ambiguities(G[k].lm(), f.lm())) {
        add_amb(amb, k, G.size() - 1);
      }
      for (auto& amb : ambiguities(f.lm(), G[k].lm())) {
        add_amb(amb, G.size() - 1, k);
      }
    }
  }
\end{minted}

Y el método \texttt{add\_amb} se implementó así:

\begin{minted}{C++}
  void add_amb(Amb<X>& amb, size_t i, size_t j) {
    if (checkDeletionCriteria(G, amb, i, j)) {
      return;
    }
    ambs.push({amb, i, j});
  }
\end{minted}

El primer \texttt{for} de \texttt{add\_poly} así como está casi que se podría paralelizar, corriendo sus distintas iteraciones en paralelo. Lo único que lo impide es que \texttt{add\_amb} hace un \texttt{ambs.push} que sería problemático si se llega a ejecutar al mismo tiempo por más de un hilo. Para solucionar eso lo que se hizo es guardar las ambigüedades en varios vectores, uno por cada polinomio de \texttt{G} y después al final de la función agregar todas las ambigüedades a \texttt{ambs}. Haciendo eso, el código paralelizado quedó así:

\begin{minted}{C++}
  void add_poly(Poly<K, X, ord>& f) {
    G.push_back(f);
    size_t lim = G.size() - 1;
    std::vector<std::vector<std::tuple<Amb<X>, size_t, size_t>>> to_add(lim);

    #pragma omp parallel for
    for (size_t k = 0; k < lim; k++) {
      for (auto& amb : ambiguities(G[k].lm(), G.back().lm())) {
        if (!checkDeletionCriteria(G, amb, k, lim)) {
          to_add[k].push_back({amb, k, lim});
        }
      }
      for (auto& amb : ambiguities(G.back().lm(), G[k].lm())) {
        if (!checkDeletionCriteria(G, amb, lim, k)) {
          to_add[k].push_back({amb, lim, k});
        }
      }
    }

    for (size_t k = 0; k < lim; k++) {
      for (auto& [amb, i, j] : to_add[k]) {
        size_t d = amb.size();
        while (ambs_per_deg.size() <= d) {
          ambs_per_deg.push_back({});
        }
        ambs_per_deg[d].push_back({amb, i, j});
      }
    }
  }
\end{minted}

El \texttt{\#pragma omp parallel for} es lo que efectivamente hace que OpenMP paralelice el \texttt{for}. En el \cref{cap:Benchmarks} se presentan resultados sobre qué tal anda la paralelización.


\chapter{Tests}\label{cap:Tests}

Los tests suelen ser una parte fundamental en la implementación de una librería robusta y confiable, así que en este trabajo se testeó todo lo más posible. A medida que se fue escribiendo el código se fueron haciendo tests del código para eliminar los bugs. Además, con algunos de los tests se hizo que realicen mediciones de tiempo para ver qué tan rápido andaba el código. Esto último se llama benchmarking. En esta sección se explican los tests y en el siguiente capítulo los benchmarks que se hicieron.

Todos los tests están en el directorio \path{test}, el cual está dividido en varios subdirectorios con varios tests cada uno. Para correr todos los tests se puede usar el comando \texttt{make test}. En el directorio \path{.github/workflows} está configurado para que al subir los cambios al repositorio se corran los tests automáticamente, aunque eso no siempre funciona porque cuando se corren los tests en GitHub se tiene que instalar SageMath y a veces falla la instalación.

Algunos tests corren el código de la librería y el código de \texttt{operator\_gb}, que es la librería hecha por Clemens Hofstadler en su tesis de máster \cite{thesis:Hof20} usando SageMath, y comparan los resultados, y otros corren solo código de la librería.

También, algunos tests corren los mains de ejemplo del directorio \path{mains} y otros directamente incluyen a los archivos de la librería y los testean directamente.

A continuación una lista de lo más importante de los tests:

\begin{itemize}
  \item Hay tests de las operaciones básicas de monomios, polinomios, reducción, etc. en el directorio \path{test/internal_tests}.
  \item Para algunos conjuntos generadores que se sabe que sus ideales generados tienen bases de Gröbner finitas hay un test que calcula una base de Gröbner con Buchberger, una con F4 y una con \texttt{operator\_gb} y chequea que los resultados sean equivalentes usando \path{mains/compare_bases.cpp}. Este archivo lo que hace es leer dos conjuntos generadores y, asumiendo que son bases de Gröbner, dice si generan el mismo ideal o no usando la función \texttt{cmpBases} explicada en la \cref{section:Comparación de bases de Gröbner (libreria)}. Este test está en el directorio \path{test/base_tests}.
  \item Para algunos conjuntos generadores construidos al azar se construye otro polinomio también al azar y tanto con Buchberger como con F4 y con \texttt{operator\_gb} se ejecutan algunos pasos del cálculo de la base de Gröbner para ver si está el polinomio en el ideal o no. Si alguno(s) de los tres da que sí está en el ideal y otro(s) no, se ejecutan los que no con muchísimos más pasos, para que tenga que terminar sí o sí porque por los que dieron que sí sabemos que (si no hay bugs) sí está. Esto está en el directorio \path{test/InIdeal_tests}. % Esto me parece que no se entiende nada, pero no se como explicarlo.
  \item Con respecto a la representación con cofactores, hay un test que corre los algoritmos con y sin representación con cofactores y se fija que el resultado de con cofactores después de convertir los \texttt{CofactorPoly} en \texttt{Poly} sea igual al resultado sin cofactores. Este test está en el directorio \path{test/cofactor_tests}.
  \item Por último hay un test que corre F4 no paralelo y paralelo con distintas cantidades de hilos y se fija que siempre dé el mismo resultado. Esto está en el directorio \path{test/parallelism_tests}.
\end{itemize}

\chapter{Benchmarks}\label{cap:Benchmarks}

En este capítulo se presentan los resultados obtenidos tras correr los tests que incluyen benchmarks. Las corridas se hicieron en la computadora `atom' de FAMAF que tiene las siguientes características:

\begin{itemize}
  \item Procesador: AMD EPYC 7643 48-Core Processor
  \item RAM: 126 GB
  \item Sistema Operativo: Debian GNU/Linux trixie/sid
  \item Versión de \texttt{g++}: 14.2.0-12
\end{itemize}

De cualquier manera, esos datos no son muy importantes porque lo importante son las relaciones entre los distintos tiempos.

Los tests que tienen benchmarks son, todos salvo uno, tests que usan los archivos del directorio \path{mains} y archivos ejecutables hechos en Python con \texttt{operator\_gb} y son corridos con distintas entradas desde un archivo de Python. Los tiempos en todos salvo uno se miden desde el programa de Python, así que incluyen el tiempo hasta que se inicializa el programa y lee la entrada y el tiempo de impresión de la salida, pero como son siempre entradas y salidas relativamente chicas, eso no debería hacer casi diferencia. En el test en el que los tiempos no se miden desde Python, se miden desde el propio C++.

Todos los tests que tienen benchmarks, salvo uno (que más adelante se explica), funcionan tomando como input un conjunto generador que se sabe que el ideal generado tiene una base de Gröbner finita. Para estos benchmarks se usaron los siguientes conjuntos generadores:

\begin{itemize}
  \item FK2 $ = \{a^2\}$
  \item FK3 $ = \{a^2,\ b^2,\ c^2,\ ac + ba + cb,\ ab + bc + ca\}$
  \item FK4 $ = \{a^2,\ b^2,\ c^2,\ d^2,\ e^2,\ f^2,\ ac + ba + cb,\ ae + da + ed,\ bf + db + fd,\ cf + ec + fe,\ ab + bc + ca,\ ad + de + ea,\ bd + df + fb,\ ce + ef + fc,\ cd + dc,\ be + eb,\ af + fa\}$
  \item tri1 $ = \{a^2 - 1,\ b^3 - 1,\ {(ababab^2ab^2)}^2 - 1\}$
  \item tri2 $ = \{a^2 - 1,\ b^3 - 1,\ {(ababab^2)}^3 - 1\}$
  \item tri3 $ = \{a^3 - 1,\ b^3 - 1,\ {(abab^2)}^2 - 1\}$
  \item tri4 $ = \{a^3 - 1,\ b^3 - 1,\ {(abaab^2)}^2 - 1\}$
  \item tri5 $ = \{a^2 - 1,\ b^5 - 1,\ {(abab^2)}^2 - 1\}$
  \item tri6 $ = \{a^2 - 1,\ b^5 - 1,\ {(ababab^4)}^2 - 1\}$
  \item tri7 $ = \{a^2 - 1,\ b^5 - 1,\ {(abab^2ab^4)}^2 - 1\}$
  \item tri8 $ = \{a^2 - 1,\ b^2 - 1,\ {(ababab^3)}^2 - 1\}$
  \item tri9 $ = \{a^2 - 1,\ b^3 - 1,\ {(abab^2)}^2 - 1\}$
  \item tri10 $ = \{a^2 - 1,\ b^3 - 1,\ {(ababab^2)}^2 - 1\}$
  \item tri11 $ = \{a^2 - 1,\ b^3 - 1,\ {(abababab^2)}^2 - 1\}$
  \item tri12 $ = \{a^2 - 1,\ b^3 - 1,\ {(ababab^2abab^2)}^2 - 1\}$
  \item tri13 $ = \{a^2 - 1,\ b^3 - 1,\ {(babababab^2ab^2)}^2 - 1\}$
  \item trit3 $ = \{a^3 - 1,\ b^3 - 1,\ c^3 - 1,\ {(ab)}^2 - 1,\ {(ac)}^2 - 1,\ {(bc)}^2 - 1\}$
  \item trit4 $ = \{a^3 - 1,\ b^3 - 1,\ c^4 - 1,\ {(ab)}^2 - 1,\ {(ac)}^2 - 1,\ {(bc)}^2 - 1\}$
  \item trit5 $ = \{a^3 - 1,\ b^3 - 1,\ c^5 - 1,\ {(ab)}^2 - 1,\ {(ac)}^2 - 1,\ {(bc)}^2 - 1\}$
\end{itemize}

Los FK provienen de la llamada teoría de álgebras de Nichols \cite{book:introNichols}, la cual es de interés para Cristian Vay, el director de este trabajo, y otros investigadores de FAMAF. Los FK definen una álgebra llamada Fomin-Kirillov y son una familia infinita parametrizada por un $n ∈ ℕ$, que fijando el $n$ se describe a continuación.

\begin{itemize}
  \item El cuerpo es $ℚ$.
  \item El alfabeto es $\{x_{ij} : i, j ∈ \{1, …, n\} : i ≠ j\}$, donde los $_{ij}$ son un par no ordenado.
  \item El conjunto generador es:
  \begin{align*}
    & \{ x_{ij}² : i, j ∈ \{1, …, n\} : i ≠ j \} \\
    & ∪ \{ x_{ij} x_{ik} + x_{ik} x_{jk} + x_{jk} x_{ij} : i, j, k ∈ \{1, …, n\} : i ≠ j ≠ k ≠ i \} \\
    & ∪ \{ x_{ik} x_{ij} + x_{jk} x_{ik} + x_{ij} x_{jk} : i, j, k ∈ \{1, …, n\} : i ≠ j ≠ k ≠ i \} \\
    & ∪ \{ x_{ij} x_{kl} + x_{kl} x_{ij} : i, j, k, l ∈ \{1, …, n\} : i ≠ j ∧ k ≠ l \} \text{.}
  \end{align*}
\end{itemize}

\noindent Se sabe que para $n ≤ 5$ los ideales generados por FK$n$ tienen bases de Gröbner finitas, pero para $n > 5$ no se sabe. En \cite{web:Nichols} y \cite{book:introNichols} se puede encontrar más información sobre las álgebras de Nichols.

Los tri están porque \cite{thesis:Hof20} los usa, con ese mismo nombre, y él los sacó del Teorema 2.12 de \cite{book:class-tri}. Los trit son directamente tomados de la Proposición 1.9 de \cite{book:class-tri}.

En todos los benchmarks que usan estos conjuntos se trabaja sobre los racionales, porque es para los racionales que se sabe que tienen bases finitas.

En las siguientes secciones se presentan los resultados de cada uno de los benchmarks.

\section{Ideales con bases de Gröbner finitas}\label{section:Ideales con bases de Gröbner finitas}

El test que usa los distintos algoritmos para calcular bases de Gröbner de ideales que se sabe que tienen bases de Gröbner finitas incluye mediciones de los tiempos y usa los casos que vimos recién.

En el siguiente gráfico se muestran los tiempos de ejecución de Buchberger de este trabajo (\texttt{Buchberger}), de F4 de este trabajo (\texttt{F4}) y de \texttt{operator\_gb} (que usa F4) para esos casos.

\noindent \begin{tikzpicture}
  \begin{axis}[
      ybar = 0.5pt, % Tipo de gráfico: barras verticales
      width = \textwidth,
      height = 0.4\textwidth,
      bar width = 0.01\textwidth, % Ancho de cada barra
      symbolic x coords = {FK2, FK3, FK4, tri1, tri2, tri3, tri4, tri5, tri6, tri7, tri8, tri9, tri10, tri11, tri12, tri13, trit3, trit4, trit5}, % Etiquetas en el eje x
      xtick = data, % Mostrar marcas en el eje x para cada dato
      ylabel = {Tiempo en segundos}, % Etiqueta del eje y
      %xlabel = {Categorías}, % Etiqueta del eje x
      xticklabel style = {rotate = 45}, % Rotar etiquetas
      ymin = 0, % Mínimo del eje y
      ymax = 215,
      enlarge x limits = 0.03, % Espaciado en los bordes
      legend style = {at = {(0.024, 0.94)}, anchor = north west}, % Estilo de la leyenda
      legend entries = {\texttt{Buchberger}, \texttt{F4}, \texttt{operator\_gb}}, % Entradas de la leyenda
      cycle list = {{brown,fill = brown!30}, {red,fill = red!30}, {blue,fill = blue!30}}, % Colores
  ]
    % Datos para cada conjunto
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.01) (tri1, 0.26) (tri10, 0.01) (tri11, 0.03) (tri12, 3.27) (tri13, 0.01) (tri2, 6.01) (tri3, 0.02) (tri4, 0.04) (tri5, 0.01) (tri6, 6.53) (tri7, 33.33) (tri8, 0.09) (tri9, 0.00) (trit3, 0.01) (trit4, 0.14) (trit5, 77.16)};
    \addplot coordinates {(FK2, 0.01) (FK3, 0.01) (FK4, 0.08) (tri1, 0.08) (tri10, 0.01) (tri11, 0.02) (tri12, 0.21) (tri13, 0.01) (tri2, 0.29) (tri3, 0.04) (tri4, 0.07) (tri5, 0.02) (tri6, 1.78) (tri7, 1.81) (tri8, 0.06) (tri9, 0.01) (trit3, 0.05) (trit4, 0.49) (trit5, 207.84)};
    \addplot coordinates {(FK2, 1.70) (FK3, 0.80) (FK4, 0.87) (tri1, 0.98) (tri10, 0.85) (tri11, 0.88) (tri12, 1.04) (tri13, 0.83) (tri2, 1.16) (tri3, 0.90) (tri4, 0.94) (tri5, 0.87) (tri6, 1.27) (tri7, 1.45) (tri8, 0.91) (tri9, 0.82) (trit3, 0.87) (trit4, 1.04) (trit5, 5.21)};
  \end{axis}
\end{tikzpicture}

A simple vista se puede ver que para los casos en los que se distingue algo \texttt{operator\_gb} es mucho más rápido, pero solo se ven los casos que demoran mucho tiempo, los otros no. Así que a continuación está el gráfico de vuelta pero solo con la parte de más abajo y dejando que las barras muy largas se salgan del gráfico.

\noindent \begin{tikzpicture}
  \begin{axis}[
      ybar = 0.5pt, % Tipo de gráfico: barras verticales
      width = \textwidth,
      height = 0.4\textwidth,
      bar width = 0.01\textwidth, % Ancho de cada barra
      symbolic x coords = {FK2, FK3, FK4, tri1, tri2, tri3, tri4, tri5, tri6, tri7, tri8, tri9, tri10, tri11, tri12, tri13, trit3, trit4, trit5}, % Etiquetas en el eje x
      xtick = data, % Mostrar marcas en el eje x para cada dato
      ylabel = {Tiempo en segundos}, % Etiqueta del eje y
      %xlabel = {Categorías}, % Etiqueta del eje x
      xticklabel style = {rotate = 45}, % Rotar etiquetas
      ymin = 0, % Mínimo del eje y
      ymax = 10,
      enlarge x limits = 0.03, % Espaciado en los bordes
      legend style = {at = {(0.024, 0.94)}, anchor = north west}, % Estilo de la leyenda
      legend entries = {\texttt{Buchberger}, \texttt{F4}, \texttt{operator\_gb}}, % Entradas de la leyenda
      cycle list = {{brown,fill = brown!30}, {red,fill=red!30}, {blue,fill=blue!30}}, % Colores
  ]
    % Datos para cada conjunto
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.01) (tri1, 0.26) (tri10, 0.01) (tri11, 0.03) (tri12, 3.27) (tri13, 0.01) (tri2, 6.01) (tri3, 0.02) (tri4, 0.04) (tri5, 0.01) (tri6, 6.53) (tri7, 33.33) (tri8, 0.09) (tri9, 0.00) (trit3, 0.01) (trit4, 0.14) (trit5, 77.16)};
    \addplot coordinates {(FK2, 0.01) (FK3, 0.01) (FK4, 0.08) (tri1, 0.08) (tri10, 0.01) (tri11, 0.02) (tri12, 0.21) (tri13, 0.01) (tri2, 0.29) (tri3, 0.04) (tri4, 0.07) (tri5, 0.02) (tri6, 1.78) (tri7, 1.81) (tri8, 0.06) (tri9, 0.01) (trit3, 0.05) (trit4, 0.49) (trit5, 207.84)};
    \addplot coordinates {(FK2, 1.70) (FK3, 0.80) (FK4, 0.87) (tri1, 0.98) (tri10, 0.85) (tri11, 0.88) (tri12, 1.04) (tri13, 0.83) (tri2, 1.16) (tri3, 0.90) (tri4, 0.94) (tri5, 0.87) (tri6, 1.27) (tri7, 1.45) (tri8, 0.91) (tri9, 0.82) (trit3, 0.87) (trit4, 1.04) (trit5, 5.21)};
  \end{axis}
\end{tikzpicture}

Acá se puede ver que en los casos más chiquitos sí pasa que \texttt{Buchberger} y \texttt{F4} le ganan a \texttt{operator\_gb}. Que en los grandes gane \texttt{operator\_gb} tiene sentido por el hecho de que tiene implementada la optimización de reducir las matrices mas eficientemente. Que en los chicos \texttt{Buchberger} y \texttt{F4} sean más rápido también tiene sentido por el hecho de que \cpp es mucho más rápido que Python.

\section{Paralelismo}

El test que verifica que de el mismo resultado correr con distintas cantidades de hilos también usa los mismos casos y mide los tiempos. En el siguiente gráfico se pueden ver los resultados.

\noindent \begin{tikzpicture}
  \begin{axis}[
      ybar = 0.5pt, % Tipo de gráfico: barras verticales
      width = \textwidth,
      height = 0.4\textwidth,
      bar width = 0.003\textwidth,
      symbolic x coords = {FK2, FK3, FK4, tri1, tri2, tri3, tri4, tri5, tri6, tri7, tri8, tri9, tri10, tri11, tri12, tri13, trit3, trit4, trit5}, % Etiquetas en el eje x
      xtick = data, % Mostrar marcas en el eje x para cada dato
      ylabel = {Tiempo en segundos}, % Etiqueta del eje y
      %xlabel = {Cantidad de hilos}, % Etiqueta del eje x
      xticklabel style = {rotate = 45}, % Rotar etiquetas
      ymin = 0, % Mínimo del eje y
      ymax = 215, % Máximo del eje y
      enlarge x limits = 0.03, % Espaciado en los bordes
      legend style = {at = {(0.024, 0.94)}, anchor = north west}, % Estilo de la leyenda
      legend entries = {1, 2, 4, 6, 8, 16}, % Entradas de la leyenda
  ]
    % Datos para cada conjunto
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.07) (tri1, 0.10) (tri10, 0.01) (tri11, 0.02) (tri12, 0.32) (tri13, 0.01) (tri2, 0.53) (tri3, 0.03) (tri4, 0.07) (tri5, 0.01) (tri6, 2.20) (tri7, 2.19) (tri8, 0.07) (tri9, 0.00) (trit3, 0.04) (trit4, 0.47) (trit5, 213.58)};
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.06) (tri1, 0.08) (tri10, 0.01) (tri11, 0.02) (tri12, 0.28) (tri13, 0.00) (tri2, 0.42) (tri3, 0.03) (tri4, 0.06) (tri5, 0.01) (tri6, 1.98) (tri7, 2.02) (tri8, 0.06) (tri9, 0.00) (trit3, 0.04) (trit4, 0.47) (trit5, 212.76)};
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.06) (tri1, 0.07) (tri10, 0.01) (tri11, 0.01) (tri12, 0.22) (tri13, 0.00) (tri2, 0.36) (tri3, 0.03) (tri4, 0.06) (tri5, 0.01) (tri6, 1.82) (tri7, 1.92) (tri8, 0.06) (tri9, 0.00) (trit3, 0.04) (trit4, 0.45) (trit5, 209.93)};
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.06) (tri1, 0.06) (tri10, 0.01) (tri11, 0.01) (tri12, 0.22) (tri13, 0.01) (tri2, 0.32) (tri3, 0.03) (tri4, 0.06) (tri5, 0.01) (tri6, 1.86) (tri7, 1.84) (tri8, 0.05) (tri9, 0.00) (trit3, 0.04) (trit4, 0.45) (trit5, 212.14)};
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.07) (tri1, 0.06) (tri10, 0.01) (tri11, 0.01) (tri12, 0.20) (tri13, 0.00) (tri2, 0.31) (tri3, 0.03) (tri4, 0.06) (tri5, 0.01) (tri6, 1.83) (tri7, 1.81) (tri8, 0.05) (tri9, 0.00) (trit3, 0.04) (trit4, 0.45) (trit5, 208.28)};
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.07) (tri1, 0.06) (tri10, 0.01) (tri11, 0.01) (tri12, 0.20) (tri13, 0.00) (tri2, 0.29) (tri3, 0.03) (tri4, 0.05) (tri5, 0.01) (tri6, 1.79) (tri7, 1.80) (tri8, 0.05) (tri9, 0.00) (trit3, 0.04) (trit4, 0.46) (trit5, 208.62)};
  \end{axis}
\end{tikzpicture}

De vuelta a simple vista no se ve casi nada, salvo que en trit5 es casi lo mismo con más hilos. Para poder ver en más detalle, a continuación un gráfico de lo mismo pero sin trit5.

\noindent \begin{tikzpicture}
  \begin{axis}[
      ybar = 0.5pt, % Tipo de gráfico: barras verticales
      width = \textwidth,
      height = 0.4\textwidth,
      bar width = 0.003\textwidth,
      symbolic x coords = {FK2, FK3, FK4, tri1, tri2, tri3, tri4, tri5, tri6, tri7, tri8, tri9, tri10, tri11, tri12, tri13, trit3, trit4}, % Etiquetas en el eje x
      xtick = data, % Mostrar marcas en el eje x para cada dato
      ylabel = {Tiempo en segundos}, % Etiqueta del eje y
      %xlabel = {Cantidad de hilos}, % Etiqueta del eje x
      xticklabel style = {rotate = 45}, % Rotar etiquetas
      ymin = 0, % Mínimo del eje y
      ymax = 2.5, % Máximo del eje y
      enlarge x limits = 0.03, % Espaciado en los bordes
      legend style = {at = {(0.024, 0.94)}, anchor = north west}, % Estilo de la leyenda
      legend entries = {1, 2, 4, 6, 8, 16}, % Entradas de la leyenda
  ]
    % Datos para cada conjunto
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.07) (tri1, 0.10) (tri10, 0.01) (tri11, 0.02) (tri12, 0.32) (tri13, 0.01) (tri2, 0.53) (tri3, 0.03) (tri4, 0.07) (tri5, 0.01) (tri6, 2.20) (tri7, 2.19) (tri8, 0.07) (tri9, 0.00) (trit3, 0.04) (trit4, 0.47)};
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.06) (tri1, 0.08) (tri10, 0.01) (tri11, 0.02) (tri12, 0.28) (tri13, 0.00) (tri2, 0.42) (tri3, 0.03) (tri4, 0.06) (tri5, 0.01) (tri6, 1.98) (tri7, 2.02) (tri8, 0.06) (tri9, 0.00) (trit3, 0.04) (trit4, 0.47)};
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.06) (tri1, 0.07) (tri10, 0.01) (tri11, 0.01) (tri12, 0.22) (tri13, 0.00) (tri2, 0.36) (tri3, 0.03) (tri4, 0.06) (tri5, 0.01) (tri6, 1.82) (tri7, 1.92) (tri8, 0.06) (tri9, 0.00) (trit3, 0.04) (trit4, 0.45)};
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.06) (tri1, 0.06) (tri10, 0.01) (tri11, 0.01) (tri12, 0.22) (tri13, 0.01) (tri2, 0.32) (tri3, 0.03) (tri4, 0.06) (tri5, 0.01) (tri6, 1.86) (tri7, 1.84) (tri8, 0.05) (tri9, 0.00) (trit3, 0.04) (trit4, 0.45)};
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.07) (tri1, 0.06) (tri10, 0.01) (tri11, 0.01) (tri12, 0.20) (tri13, 0.00) (tri2, 0.31) (tri3, 0.03) (tri4, 0.06) (tri5, 0.01) (tri6, 1.83) (tri7, 1.81) (tri8, 0.05) (tri9, 0.00) (trit3, 0.04) (trit4, 0.45)};
    \addplot coordinates {(FK2, 0.00) (FK3, 0.00) (FK4, 0.07) (tri1, 0.06) (tri10, 0.01) (tri11, 0.01) (tri12, 0.20) (tri13, 0.00) (tri2, 0.29) (tri3, 0.03) (tri4, 0.05) (tri5, 0.01) (tri6, 1.79) (tri7, 1.80) (tri8, 0.05) (tri9, 0.00) (trit3, 0.04) (trit4, 0.46)};
  \end{axis}
\end{tikzpicture}

Se puede ver que aumentar los hilos hace que sea un poco más rápido, pero muy poco. Esto tiene sentido por el hecho de que solo la parte de agregar ambigüedades está paralelizada.

\section{Representación con cofactores}

El test que compara ejecutar Buchberger con y sin representación con cofactores también usa los mismos casos y mide los tiempos. Este es el único benchmark que no funciona con mains del directorio \path{mains} sino que usa sus propios archivos, y además este benchmark mide los tiempos desde \cpp. En el siguiente gráfico se pueden ver los resultados.

\noindent \begin{tikzpicture}
  \begin{axis}[
      ybar = 0.5pt, % Tipo de gráfico: barras verticales
      width = \textwidth,
      height = 0.4\textwidth,
      bar width = 0.015\textwidth, % Ancho de cada barra
      symbolic x coords = {FK2, FK3, FK4, tri1, tri2, tri3, tri4, tri5, tri6, tri7, tri8, tri9, tri10, tri11, tri12, tri13, trit3, trit4, trit5}, % Etiquetas en el eje x
      xtick = data, % Mostrar marcas en el eje x para cada dato
      ylabel = {Tiempo en segundos}, % Etiqueta del eje y
      %xlabel = {Categorías}, % Etiqueta del eje x
      xticklabel style = {rotate = 45}, % Rotar etiquetas
      ymin = 0, % Mínimo del eje y
      %ymax = 215,
      enlarge x limits = 0.03, % Espaciado en los bordes
      legend style = {at = {(0.024, 0.94)}, anchor = north west}, % Estilo de la leyenda
      legend entries = {Normal, Representación con cofactores}, % Entradas de la leyenda
  ]
    % Datos para cada conjunto
    \addplot coordinates {(FK2, 0.0) (FK3, 0.0) (FK4, 0.007) (tri1, 0.238) (tri10, 0.006) (tri11, 0.026) (tri12, 2.99) (tri13, 0.005) (tri2, 5.497) (tri3, 0.016) (tri4, 0.03) (tri5, 0.006) (tri6, 5.929) (tri7, 30.415) (tri8, 0.083) (tri9, 0.0) (trit3, 0.008) (trit4, 0.126) (trit5, 70.421)};
    \addplot coordinates {(FK2, 0.0) (FK3, 0.0) (FK4, 0.011) (tri1, 0.383) (tri10, 0.007) (tri11, 0.035) (tri12, 4.26) (tri13, 0.016) (tri2, 8.544) (tri3, 0.022) (tri4, 0.052) (tri5, 0.009) (tri6, 7.749) (tri7, 101.729) (tri8, 0.113) (tri9, 0.001) (trit3, 0.011) (trit4, 0.173) (trit5, 94.521)};
  \end{axis}
\end{tikzpicture}

Se puede ver que la representación con cofactores lleva más tiempo, pero en general no tanto más. Acá no hay demasiado más para ver en la parte de abajo, así que no se hace un gráfico con solo la parte de más abajo.

\section{Pertenencia a ideales}

El test que para conjuntos generadores construidos al azar y polinomios construidos al azar ve con los distintos algoritmos si el polinomio está en el ideal generado o no, también mide los tiempos. Estos corren tanto para los racionales como para la aritmética modular, pero para los racionales incluyen la comparación con \texttt{operator\_gb} y para la aritmética modular no, porque \texttt{operator\_gb} es solo para racionales. Este benchmark imprime los tiempos promedios sobre todos los conjuntos generadores construidos al azar y polinomios construidos al azar y también los tiempos máximos. Estos son los resultados para los racionales (tal cual los imprime el test, \texttt{Buch} es \texttt{Buchberger} y \texttt{GB} es \texttt{operator\_gb}):

\begin{minted}{text}
  Average times:
  Buch: 0.0041429424s
  F4: 0.0105502963s
  GB: 0.8995876765s
  Max times:
  Buch: 0.1320753098s
  F4: 0.0925185680s
  GB: 4.6477575302s
\end{minted}

Y estos para la aritmética modular (también tal cual los imprime el test):

\begin{minted}{text}
  Average times:
  Buch_Zp: 0.0028817248s
  F4_Zp: 0.0072190428s
  Max times:
  Buch_Zp: 0.0779249668s
  F4_Zp: 0.0799708366s
\end{minted}

Estos datos igualmente mucho no dicen, porque como buscan por una cierta cantidad de pasos, y los pasos son distintos en los distintos algoritmos, no se puede comparar. En particular, un paso de \texttt{F4} puede corresponder a muchos pasos de \texttt{Buchberger}.

Este benchmark se usó principalmente para tenerlo como comparación cuando se hacían cambios en el código. Como se corrían siempre todos los tests y benchmarks antes y después del cambio, este servía para comparar el antes y el después.

\chapter{Conclusiones}\label{cap:Conclusiones}

El tema de este trabajo fue propuesto por el director Cristian Vay por su interés ya mencionado en usarlo para las álgebras de Nichols. Para mí este era un tema completamente nuevo, así que empecé por estudiar el tema. Primero leyendo \cite{book:ideals-varieties-algorithms} que trata sobre bases de Gröbner conmutativas. Mientras leía ese libro hice también una implementación propia, sin mucha prolijidad, de bases de Gröbner conmutativas en \cpp. Esto fue bueno porque me sirvió de práctica para después hacer la implementación de bases de Gröbner no conmutativas. Después de eso fui leyendo la tesis de master \cite{thesis:Hof20} y la tesis de doctorado \cite{phdthesis:Hof23} de Clemens Hofstadler, que son textos sobre bases de Gröbner no conmutativas, e implementando lo explicado en \cpp.

A continuación menciono algunos aspectos positivas y después algunos posibles trabajos futuros.

\section{Aspectos positivos de este trabajo}

La implementación de bases de Gröbner no conmutativas presentada en esta tesis es la primera en \cpp (según mi conocimiento). Esto es muy bueno porque \cpp es un lenguaje particularmente rápido y en el que es fácil paralelizar. Esto probablemente sea útil si se quieren correr casos que se demoran mucho.

También, es la primer implementación de F4 que funciona para un cuerpo arbitrario (según mi conocimiento). Esto está bueno para que si en algún momento alguien lo quiere usar para algún cuerpo distinto lo pueda hacer.

La implementación del algoritmo Buchberger incluye la parte de la representación con cofactores, que si bien es algo muy fácil, la única otra implementación que lo hace es \texttt{operator\_gb} (según mi conocimiento).

\section{Trabajos futuros}\label{section:trabajos futuros}

Como ya se dijo varias veces, falta usar la reducción por filas de Faugère-Lachartre que es más eficiente, y ese es uno de los trabajos futuros más importantes para hacer.

Del algoritmo F4 (y de Buchberger) para el caso conmutativo hay varias implementaciones en \cpp, por ejemplo \cite{lib:openf4, lib:mathic, lib:M4GB}, inclusive en paralelo, como \cite{DBLP:journals/jsc/Reeves98, lib:parallelGBC}. Muchas son solo para los enteros módulo un primo, pero algunas también para los racionales. Es muy probable que analizando esas implementaciones se puedan sacar muchas ideas o implementaciones útiles, en particular, podría ser que ya alguien haya implementado la reducción por filas de Faugère-Lachartre en \cpp.

Cuando se fue haciendo la librería, muchas veces pasaba que había cambios que parecían optimizaciones de implementación pero al hacerlos el código resultaba andar más lento (a veces por muy poquito), así que se descartaban esos cambios. Sería bueno agregar más tests y probar hacer cambios para ver si mejoran o no los tiempos teniendo más tests, porque podría ser que fueran un poquito más lento justo en los casos de los tests pero que en general sí fueran más rápidas.

Se podría analizar cuánto tiempo se consume en las distintas partes de los algoritmos, porque eso podría ayudar a saber en qué partes se pueden realizar optimizaciones.

En la librería hay una especialización de \texttt{rref} para hacer que \texttt{rref} para \texttt{mpq\_class} sea más rápido usando la implementación de la librería. Para aritmética modular se está usando un tipo propio definido en el archivo \path{extras/ModularArithmetic.hpp} y se usa la versión genérica de \texttt{rref}. Es posible que existan librerías que tengan tipos de aritmética modular más rápidos que el que se implementó y con una reducción por filas más rápida, y en ese caso sería bueno usarlas.

Actualmente la librería no viene con nada para instalarla automáticamente porque en \cpp eso no es fácil de hacer. Esto significa que si alguien quiere usar la librería tiene que arreglárselas y probablemente hacer algo medio feo como poner los archivos en alguna carpeta particular. Otras librerías de \cpp sí vienen con algo para eso, así que sería bueno hacerlo para esta librería también.

Los ideales de la \cref{def:ideal} no son los únicos; también existen los ideales a izquierda que se definen de la siguiente manera.
\begin{definition}
  Sean $R$ un anillo e $I ⊆ R$. Se define que $I$ es un ideal a izquierda de $R$ si y solo si:
  \begin{enumerate}
    \item $I ≠ ∅$.
    \item $∀a, b ∈ I : a + b ∈ I$.
    \item $∀a ∈ I, r ∈ R : r a ∈ I$.
  \end{enumerate}
\end{definition}
\noindent Con los ideales a izquierda sobre polinomios no conmutativos también hay toda una teoría de bases de Gröbner. En \cite{phdthesis:Hof23} se puede encontrar una explicación sobre esta teoría. Implementar el cálculo de bases de Gröbner de la teoría de ideales a izquierda en \cpp es otro trabajo que se podría hacer. Análogamente, también se pueden considerar ideales a derecha.

Como ya se mencionó en el \cref{cap:Benchmarks}, no se sabe para $n ≥ 6$ si los FK$n$ tienen una base finita o no. Cuando se tenga una versión muy buena de F4, se la podría correr por mucho tiempo para FK6 para ver si llega en algún momento a una base finita.

Actualmente para estudiar el tema de bases de Gröbner, tanto conmutativas como no conmutativas, hay libros y artículos sobre el tema, en particular \cite{book:ideals-varieties-algorithms} para conmutativas y \cite{thesis:Hof20} para no conmutativas, pero para practicar las implementaciones no hay mucha ayuda. Por ejemplo, cuando implementé las bases de Gröbner conmutativas para practicar tuve que hacer mi propio testing. Para que sea más fácil estudiar el tema sería útil preparar problemas del estilo de programación competitiva sobre el tema. Estos problemas se podrían poner en, por ejemplo, \href{https://judge.yosupo.jp}{Library Checker}, o en un grupo de \href{https://codeforces.com}{Codeforces}.

Por último, otra cosa que se podría hacer es implementar el cálculo de bases de Gröbner no conmutativas en otros lenguajes, como por ejemplo Rust, que, además de ser un lenguaje muy rápido como \cpp, maneja la memoria de forma más segura, previniendo los errores de memoria, que en la implementación en \cpp podría haber porque se usan referencias en algunos lugares.

\chapter*{Bibliografía}
\addcontentsline{toc}{chapter}{Bibliografía} % Estás dos lineas están así para que no aparesca como capítulo numerado, pero si aparesca en el índice

\printbibliography[heading=none]

\end{document}
